[
  {
    "objectID": "homework/hw3_questions.html",
    "href": "homework/hw3_questions.html",
    "title": "Multinomial Logit Model",
    "section": "",
    "text": "This assignment expores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm."
  },
  {
    "objectID": "homework/hw3_questions.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "href": "homework/hw3_questions.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "title": "Multinomial Logit Model",
    "section": "1. Likelihood for the Multi-nomial Logit (MNL) Model",
    "text": "1. Likelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 3 products, then either \\(y=3\\) or \\(y=(0,0,1)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, brand, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 3 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]"
  },
  {
    "objectID": "homework/hw3_questions.html#simulate-conjoint-data",
    "href": "homework/hw3_questions.html#simulate-conjoint-data",
    "title": "Multinomial Logit Model",
    "section": "2. Simulate Conjoint Data",
    "text": "2. Simulate Conjoint Data\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a “no choice” option; each simulated respondent must select one of the 3 alternatives.\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from $4 to $32 in increments of $4.\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer \\(i\\) for hypothethical streaming service \\(j\\) is\n\\[\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n\\]\nwhere the variables are binary indicators and \\(\\varepsilon\\) is Type 1 Extreme Value (ie, Gumble) distributed.\nThe following code provides the simulation of the conjoint data.\n\nimport numpy as np\nimport pandas as pd\n\n# Set random seed\nnp.random.seed(123)\n\n# Define attribute levels\nbrands = [\"N\", \"P\", \"H\"]  # Netflix, Prime, Hulu\nads = [\"Yes\", \"No\"]\nprices = list(range(8, 33, 4))  # from 8 to 32 by 4\n\n# Generate full factorial design\nimport itertools\nprofiles = pd.DataFrame(\n    list(itertools.product(brands, ads, prices)), \n    columns=[\"brand\", \"ad\", \"price\"])\n\n# Utility functions (true parameters)\nbrand_utils = {\"N\": 1.0, \"P\": 0.5, \"H\": 0.0}\nad_utils = {\"Yes\": -0.8, \"No\": 0.0}\nprice_util = lambda p: -0.1 * p\n\n# Settings\nn_peeps = 100\nn_tasks = 10\nn_alts = 3\n\n# Function to simulate one respondent\ndef simulate_respondent(pid):\n    respondent_data = []\n    for task in range(1, n_tasks + 1):\n        alts = profiles.sample(n=n_alts).copy()\n        alts[\"resp\"] = pid\n        alts[\"task\"] = task\n        \n        # Calculate deterministic utility\n        alts[\"v\"] = (\n            alts[\"brand\"].map(brand_utils) +\n            alts[\"ad\"].map(ad_utils) +\n            alts[\"price\"].apply(price_util)\n        ).round(10)\n        \n        # Add Gumbel-distributed error (Type I Extreme Value)\n        gumbel_noise = -np.log(-np.log(np.random.rand(n_alts)))\n        alts[\"u\"] = alts[\"v\"] + gumbel_noise\n        \n        # Choose the alternative with max utility\n        alts[\"choice\"] = (alts[\"u\"] == alts[\"u\"].max()).astype(int)\n        \n        respondent_data.append(alts)\n    \n    return pd.concat(respondent_data, ignore_index=True)\n\n# Simulate for all respondents\ndf_list = [simulate_respondent(i) for i in range(1, n_peeps + 1)]\nconjoint_data = pd.concat(df_list, ignore_index=True)\n\n# Keep only relevant columns (as if unobservable utility components are hidden)\nconjoint_data = conjoint_data[[\"resp\", \"task\", \"brand\", \"ad\", \"price\", \"choice\"]]"
  },
  {
    "objectID": "homework/hw3_questions.html#preparing-the-data-for-estimation",
    "href": "homework/hw3_questions.html#preparing-the-data-for-estimation",
    "title": "Multinomial Logit Model",
    "section": "3. Preparing the Data for Estimation",
    "text": "3. Preparing the Data for Estimation\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables.\nTo estimate the MNL likelihood, we must reshape the data to panel (long) format, and encode categorical variables (brand and ad exposure) into binary indicators. Each row represents one alternative within a choice task.\n\nimport pandas as pd\nimport numpy as np\n\nconjoint_data = pd.read_csv(\"../data/conjoint_data.csv\")\n\n# One-hot encode\nX = pd.get_dummies(conjoint_data, columns=[\"brand\", \"ad\"], drop_first=True)\n\nX.rename(columns={\n    \"brand_P\": \"Prime\",\n    \"brand_N\": \"Netflix\",\n    \"ad_Yes\": \"Ads\"\n}, inplace=True)\n\nfor col in ['resp', 'task', 'choice']:\n    X[col] = X[col].astype('int')\n\nX.head()\n\n\n\n\n\n\n\n\nresp\ntask\nchoice\nprice\nNetflix\nPrime\nAds\n\n\n\n\n0\n1\n1\n1\n28\nTrue\nFalse\nTrue\n\n\n1\n1\n1\n0\n16\nFalse\nFalse\nTrue\n\n\n2\n1\n1\n0\n16\nFalse\nTrue\nTrue\n\n\n3\n1\n2\n0\n32\nTrue\nFalse\nTrue\n\n\n4\n1\n2\n1\n16\nFalse\nTrue\nTrue\n\n\n\n\n\n\n\nNow we have the data in a suitable format for estimating the likelihood. We will treat “Hulu” and “Ad-Free” as the base categories."
  },
  {
    "objectID": "homework/hw3_questions.html#estimation-via-maximum-likelihood",
    "href": "homework/hw3_questions.html#estimation-via-maximum-likelihood",
    "title": "Multinomial Logit Model",
    "section": "4. Estimation via Maximum Likelihood",
    "text": "4. Estimation via Maximum Likelihood\nWe estimate the multinomial logit model by maximizing the joint log-likelihood across all respondents and tasks. The 4 parameters are:\n- \\(\\beta_\\text{netflix}\\)\n- \\(\\beta_\\text{prime}\\)\n- \\(\\beta_\\text{ads}\\)\n- \\(\\beta_\\text{price}\\)\n\nfrom scipy.optimize import minimize\n\nfeatures = [\"Netflix\", \"Prime\", \"Ads\", \"price\"]\nX[features] = X[features].astype(float)\n\ngroups = X.groupby([\"resp\", \"task\"])\n\nX_mat = X[features].to_numpy()\ny = X[\"choice\"].to_numpy()\nn_rows = X.shape[0]\nn_params = len(features)\n\nWe now proceed to implement the log-likelihood function for the MNL model introduced earlier. For clarity, we restate the key expressions used in estimation below.\n\nLog-Likelihood Function for the Multinomial Logit Model\nTo estimate the parameters of the multinomial logit (MNL) model, we maximize the log-likelihood function based on the observed choices. For each respondent \\(i\\) and choice task \\(t\\), let \\(\\mathcal{J}_{it}\\) be the set of alternatives available (typically 3). Let \\(x_{ijt}\\) be the feature vector of alternative \\(j\\), and \\(\\beta\\) the parameter vector. The utility of alternative \\(j\\) is modeled as:\n\\[\nU_{ijt} = x_{ijt}^\\top \\beta + \\varepsilon_{ijt}\n\\]\nAssuming \\(\\varepsilon_{ijt}\\) follows an i.i.d. Type I Extreme Value distribution, the probability that individual \\(i\\) chooses option \\(j\\) in task \\(t\\) is:\n\\[\n\\mathbb{P}_{ijt} = \\frac{\\exp(x_{ijt}^\\top \\beta)}{\\sum_{k \\in \\mathcal{J}_{it}} \\exp(x_{ikt}^\\top \\beta)}\n\\]\nLet \\(\\delta_{ijt}\\) be an indicator variable equal to 1 if alternative \\(j\\) was chosen by respondent \\(i\\) in task \\(t\\), and 0 otherwise. The log-likelihood across all respondents and tasks is:\n\\[\n\\ell(\\beta) = \\sum_{i} \\sum_{t} \\sum_{j \\in \\mathcal{J}_{it}} \\delta_{ijt} \\cdot \\log \\mathbb{P}_{ijt}\n\\]\nThis is the function we aim to maximize with respect to \\(\\beta\\) using numerical optimization.\n\ndef neg_log_likelihood(beta, X=X_mat, y=y, groups=groups):\n    utilities = X @ beta\n    log_likelihood = 0\n    start = 0\n\n    for _, group in groups:\n        n = group.shape[0]\n        util_slice = utilities[start:start + n]\n        choice_slice = y[start:start + n]\n        denom = np.sum(np.exp(util_slice))\n        probs = np.exp(util_slice) / denom\n        log_likelihood += np.log(probs @ choice_slice)\n        start += n\n\n    return -log_likelihood\n\nThe table below presents the MLEs for each parameter in the MNL model, along with standard errors and 95% confidence intervals. These values are estimated using scipy.optimize.minimize() with the BFGS method.\n\n# Initial guess\ninit_beta = np.zeros(n_params)\n\n# Optimize\nresult = minimize(neg_log_likelihood, init_beta, method='BFGS')\n\n# Estimated betas\nbeta_hat = result.x\nhessian_inv = result.hess_inv\n\n# Standard errors\nse = np.sqrt(np.diag(hessian_inv))\n\n# 95% confidence intervals\nz = 1.96\nci_lower = beta_hat - z * se\nci_upper = beta_hat + z * se\n\n# Combine results\nresults = pd.DataFrame({\n    \"parameter\": features,\n    \"estimate\": beta_hat,\n    \"std_error\": se,\n    \"ci_lower\": ci_lower,\n    \"ci_upper\": ci_upper\n})\n\nfrom IPython.display import display\ndisplay(results.round(4))\n\n\n\n\n\n\n\n\nparameter\nestimate\nstd_error\nci_lower\nci_upper\n\n\n\n\n0\nNetflix\n0.9412\n0.0335\n0.8756\n1.0068\n\n\n1\nPrime\n0.5016\n0.1180\n0.2704\n0.7329\n\n\n2\nAds\n-0.7320\n0.0898\n-0.9080\n-0.5560\n\n\n3\nprice\n-0.0995\n0.0064\n-0.1121\n-0.0868"
  },
  {
    "objectID": "homework/hw3_questions.html#estimation-via-bayesian-methods",
    "href": "homework/hw3_questions.html#estimation-via-bayesian-methods",
    "title": "Multinomial Logit Model",
    "section": "5. Estimation via Bayesian Methods",
    "text": "5. Estimation via Bayesian Methods\nWe now estimate the MNL model using Bayesian inference, via the Metropolis-Hastings (MH) algorithm. We use the same log-likelihood function from the MLE section and combine it with log-priors to compute the unnormalized log-posterior.\nThe priors are: - \\(\\beta_{\\text{Netflix}}, \\beta_{\\text{Prime}}, \\beta_{\\text{Ads}} \\sim \\mathcal{N}(0, 5^2)\\)\n- \\(\\beta_{\\text{price}} \\sim \\mathcal{N}(0, 1^2)\\)\nThe proposal distribution is a multivariate normal with zero covariance, i.e., independent proposals: - First 3 dimensions: \\(\\mathcal{N}(0, 0.05)\\)\n- Last dimension (price): \\(\\mathcal{N}(0, 0.005)\\)\n\n# Prior log densities\ndef log_prior(beta):\n    lp = -0.5 * (beta[0:3]**2 / 25).sum()\n    lp += -0.5 * (beta[3]**2 / 1)\n    return lp\n\n# Log posterior = log likelihood + log prior\ndef log_posterior(beta):\n    return -neg_log_likelihood(beta) + log_prior(beta)\n\n\n# MH sampler\ndef metropolis_sampler(log_post_fn, start, steps=11000):\n    draws = np.zeros((steps, len(start)))\n    draws[0] = start\n    current_lp = log_post_fn(start)\n\n    for t in range(1, steps):\n        # Propose: independent normal steps\n        proposal = draws[t-1] + np.array([\n            np.random.normal(0, np.sqrt(0.05)),\n            np.random.normal(0, np.sqrt(0.05)),\n            np.random.normal(0, np.sqrt(0.05)),\n            np.random.normal(0, np.sqrt(0.005))\n        ])\n\n        proposal_lp = log_post_fn(proposal)\n\n        # MH acceptance\n        log_accept_ratio = proposal_lp - current_lp\n        if np.log(np.random.rand()) &lt; log_accept_ratio:\n            draws[t] = proposal\n            current_lp = proposal_lp\n        else:\n            draws[t] = draws[t-1]\n\n    return draws\n\n\nnp.random.seed(42)\n\nstart_beta = np.zeros(4)\nsamples = metropolis_sampler(log_posterior, start=start_beta, steps=11000)\n\nposterior = samples[1000:]  # remove burn-in\n\n# Compute MCMC acceptance rate\naccepted = np.sum(np.any(samples[1:] != samples[:-1], axis=1))\naccept_rate = accepted / (samples.shape[0] - 1)\nprint(f\"Acceptance rate: {accept_rate:.3f}\")\n\nAcceptance rate: 0.018\n\n\n\nimport matplotlib.pyplot as plt\n\nparam_names = [\"$\\\\beta_{netflix}$\", \"$\\\\beta_{prime}$\", \"$\\\\beta_{ads}$\", \"$\\\\beta_{price}$\"]\n\nfig, axes = plt.subplots(4, 2, figsize=(12, 10))\n\nfor i in range(4):\n    # Trace plot\n    axes[i, 0].plot(posterior[:, i], linewidth=0.7)\n    axes[i, 0].set_title(f\"Trace plot: {param_names[i]}\")\n    axes[i, 0].set_ylabel(\"Value\")\n    axes[i, 0].grid(alpha=0.3)\n\n    # Histogram\n    axes[i, 1].hist(posterior[:, i], bins=30, density=True)\n    axes[i, 1].axvline(posterior[:, i].mean(), color=\"red\", linestyle=\"--\", label=\"Mean\")\n    axes[i, 1].set_title(f\"Posterior histogram: {param_names[i]}\")\n    axes[i, 1].set_xlabel(\"Parameter value\")\n    axes[i, 1].set_ylabel(\"Density\")\n    axes[i, 1].grid(alpha=0.3)\n    axes[i, 1].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nPosterior Diagnostics\nThe trace plots and posterior histograms for all four parameters are shown below. Visually, the trace plots indicate that the Markov chains mix reasonably well and remain stable over time, suggesting adequate convergence. The chains explore the parameter space with some local variability but no major trends or drifts, which supports the credibility of the resulting posterior estimates.\nThe posterior histograms further confirm the patterns observed in the trace plots:\n\n\\(\\beta_{netflix}\\) shows a slightly right-skewed distribution centered near 0.93, with reasonable spread and no multimodality.\n\\(\\beta_{prime}\\) exhibits greater dispersion but maintains a unimodal shape around 0.49.\n\\(\\beta_{ads}\\) is centered tightly around -0.73, with most posterior mass between -0.90 and -0.57.\n\\(\\beta_{price}\\) is the most precise, with a narrow posterior centered around -0.099, confirming strong evidence for price sensitivity.\n\nThese diagnostics suggest that the Metropolis-Hastings sampler successfully captured the posterior distributions for all parameters, with trace plots supporting convergence and histograms indicating stable inference. No signs of severe autocorrelation or non-convergence are evident.\n\nposterior_summary = pd.DataFrame({\n    \"parameter\": features,\n    \"mean\": posterior.mean(axis=0),\n    \"std_dev\": posterior.std(axis=0),\n    \"ci_lower\": np.percentile(posterior, 2.5, axis=0),\n    \"ci_upper\": np.percentile(posterior, 97.5, axis=0)\n}).round(4)\n\ndisplay(posterior_summary)\n\n\n\n\n\n\n\n\nparameter\nmean\nstd_dev\nci_lower\nci_upper\n\n\n\n\n0\nNetflix\n0.9325\n0.1000\n0.7136\n1.1181\n\n\n1\nPrime\n0.4934\n0.0917\n0.3041\n0.6559\n\n\n2\nAds\n-0.7307\n0.0882\n-0.9066\n-0.5691\n\n\n3\nprice\n-0.0994\n0.0061\n-0.1109\n-0.0885\n\n\n\n\n\n\n\nThe table above summarizes the posterior means, standard deviations, and 95% credible intervals for each parameter, based on 10,000 retained samples from the Metropolis-Hastings sampler.\nThese can be directly compared with the MLE results from Section 4 to assess similarity in point estimates and uncertainty.\n\n\nComparison of MLE and Bayesian Estimates\nThe table below compares the parameter estimates obtained via Maximum Likelihood Estimation (MLE) and Bayesian inference (via Metropolis-Hastings MCMC). Overall, the point estimates from both methods are very similar, with only minor differences across parameters. This consistency suggests that the data are informative and the priors used in the Bayesian method are relatively non-influential.\n\nFor \\(\\beta_{netflix}\\), the MLE estimate is 0.9412 (95% CI: [0.8756, 1.0068]), while the Bayesian posterior mean is 0.9325 (95% CI: [0.7136, 1.1181]). Both suggest a strong positive preference for Netflix, and their intervals largely overlap.\nFor \\(\\beta_{prime}\\), both approaches yield moderate positive estimates (MLE: 0.5016; Bayes: 0.4934), with the Bayesian interval slightly narrower due to the influence of the prior.\nFor \\(\\beta_{ads}\\), both estimates indicate a strong negative effect of ads on utility (MLE: -0.7320; Bayes: -0.7307), with virtually identical standard errors and credible/confidence intervals.\nThe coefficient on price is also consistent (MLE: -0.0995; Bayes: -0.0994), confirming that higher prices reduce the probability of choice.\n\nThe Bayesian credible intervals are slightly wider for some parameters (e.g., Netflix), likely reflecting greater uncertainty due to the prior. Overall, both methods tell a coherent story: consumers prefer Netflix and Prime over Hulu, dislike ads, and are price-sensitive."
  },
  {
    "objectID": "homework/hw3_questions.html#discussion",
    "href": "homework/hw3_questions.html#discussion",
    "title": "Multinomial Logit Model",
    "section": "6. Discussion",
    "text": "6. Discussion\n\nInterpretation of Parameter Estimates\nIf we did not know the data were simulated, we would still conclude from the estimates that respondents generally:\n\nPrefer Netflix over Amazon Prime, and both over Hulu (the omitted reference level). This is reflected in the fact that \\(\\beta_{\\text{Netflix}} &gt; \\beta_{\\text{Prime}} &gt; 0\\).\nDislike advertisements, as indicated by the consistently negative \\(\\beta_{\\text{ads}}\\).\nAre price-sensitive, as \\(\\beta_{\\text{price}} &lt; 0\\) implies that the likelihood of choosing an alternative decreases as its price increases.\n\nSpecifically, \\(\\beta_{\\text{Netflix}} &gt; \\beta_{\\text{Prime}}\\) means that, holding other attributes constant, the utility (and therefore the probability of being chosen) is higher for Netflix than Prime. This aligns with common consumer preferences in the streaming market. Likewise, a negative \\(\\beta_{\\text{price}}\\) is consistent with economic theory: higher prices reduce demand.\n\n\nToward a Hierarchical (Multi-level) Model\nIn our current model, all respondents share the same set of preference parameters \\(\\beta\\) — that is, we assume homogeneous preferences across individuals. However, in real-world conjoint studies, different people often have different tastes.\nTo model this heterogeneity, we can move to a hierarchical (random-parameter) model. In this framework, each respondent \\(i\\) has their own parameter vector \\(\\beta_i\\), which is drawn from a population-level distribution:\n\\[\n\\beta_i \\sim \\mathcal{N}(\\mu, \\Sigma)\n\\]\nHere, \\(\\mu\\) represents the average preference across the population, and \\(\\Sigma\\) captures the variation between individuals.\nTo simulate such data, we would: 1. Draw a unique \\(\\beta_i\\) for each respondent from the population distribution. 2. Use that \\(\\beta_i\\) to simulate choices for each of their tasks.\nTo estimate the parameters, we would need to use Bayesian hierarchical methods or maximum simulated likelihood, such as: - MCMC with Gibbs or Hamiltonian sampling - Hierarchical Bayes via Stan or PyMC - Mixed logit estimation (if using frequentist methods)\nThese models are more complex, but they better capture real consumer behavior by accounting for individual-level preference variation.\n\n\nSampler Performance Note\nThe Metropolis-Hastings algorithm yielded an acceptance rate of approximately 1.8%, which is substantially lower than the typical recommended range (20%–40%). This suggests that the proposal distribution may not have been well-tuned for the posterior geometry, potentially resulting in poor mixing and inefficient exploration of the parameter space. While the trace plots do show some movement across the support, future implementations could benefit from increasing the proposal variance or adopting adaptive MCMC methods to improve efficiency."
  },
  {
    "objectID": "homework/hw2_questions.html",
    "href": "homework/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n\nCode\nimport pandas as pd\ndf = pd.read_csv(\"../data/blueprinty.csv\")\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(1, 2, figsize=(10, 4), sharey=True)\n\ndf[df['iscustomer'] == 1]['patents'].hist(ax=ax[0], bins=10, color='skyblue')\nax[0].set_title('Customers')\nax[0].set_xlabel('Number of Patents')\n\ndf[df['iscustomer'] == 0]['patents'].hist(ax=ax[1], bins=10, color='salmon')\nax[1].set_title('Non-Customers')\nax[1].set_xlabel('Number of Patents')\n\nfig.suptitle('Histogram of Patents by Customer Status')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf.groupby('iscustomer')['patents'].mean()\n\n\niscustomer\n0    3.473013\n1    4.133056\nName: patents, dtype: float64\n\n\nCustomers tend to have more patents on average (4.13 vs. 3.47), and their distribution is more spread out, with more firms holding a higher number of patents. Non-customers are more concentrated at lower patent counts.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n\nCode\nimport seaborn as sns\n\nfig, axs = plt.subplots(1, 2, figsize=(12, 4))\n\nsns.countplot(data=df, x='region', hue='iscustomer', ax=axs[0])\naxs[0].set_title('Region by Customer Status')\naxs[0].set_xlabel('Region')\naxs[0].set_ylabel('Count')\naxs[0].legend(title='Customer')\n\nsns.kdeplot(data=df, x='age', hue='iscustomer', ax=axs[1], fill=True)\naxs[1].set_title('Age Distribution by Customer Status')\naxs[1].set_xlabel('Age')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf.groupby('iscustomer')['age'].mean()\n\n\niscustomer\n0    26.101570\n1    26.900208\nName: age, dtype: float64\n\n\nCustomers tend to be slightly older (average age 26.9 vs. 26.1). The regional distribution shows a clear difference: a much higher number of customers come from the Northeast, while other regions like the Midwest and Southwest are dominated by non-customers.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\n\nWe assume the number of patents \\(Y \\sim \\text{Poisson}(\\lambda)\\), with density:\n\\[\nf(Y \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^Y}{Y!}\n\\]\nFor a sample \\(Y_1, Y_2, \\dots, Y_n\\), the log-likelihood is:\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^n \\left( -\\lambda + Y_i \\log \\lambda - \\log(Y_i!) \\right)\n\\]\nLog-Likelihood Function in Python\n\n\nCode\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef poisson_log_likelihood(lmbda, y):\n    y = np.asarray(y)\n    return np.sum(-lmbda + y * np.log(lmbda) - gammaln(y + 1))\n\n\nPlotting the Log-Likelihood Curve\n\n\nCode\nimport matplotlib.pyplot as plt\n\ny = df['patents'].values\nlambdas = np.linspace(0.1, 10, 100)\nlogliks = [poisson_log_likelihood(lmbda, y) for lmbda in lambdas]\n\nplt.plot(lambdas, logliks)\nplt.xlabel(\"λ (lambda)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.title(\"Poisson Log-Likelihood for Different λ\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nThe analytical MLE for \\(\\lambda\\) is simply the sample mean of \\(Y\\), since:\n\\[\n\\hat{\\lambda}_{\\text{MLE}} = \\bar{Y}\n\\] Sample Mean\n\n\nCode\nybar = np.mean(y)\nybar\n\n\n3.6846666666666668\n\n\nNumerical Maximization Using scipy.optimize\n\n\nCode\nfrom scipy.optimize import minimize\n\ndef neg_log_likelihood(lmbda):\n    return -poisson_log_likelihood(lmbda[0], y)\n\nresult = minimize(neg_log_likelihood, x0=[1.0], bounds=[(0.001, None)])\nresult.x[0]\n\n\n3.6846662953477973\n\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n\nWe now extend our Poisson model to allow the rate of patent awards to depend on firm characteristics via:\n\\[\nY_i \\sim \\text{Poisson}(\\lambda_i), \\quad \\text{where } \\lambda_i = \\exp(X_i^\\top \\beta)\n\\]\nCovariates include: age, age squared, region (as dummies), and whether the firm is a Blueprinty customer.\n\n\n\n\nCode\nimport pandas as pd\nimport statsmodels.api as sm\n\ndf['age_squared'] = df['age'] ** 2\nX_df = pd.get_dummies(df[['age', 'age_squared', 'region', 'iscustomer']], drop_first=True)\nX_df = sm.add_constant(X_df)\n\nX_df = X_df.astype(float)\nX = X_df\ny = df['patents'].values\n\n\n\n\n\n\n\nCode\nimport statsmodels.api as sm\n\nmodel = sm.GLM(y, X, family=sm.families.Poisson())\nglm_result = model.fit()\n\nglm_result.summary()\n\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\ny\nNo. Observations:\n1500\n\n\nModel:\nGLM\nDf Residuals:\n1492\n\n\nModel Family:\nPoisson\nDf Model:\n7\n\n\nLink Function:\nLog\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-3258.1\n\n\nDate:\nFri, 16 May 2025\nDeviance:\n2143.3\n\n\nTime:\n16:51:38\nPearson chi2:\n2.07e+03\n\n\nNo. Iterations:\n5\nPseudo R-squ. (CS):\n0.1360\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n-0.5089\n0.183\n-2.778\n0.005\n-0.868\n-0.150\n\n\nage\n0.1486\n0.014\n10.716\n0.000\n0.121\n0.176\n\n\nage_squared\n-0.0030\n0.000\n-11.513\n0.000\n-0.003\n-0.002\n\n\niscustomer\n0.2076\n0.031\n6.719\n0.000\n0.147\n0.268\n\n\nregion_Northeast\n0.0292\n0.044\n0.669\n0.504\n-0.056\n0.115\n\n\nregion_Northwest\n-0.0176\n0.054\n-0.327\n0.744\n-0.123\n0.088\n\n\nregion_South\n0.0566\n0.053\n1.074\n0.283\n-0.047\n0.160\n\n\nregion_Southwest\n0.0506\n0.047\n1.072\n0.284\n-0.042\n0.143\n\n\n\n\n\n\n\n\n\n\nCode\nsummary_df = pd.DataFrame({\n    \"Coefficient\": glm_result.params,\n    \"Std. Error\": glm_result.bse,\n    \"z-value\": glm_result.tvalues,\n    \"p-value\": glm_result.pvalues\n})\n\nsummary_df\n\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\nz-value\np-value\n\n\n\n\nconst\n-0.508920\n0.183179\n-2.778269\n5.464935e-03\n\n\nage\n0.148619\n0.013869\n10.716250\n8.539597e-27\n\n\nage_squared\n-0.002970\n0.000258\n-11.513237\n1.131496e-30\n\n\niscustomer\n0.207591\n0.030895\n6.719179\n1.827509e-11\n\n\nregion_Northeast\n0.029170\n0.043625\n0.668647\n5.037205e-01\n\n\nregion_Northwest\n-0.017575\n0.053781\n-0.326782\n7.438327e-01\n\n\nregion_South\n0.056561\n0.052662\n1.074036\n2.828066e-01\n\n\nregion_Southwest\n0.050576\n0.047198\n1.071568\n2.839141e-01\n\n\n\n\n\n\n\n\n\n\nFirms that are Blueprinty customers are expected to produce about 23%(i.e. exp(0.2076)-1) more patents than non-customers, holding other factors constant. This effect is statistically significant.\nPatent output increases with firm age, but at a decreasing rate—suggesting older firms patent more, but the effect tapers off.\nRegional differences are not statistically significant, indicating little variation in patenting across regions once other firm characteristics are controlled for.\n\n\n\nWe create two fake datasets: - X_0 with iscustomer = 0 for all firms (as if no firm were a customer) - X_1 with iscustomer = 1 for all firms (as if all firms were customers)\nWe use the fitted model to compute predicted number of patents for each case, then take the difference.\n\n\nCode\nX_0 = X.copy()\nX_1 = X.copy()\n\nX_0.loc[:, X.columns.str.contains(\"iscustomer\")] = 0\nX_1.loc[:, X.columns.str.contains(\"iscustomer\")] = 1\n\n# Predict expected patent counts\ny_pred_0 = glm_result.predict(X_0)\ny_pred_1 = glm_result.predict(X_1)\n\n# Difference in predicted patents\ndiff = y_pred_1 - y_pred_0\ndiff.mean()\n\n\n0.7927680710452626\n\n\n\n\n\nOn average, firms are predicted to receive 0.79(i.e. 3.47 * 0.23) more patents over five years if they are Blueprinty customers, compared to if they are not—holding all other firm characteristics constant. This suggests a meaningful positive effect of the software on patenting success."
  },
  {
    "objectID": "homework/hw2_questions.html#blueprinty-case-study",
    "href": "homework/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n\nCode\nimport pandas as pd\ndf = pd.read_csv(\"../data/blueprinty.csv\")\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(1, 2, figsize=(10, 4), sharey=True)\n\ndf[df['iscustomer'] == 1]['patents'].hist(ax=ax[0], bins=10, color='skyblue')\nax[0].set_title('Customers')\nax[0].set_xlabel('Number of Patents')\n\ndf[df['iscustomer'] == 0]['patents'].hist(ax=ax[1], bins=10, color='salmon')\nax[1].set_title('Non-Customers')\nax[1].set_xlabel('Number of Patents')\n\nfig.suptitle('Histogram of Patents by Customer Status')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf.groupby('iscustomer')['patents'].mean()\n\n\niscustomer\n0    3.473013\n1    4.133056\nName: patents, dtype: float64\n\n\nCustomers tend to have more patents on average (4.13 vs. 3.47), and their distribution is more spread out, with more firms holding a higher number of patents. Non-customers are more concentrated at lower patent counts.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n\nCode\nimport seaborn as sns\n\nfig, axs = plt.subplots(1, 2, figsize=(12, 4))\n\nsns.countplot(data=df, x='region', hue='iscustomer', ax=axs[0])\naxs[0].set_title('Region by Customer Status')\naxs[0].set_xlabel('Region')\naxs[0].set_ylabel('Count')\naxs[0].legend(title='Customer')\n\nsns.kdeplot(data=df, x='age', hue='iscustomer', ax=axs[1], fill=True)\naxs[1].set_title('Age Distribution by Customer Status')\naxs[1].set_xlabel('Age')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf.groupby('iscustomer')['age'].mean()\n\n\niscustomer\n0    26.101570\n1    26.900208\nName: age, dtype: float64\n\n\nCustomers tend to be slightly older (average age 26.9 vs. 26.1). The regional distribution shows a clear difference: a much higher number of customers come from the Northeast, while other regions like the Midwest and Southwest are dominated by non-customers.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\n\nWe assume the number of patents \\(Y \\sim \\text{Poisson}(\\lambda)\\), with density:\n\\[\nf(Y \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^Y}{Y!}\n\\]\nFor a sample \\(Y_1, Y_2, \\dots, Y_n\\), the log-likelihood is:\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^n \\left( -\\lambda + Y_i \\log \\lambda - \\log(Y_i!) \\right)\n\\]\nLog-Likelihood Function in Python\n\n\nCode\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef poisson_log_likelihood(lmbda, y):\n    y = np.asarray(y)\n    return np.sum(-lmbda + y * np.log(lmbda) - gammaln(y + 1))\n\n\nPlotting the Log-Likelihood Curve\n\n\nCode\nimport matplotlib.pyplot as plt\n\ny = df['patents'].values\nlambdas = np.linspace(0.1, 10, 100)\nlogliks = [poisson_log_likelihood(lmbda, y) for lmbda in lambdas]\n\nplt.plot(lambdas, logliks)\nplt.xlabel(\"λ (lambda)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.title(\"Poisson Log-Likelihood for Different λ\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nThe analytical MLE for \\(\\lambda\\) is simply the sample mean of \\(Y\\), since:\n\\[\n\\hat{\\lambda}_{\\text{MLE}} = \\bar{Y}\n\\] Sample Mean\n\n\nCode\nybar = np.mean(y)\nybar\n\n\n3.6846666666666668\n\n\nNumerical Maximization Using scipy.optimize\n\n\nCode\nfrom scipy.optimize import minimize\n\ndef neg_log_likelihood(lmbda):\n    return -poisson_log_likelihood(lmbda[0], y)\n\nresult = minimize(neg_log_likelihood, x0=[1.0], bounds=[(0.001, None)])\nresult.x[0]\n\n\n3.6846662953477973\n\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n\nWe now extend our Poisson model to allow the rate of patent awards to depend on firm characteristics via:\n\\[\nY_i \\sim \\text{Poisson}(\\lambda_i), \\quad \\text{where } \\lambda_i = \\exp(X_i^\\top \\beta)\n\\]\nCovariates include: age, age squared, region (as dummies), and whether the firm is a Blueprinty customer.\n\n\n\n\nCode\nimport pandas as pd\nimport statsmodels.api as sm\n\ndf['age_squared'] = df['age'] ** 2\nX_df = pd.get_dummies(df[['age', 'age_squared', 'region', 'iscustomer']], drop_first=True)\nX_df = sm.add_constant(X_df)\n\nX_df = X_df.astype(float)\nX = X_df\ny = df['patents'].values\n\n\n\n\n\n\n\nCode\nimport statsmodels.api as sm\n\nmodel = sm.GLM(y, X, family=sm.families.Poisson())\nglm_result = model.fit()\n\nglm_result.summary()\n\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\ny\nNo. Observations:\n1500\n\n\nModel:\nGLM\nDf Residuals:\n1492\n\n\nModel Family:\nPoisson\nDf Model:\n7\n\n\nLink Function:\nLog\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-3258.1\n\n\nDate:\nFri, 16 May 2025\nDeviance:\n2143.3\n\n\nTime:\n16:51:38\nPearson chi2:\n2.07e+03\n\n\nNo. Iterations:\n5\nPseudo R-squ. (CS):\n0.1360\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n-0.5089\n0.183\n-2.778\n0.005\n-0.868\n-0.150\n\n\nage\n0.1486\n0.014\n10.716\n0.000\n0.121\n0.176\n\n\nage_squared\n-0.0030\n0.000\n-11.513\n0.000\n-0.003\n-0.002\n\n\niscustomer\n0.2076\n0.031\n6.719\n0.000\n0.147\n0.268\n\n\nregion_Northeast\n0.0292\n0.044\n0.669\n0.504\n-0.056\n0.115\n\n\nregion_Northwest\n-0.0176\n0.054\n-0.327\n0.744\n-0.123\n0.088\n\n\nregion_South\n0.0566\n0.053\n1.074\n0.283\n-0.047\n0.160\n\n\nregion_Southwest\n0.0506\n0.047\n1.072\n0.284\n-0.042\n0.143\n\n\n\n\n\n\n\n\n\n\nCode\nsummary_df = pd.DataFrame({\n    \"Coefficient\": glm_result.params,\n    \"Std. Error\": glm_result.bse,\n    \"z-value\": glm_result.tvalues,\n    \"p-value\": glm_result.pvalues\n})\n\nsummary_df\n\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\nz-value\np-value\n\n\n\n\nconst\n-0.508920\n0.183179\n-2.778269\n5.464935e-03\n\n\nage\n0.148619\n0.013869\n10.716250\n8.539597e-27\n\n\nage_squared\n-0.002970\n0.000258\n-11.513237\n1.131496e-30\n\n\niscustomer\n0.207591\n0.030895\n6.719179\n1.827509e-11\n\n\nregion_Northeast\n0.029170\n0.043625\n0.668647\n5.037205e-01\n\n\nregion_Northwest\n-0.017575\n0.053781\n-0.326782\n7.438327e-01\n\n\nregion_South\n0.056561\n0.052662\n1.074036\n2.828066e-01\n\n\nregion_Southwest\n0.050576\n0.047198\n1.071568\n2.839141e-01\n\n\n\n\n\n\n\n\n\n\nFirms that are Blueprinty customers are expected to produce about 23%(i.e. exp(0.2076)-1) more patents than non-customers, holding other factors constant. This effect is statistically significant.\nPatent output increases with firm age, but at a decreasing rate—suggesting older firms patent more, but the effect tapers off.\nRegional differences are not statistically significant, indicating little variation in patenting across regions once other firm characteristics are controlled for.\n\n\n\nWe create two fake datasets: - X_0 with iscustomer = 0 for all firms (as if no firm were a customer) - X_1 with iscustomer = 1 for all firms (as if all firms were customers)\nWe use the fitted model to compute predicted number of patents for each case, then take the difference.\n\n\nCode\nX_0 = X.copy()\nX_1 = X.copy()\n\nX_0.loc[:, X.columns.str.contains(\"iscustomer\")] = 0\nX_1.loc[:, X.columns.str.contains(\"iscustomer\")] = 1\n\n# Predict expected patent counts\ny_pred_0 = glm_result.predict(X_0)\ny_pred_1 = glm_result.predict(X_1)\n\n# Difference in predicted patents\ndiff = y_pred_1 - y_pred_0\ndiff.mean()\n\n\n0.7927680710452626\n\n\n\n\n\nOn average, firms are predicted to receive 0.79(i.e. 3.47 * 0.23) more patents over five years if they are Blueprinty customers, compared to if they are not—holding all other firm characteristics constant. This suggests a meaningful positive effect of the software on patenting success."
  },
  {
    "objectID": "homework/hw2_questions.html#airbnb-case-study",
    "href": "homework/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\n\nLoad and Clean Data\n\n\nCode\nimport pandas as pd\n\ndf = pd.read_csv(\"../data/airbnb.csv\")\n\n# Drop rows with missing values in relevant columns\ndf = df[['days', 'room_type', 'bathrooms', 'bedrooms', 'price', 'number_of_reviews',\n         'review_scores_cleanliness', 'review_scores_location', 'review_scores_value',\n         'instant_bookable']].dropna()\n\n# Convert instant_bookable to binary\ndf['instant_bookable'] = df['instant_bookable'].map({'t': 1, 'f': 0})\n\n\n\n\nExploratory Data Analysis\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Correlation heatmap for numeric features\nnumeric_cols = df.select_dtypes(include='number').columns\ncorr = df[numeric_cols].corr()\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(corr, annot=True, cmap='coolwarm')\nplt.title(\"Correlation Heatmap of Numeric Variables\")\nplt.show()\n\n# Dashboard 1: Scatter plots of numeric predictors vs number_of_reviews\nfig, axs = plt.subplots(1, 3, figsize=(18, 5))\nsns.scatterplot(x=df['days'], y=df['number_of_reviews'], ax=axs[0])\naxs[0].set_title(\"Reviews vs Days\")\n\nsns.scatterplot(x=df['price'], y=df['number_of_reviews'], ax=axs[1])\naxs[1].set_title(\"Reviews vs Price\")\n\nsns.scatterplot(x=df['review_scores_location'], y=df['number_of_reviews'], ax=axs[2])\naxs[2].set_title(\"Reviews vs Location Score\")\n\nplt.tight_layout()\nplt.show()\n\n# Dashboard 2: Boxplot of reviews by room type\nplt.figure(figsize=(6, 5))\nsns.boxplot(data=df, x='room_type', y='number_of_reviews')\nplt.title(\"Number of Reviews by Room Type\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReview score variables are correlated; don’t include all of them together.\nNumber of reviews is highly skewed — Poisson is reasonable, but check overdispersion.\nHigher prices tend to get fewer reviews; log-transforming price is helpful.\nReviews increase with location score, especially at high values.\nShared rooms get fewer reviews; room type matters and may interact with other variables.\n\n\n\nFit Poisson Regression Model\n\n\nCode\nimport statsmodels.api as sm\n\n# Log and squared transforms\ndf['log_price'] = np.log1p(df['price'])\ndf['log_days'] = np.log1p(df['days'])\n\n# Interaction terms\ndf['price_x_days'] = df['price'] * df['days']\ndf['bookable_x_room'] = df['instant_bookable'] * (df['room_type'] == 'Entire home/apt').astype(int)\n\n\n# Define predictors\nX = df[['log_days','log_price',\n        'review_scores_value', 'instant_bookable',\n        'price_x_days', 'bookable_x_room']]\n\n# Add room type dummies\nroom_dummies = pd.get_dummies(df[['room_type','bedrooms', 'bathrooms']], drop_first=True)\nX = pd.concat([X, room_dummies], axis=1)\n\n# Add intercept\nX = sm.add_constant(X)\nX = X.astype(float)\ny = df['number_of_reviews']\n\nmodel = sm.GLM(y, X, family=sm.families.Poisson())\nglm_result = model.fit()\nglm_result.summary()\n\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\nnumber_of_reviews\nNo. Observations:\n30160\n\n\nModel:\nGLM\nDf Residuals:\n30149\n\n\nModel Family:\nPoisson\nDf Model:\n10\n\n\nLink Function:\nLog\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-4.8964e+05\n\n\nDate:\nFri, 16 May 2025\nDeviance:\n8.5782e+05\n\n\nTime:\n16:51:39\nPearson chi2:\n1.22e+06\n\n\nNo. Iterations:\n7\nPseudo R-squ. (CS):\n0.9680\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n-1.0944\n0.027\n-40.254\n0.000\n-1.148\n-1.041\n\n\nlog_days\n0.5646\n0.002\n249.950\n0.000\n0.560\n0.569\n\n\nlog_price\n0.1275\n0.003\n39.293\n0.000\n0.121\n0.134\n\n\nreview_scores_value\n-0.0465\n0.001\n-33.716\n0.000\n-0.049\n-0.044\n\n\ninstant_bookable\n0.4976\n0.004\n124.468\n0.000\n0.490\n0.505\n\n\nprice_x_days\n-2.755e-07\n6.81e-09\n-40.463\n0.000\n-2.89e-07\n-2.62e-07\n\n\nbookable_x_room\n0.0230\n0.006\n3.971\n0.000\n0.012\n0.034\n\n\nbedrooms\n0.0774\n0.002\n37.365\n0.000\n0.073\n0.081\n\n\nbathrooms\n-0.1074\n0.004\n-28.239\n0.000\n-0.115\n-0.100\n\n\nroom_type_Private room\n0.0862\n0.004\n23.458\n0.000\n0.079\n0.093\n\n\nroom_type_Shared room\n0.0090\n0.009\n0.978\n0.328\n-0.009\n0.027\n\n\n\n\n\n\n\nShow Coefficients and Exponentiated Effects\n\n\nCode\ncoef_df = pd.DataFrame({\n    'Coefficient': glm_result.params,\n    'Exp(Coefficient)': np.exp(glm_result.params),\n    'p-value': glm_result.pvalues\n})\n\ncoef_df.sort_values('p-value')  # sort by significance\n\n\n\n\n\n\n\n\n\nCoefficient\nExp(Coefficient)\np-value\n\n\n\n\nconst\n-1.094405e+00\n0.334739\n0.000000e+00\n\n\nlog_days\n5.645793e-01\n1.758708\n0.000000e+00\n\n\nlog_price\n1.274563e-01\n1.135935\n0.000000e+00\n\n\ninstant_bookable\n4.975882e-01\n1.644750\n0.000000e+00\n\n\nprice_x_days\n-2.754803e-07\n1.000000\n0.000000e+00\n\n\nbedrooms\n7.738920e-02\n1.080463\n1.470202e-305\n\n\nreview_scores_value\n-4.651199e-02\n0.954553\n3.400566e-249\n\n\nbathrooms\n-1.074441e-01\n0.898127\n1.967010e-175\n\n\nroom_type_Private room\n8.620330e-02\n1.090028\n1.092157e-121\n\n\nbookable_x_room\n2.297251e-02\n1.023238\n7.168259e-05\n\n\nroom_type_Shared room\n9.039075e-03\n1.009080\n3.280893e-01\n\n\n\n\n\n\n\n\n\nInterpretation\nThe Poisson regression results show that several listing features significantly affect the expected number of reviews:\n\nLog Days has the strongest effect — older listings get substantially more reviews.\nLog Price is positively associated with review count, suggesting that higher-priced listings may attract more engagement.\nInstant Bookable listings receive about 64% more reviews, highlighting convenience as a key factor.\nBedrooms have a moderate positive effect, while bathrooms show a small but negative association.\nReview Score (Value) is negatively associated with reviews, which may reflect multicollinearity with other score metrics.\nRoom Type matters: private rooms receive slightly more reviews, but shared rooms are not significantly different from the baseline (entire home/apt).\nThe interaction terms (price × days, bookable × room) have statistically significant but small effects.\n\nOverall, the model fits well, and the most impactful predictors are listing age, instant bookability, and price."
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Miya(Mia) Huang",
    "section": "",
    "text": "Welcome!"
  },
  {
    "objectID": "homework/hw1_questions.html",
    "href": "homework/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nIn this field experiment, each donor in the sample was randomly assigned to receive one of several versions of a fundraising letter. The control group received the nonprofit’s standard appeal—a typical letter asking for support, with no mention of any special incentives.\nThe treatment group, on the other hand, received letters that included a matching grant offer. These letters stated that a concerned member of the organization would match the recipient’s donation at a fixed rate. Every dollar donated would be matched immediately, increasing the total contribution the organization would receive.\nThe treatment letters varied across three key dimensions:\n\nMatching ratio\nDonors were randomly assigned to receive a match offer of 1:1, 2:1, or 3:1, meaning the organization would receive $2, $3, or $4 for every $1 donated.\nMaximum match amount\nThe total available match funding was stated as either $25,000, $50,000, $100,000, or left unstated.\nSuggested donation amount\nEach letter included a reply card with three suggested gift levels based on the donor’s previous highest contribution: the same amount, 1.25×, or 1.5×. One of these was used as an example in the match statement.\n\nAside from these randomized variations, all letters were identical in format, tone, and content. This design allowed the researchers to isolate the effect of the matching grant mechanism and its specific features on donor behavior.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "homework/hw1_questions.html#introduction",
    "href": "homework/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nIn this field experiment, each donor in the sample was randomly assigned to receive one of several versions of a fundraising letter. The control group received the nonprofit’s standard appeal—a typical letter asking for support, with no mention of any special incentives.\nThe treatment group, on the other hand, received letters that included a matching grant offer. These letters stated that a concerned member of the organization would match the recipient’s donation at a fixed rate. Every dollar donated would be matched immediately, increasing the total contribution the organization would receive.\nThe treatment letters varied across three key dimensions:\n\nMatching ratio\nDonors were randomly assigned to receive a match offer of 1:1, 2:1, or 3:1, meaning the organization would receive $2, $3, or $4 for every $1 donated.\nMaximum match amount\nThe total available match funding was stated as either $25,000, $50,000, $100,000, or left unstated.\nSuggested donation amount\nEach letter included a reply card with three suggested gift levels based on the donor’s previous highest contribution: the same amount, 1.25×, or 1.5×. One of these was used as an example in the match statement.\n\nAside from these randomized variations, all letters were identical in format, tone, and content. This design allowed the researchers to isolate the effect of the matching grant mechanism and its specific features on donor behavior.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "homework/hw1_questions.html#data",
    "href": "homework/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\nThe dataset consists of 50,083 observations and 51 variables, representing prior donors who were part of a large-scale direct mail fundraising experiment. Each row corresponds to an individual who received a solicitation letter, and the variables capture both treatment assignments and individual characteristics.\nThe bar plots show the distribution of key categorical variables in the dataset. The sample is unbalanced across treatment groups by design, with approximately two-thirds assigned to treatment. Only 2.1% of individuals donated, indicating a low response rate. Most of the sample resides in blue states (59.5%), and the majority of donors are male (70.6%) and not donating as a couple (88.7%).\nThe histograms of numeric variables show that both donation-related and historical giving behaviors are highly skewed to the right. Most donation amounts are under $100, and both highest past donation and frequency of past donations exhibit long right tails, suggesting that a small subset of donors account for a disproportionately large share of past giving. The recency variable (mrm2) also shows many donors who haven’t contributed recently, with the most frequent values clustered between 0 and 12 months.\n\n\nCode\nimport pandas as pd\ndf = pd.read_stata(\"../data/karlan_list_2007.dta\")\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# categorical\ncategorical_vars = ['treatment', 'gave', 'red0', 'female', 'couple']\nlabel_map = {\n    'treatment': {0: 'Control', 1: 'Treatment'},\n    'gave': {0: 'Did Not Give', 1: 'Gave'},\n    'red0': {0: 'Blue State', 1: 'Red State'},\n    'female': {0: 'Male', 1: 'Female'},\n    'couple': {0: 'Single', 1: 'Couple'}\n}\n\nfig, axes = plt.subplots(2, 3, figsize=(16, 8))\naxes = axes.flatten()\n\nfor i, var in enumerate(categorical_vars):\n    ax = axes[i]\n    counts = df[var].value_counts().sort_index()\n    labels = list(label_map[var].values())\n    bars = ax.bar(labels, counts.values, color=sns.color_palette(\"pastel\"))\n\n    for bar, count in zip(bars, counts.values):\n        pct = count / df.shape[0]\n        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height(), f'{pct:.1%}',\n                ha='center', va='bottom', fontsize=9)\n\n    ax.set_title(f\"{var.capitalize()} Distribution\")\n    ax.set_ylabel(\"Count\")\n\nif len(categorical_vars) &lt; len(axes):\n    fig.delaxes(axes[-1])\n\nplt.tight_layout()\nplt.show()\n\n# numeric\nnumeric_vars = ['amount', 'hpa', 'mrm2', 'freq']\ntitles = {\n    'amount': 'Donation Amount',\n    'hpa': 'Highest Past Donation',\n    'mrm2': 'Months Since Last Donation',\n    'freq': 'Donation Frequency'\n}\n\nxlims = {\n    'amount': (0, 200),\n    'hpa': (0, 250),\n    'mrm2': (0, 60),\n    'freq': (0, 50)\n}\n\nfig, axes = plt.subplots(2, 2, figsize=(12, 6))\naxes = axes.flatten()\n\nfor i, var in enumerate(numeric_vars):\n    ax = axes[i]\n    data = df[df['gave'] == 1][var] if var == 'amount' else df[var]\n    \n    sns.histplot(data, bins=30, kde=True, ax=ax, color='skyblue')\n    ax.set_title(titles[var])\n    ax.set_xlabel(var)\n    ax.set_ylabel(\"Frequency\")\n    ax.set_xlim(xlims[var])\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\n\nCode\nimport pandas as pd\ndf = pd.read_stata(\"../data/karlan_list_2007.dta\")  \n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport statsmodels.api as sm\nfrom scipy.stats import t as t_dist\n\n# Variables\ntest_vars = ['mrm2', 'hpa', 'freq']\nvar_labels = {\n    'mrm2': 'Months Since Last Donation',\n    'hpa': 'Highest Past Donation',\n    'freq': 'Donation Frequency'\n}\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\naxes = axes.flatten()\n\nfor i, var in enumerate(test_vars):\n    means = [\n        df[df['treatment'] == 0][var].mean(),\n        df[df['treatment'] == 1][var].mean()\n    ]\n    labels = ['Control', 'Treatment']\n    colors = ['#6baed6', '#fd8d3c']\n\n    bars = axes[i].bar(labels, means, color=colors)\n    axes[i].set_title(var_labels[var])\n    axes[i].set_ylabel(\"Mean\")\n\n    # Add value labels\n    for bar, mean in zip(bars, means):\n        axes[i].text(bar.get_x() + bar.get_width()/2, bar.get_height(),\n                     f\"{mean:.2f}\", ha='center', va='bottom', fontsize=9)\n\nplt.tight_layout()\nplt.show()\n\n# t-test and regression comparison\nresults = []\n\nfor var in test_vars:\n    x1 = df[df[\"treatment\"] == 1][var].dropna()\n    x0 = df[df[\"treatment\"] == 0][var].dropna()\n\n    mean1 = x1.mean()\n    mean0 = x0.mean()\n    diff = mean1 - mean0\n    var1 = x1.var(ddof=1)\n    var0 = x0.var(ddof=1)\n    n1 = len(x1)\n    n0 = len(x0)\n\n    se = np.sqrt(var1 / n1 + var0 / n0)\n    t_stat = diff / se\n\n    df_denom = (var1/n1 + var0/n0)**2 / ((var1**2 / (n1**2 * (n1 - 1))) + (var0**2 / (n0**2 * (n0 - 1))))\n    ttest_p = 2 * (1 - t_dist.cdf(abs(t_stat), df=df_denom))\n\n    X = sm.add_constant(df[['treatment']])\n    model = sm.OLS(df[var], X, missing='drop').fit()\n    reg_t = model.tvalues['treatment']\n    reg_p = model.pvalues['treatment']\n\n    results.append({\n        'Variable': var,\n        'Mean_Treatment': round(mean1, 3),\n        'Mean_Control': round(mean0, 3),\n        'Difference': round(diff, 3),\n        'T-test t': round(t_stat, 3),\n        'T-test p': round(ttest_p, 4),\n        'Reg t': round(reg_t, 3),\n        'Reg p-value': round(reg_p, 4)\n    })\n\npd.DataFrame(results)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nMean_Treatment\nMean_Control\nDifference\nT-test t\nT-test p\nReg t\nReg p-value\n\n\n\n\n0\nmrm2\n13.012\n12.998000\n0.014\n0.120\n0.9049\n0.119\n0.9049\n\n\n1\nhpa\n59.597\n58.959999\n0.637\n0.970\n0.3318\n0.944\n0.3451\n\n\n2\nfreq\n8.035\n8.047000\n-0.012\n-0.111\n0.9117\n-0.111\n0.9117\n\n\n\n\n\n\n\nThe bar plots visually confirm that the treatment and control groups are nearly identical in their pre-treatment characteristics. There is no meaningful difference in the average months since last donation, highest past donation, or donation frequency.\nStatistical tests further support this conclusion. Both t-tests (using the class formula) and simple regressions yield non-significant results at the 95% confidence level, with very small t-statistics and large p-values across all variables.\nThese findings align with Table 1 in Karlan and List (2007), providing strong evidence that the random assignment was successfully implemented and that treatment and control groups are balanced in observable covariates. This supports the internal validity of any subsequent causal analysis."
  },
  {
    "objectID": "homework/hw1_questions.html#experimental-results",
    "href": "homework/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf = pd.read_stata(\"../data/karlan_list_2007.dta\")\n\ndonation_rates = df.groupby('treatment')['gave'].mean()\nlabels = ['Control', 'Treatment']\ncolors = ['#6baed6', '#fd8d3c']\n\n# Plot\nfig, ax = plt.subplots(figsize=(6, 4))\nbars = ax.bar(labels, donation_rates.values, color=colors)\n\nfor bar, rate in zip(bars, donation_rates.values):\n    ax.text(bar.get_x() + bar.get_width() / 2,\n            bar.get_height() + 0.002,\n            f\"{rate:.2%}\", ha='center', va='bottom', fontsize=10)\n\nax.set_ylabel(\"Proportion Donated\")\nax.set_title(\"Donation Rate by Treatment Group\")\nplt.ylim(0, max(donation_rates.values) + 0.01)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe treatment group has a higher donation rate (2.20%) than the control group (1.79%), suggesting a positive effect of the matching offer on donor response.\n\n\nCode\nimport pandas as pd\ndf = pd.read_stata(\"../data/karlan_list_2007.dta\")\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom scipy.stats import t as t_dist\n\ny_treat = df[df[\"treatment\"] == 1][\"gave\"]\ny_ctrl = df[df[\"treatment\"] == 0][\"gave\"]\n\nmean_treat = y_treat.mean()\nmean_ctrl = y_ctrl.mean()\ndiff = mean_treat - mean_ctrl\nn_treat = len(y_treat)\nn_ctrl = len(y_ctrl)\nvar_treat = y_treat.var(ddof=1)\nvar_ctrl = y_ctrl.var(ddof=1)\n\nse = np.sqrt(var_treat / n_treat + var_ctrl / n_ctrl)\nt_stat = diff / se\ndf_denom = (var_treat/n_treat + var_ctrl/n_ctrl)**2 / (\n    (var_treat**2 / (n_treat**2 * (n_treat - 1))) +\n    (var_ctrl**2 / (n_ctrl**2 * (n_ctrl - 1)))\n)\np_val = 2 * (1 - t_dist.cdf(abs(t_stat), df=df_denom))\n\nX = sm.add_constant(df[\"treatment\"])\nmodel = sm.OLS(df[\"gave\"], X, missing ='drop').fit()\n\n# results\nprint(\"T-test result:\")\nprint(f\"  • Mean difference in donation rate (Treatment - Control): { round(diff, 4)}\")\nprint(f\"  • t-statistic: {round(t_stat, 3)}\")\nprint(f\"  • p-value: {round(p_val, 4)}\")\n\nprint(\"\\nLinear regression result:\")\nprint(f\"  • Coefficient on treatment: {round(model.params['treatment'], 4)}\")\nprint(f\"  • t-statistic: {round(model.tvalues['treatment'], 3)}\")\nprint(f\"  • p-value: {round(model.pvalues['treatment'], 4)}\")\n\n\nT-test result:\n  • Mean difference in donation rate (Treatment - Control): 0.0042\n  • t-statistic: 3.209\n  • p-value: 0.0013\n\nLinear regression result:\n  • Coefficient on treatment: 0.0042\n  • t-statistic: 3.101\n  • p-value: 0.0019\n\n\nBoth the t-test and the linear regression show that the treatment group had a higher donation rate than the control group, and this difference is statistically significant at the 1% level. Specifically, receiving a matching grant offer increases the probability of donating by about 0.42 percentage points—rising from roughly 1.79% in the control group to 2.21% in the treatment group. While this absolute change may appear small, it represents a relative increase of over 20%, which is substantial given the typically low baseline response rate in charitable giving.\nTo further support this finding, I estimate a probit model where the outcome is whether a donation was made and the explanatory variable is treatment assignment. The coefficient on treatment is 0.087, which is also statistically significant at the 1% level. Although the coefficient cannot be interpreted directly as a probability, it indicates an increase in the latent propensity to donate among individuals who received the matching message.\nTaken together, the results provide consistent evidence that even modest psychological framing—such as mentioning that a donation will be matched—can meaningfully increase donor response.\n\n\nCode\nimport pandas as pd\ndf = pd.read_stata(\"../data/karlan_list_2007.dta\")\n\nfrom statsmodels.discrete.discrete_model import Probit\nimport statsmodels.api as sm\n\nX = sm.add_constant(df['treatment'])\nprobit_model = Probit(df['gave'], X).fit()\n\nprint(probit_model.summary())\n\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n                          Probit Regression Results                           \n==============================================================================\nDep. Variable:                   gave   No. Observations:                50083\nModel:                         Probit   Df Residuals:                    50081\nMethod:                           MLE   Df Model:                            1\nDate:                Fri, 16 May 2025   Pseudo R-squ.:               0.0009783\nTime:                        16:51:44   Log-Likelihood:                -5030.5\nconverged:                       True   LL-Null:                       -5035.4\nCovariance Type:            nonrobust   LLR p-value:                  0.001696\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -2.1001      0.023    -90.073      0.000      -2.146      -2.054\ntreatment      0.0868      0.028      3.113      0.002       0.032       0.141\n==============================================================================\n\n\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\nI conduct three pairwise t-tests comparing donation rates between match ratios (2:1 vs. 1:1, 3:1 vs. 1:1, and 3:1 vs. 2:1). The differences in mean donation rates across these groups are all very small and none are statistically significant at the 5% level. All p-values are well above 0.30. This suggests that the presence of a match itself may be sufficient to motivate behavior, while increasing the match ratio beyond 1:1 offers little incremental benefit.\n\n\nCode\nimport pandas as pd\ndf = pd.read_stata(\"../data/karlan_list_2007.dta\")\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import t as t_dist\n\ntreatment_df = df[df[\"treatment\"] == 1]\n\nratios = {\n    1: treatment_df[treatment_df[\"ratio\"] == 1][\"gave\"],\n    2: treatment_df[treatment_df[\"ratio\"] == 2][\"gave\"],\n    3: treatment_df[treatment_df[\"ratio\"] == 3][\"gave\"]\n}\n\n# t-test\ndef manual_ttest(group1, group2):\n    mean_diff = group1.mean() - group2.mean()\n    se = np.sqrt(group1.var(ddof=1)/len(group1) + group2.var(ddof=1)/len(group2))\n    t_stat = mean_diff / se\n    df_denom = (group1.var(ddof=1)/len(group1) + group2.var(ddof=1)/len(group2))**2 / (\n        (group1.var(ddof=1)**2 / ((len(group1)**2)*(len(group1)-1))) + \n        (group2.var(ddof=1)**2 / ((len(group2)**2)*(len(group2)-1)))\n    )\n    p_val = 2 * (1 - t_dist.cdf(abs(t_stat), df=df_denom))\n    return round(mean_diff, 4), round(t_stat, 3), round(p_val, 4)\n\nresults = []\nfor high, low in [(2, 1), (3, 1), (3, 2)]:\n    diff, t_stat, p_val = manual_ttest(ratios[high], ratios[low])\n    results.append({\n        \"Comparison\": f\"{high}:1 vs {low}:1\",\n        \"Mean Diff\": diff,\n        \"T-stat\": t_stat,\n        \"P-value\": p_val\n    })\n\npd.DataFrame(results)\n\n\n\n\n\n\n\n\n\nComparison\nMean Diff\nT-stat\nP-value\n\n\n\n\n0\n2:1 vs 1:1\n0.0019\n0.965\n0.3345\n\n\n1\n3:1 vs 1:1\n0.0020\n1.015\n0.3101\n\n\n2\n3:1 vs 2:1\n0.0001\n0.050\n0.9600\n\n\n\n\n\n\n\nTo determine whether higher match ratios influence the likelihood of donating, I regress the binary outcome gave on the categorical variable ratio, using only the treatment group. The 1:1 match ratio serves as the reference category, and the model estimates how donation rates differ under 2:1 and 3:1 matching offers.\n\n\nCode\nimport pandas as pd\ndf = pd.read_stata(\"../data/karlan_list_2007.dta\")\n\ntreat_df = df[df[\"ratio\"].isin([1, 2, 3])].copy()\n\ntreat_df[\"ratio\"] = treat_df[\"ratio\"].astype(\"category\")\n\nimport statsmodels.formula.api as smf\nmodel = smf.ols(\"gave ~ C(ratio, Treatment(reference=1))\", data=treat_df).fit()\n\nparams = model.params\npvals = model.pvalues\n\nprint(\"Linear regression comparing match ratios (baseline: 1:1):\")\nprint(f\"  • Intercept (1:1 match): {params['Intercept']:.4f}\")\nprint(f\"  • 2:1 vs 1:1 match: Coef = {params['C(ratio, Treatment(reference=1))[T.2]']:.4f}, p = {pvals['C(ratio, Treatment(reference=1))[T.2]']:.4f}\")\nprint(f\"  • 3:1 vs 1:1 match: Coef = {params['C(ratio, Treatment(reference=1))[T.3]']:.4f}, p = {pvals['C(ratio, Treatment(reference=1))[T.3]']:.4f}\")\n\n\nLinear regression comparing match ratios (baseline: 1:1):\n  • Intercept (1:1 match): 0.0207\n  • 2:1 vs 1:1 match: Coef = 0.0019, p = 0.3383\n  • 3:1 vs 1:1 match: Coef = 0.0020, p = 0.3133\n\n\nThe estimated coefficients for ratio2 and ratio3 are positive, suggesting slightly higher donation rates compared to the 1:1 baseline. However, both coefficients are not statistically significant (p-values &gt; 0.3), indicating that the observed differences could be due to random variation rather than a true treatment effect.\nTo assess whether increasing the match ratio leads to higher response, I compared donation rates directly from the data and from the regression coefficients.\n\n\nCode\nimport pandas as pd\ndf = pd.read_stata(\"../data/karlan_list_2007.dta\")\n\nrate_1 = df[(df[\"treatment\"] == 1) & (df[\"ratio\"] == 1)][\"gave\"].mean()\nrate_2 = df[(df[\"treatment\"] == 1) & (df[\"ratio\"] == 2)][\"gave\"].mean()\nrate_3 = df[(df[\"treatment\"] == 1) & (df[\"ratio\"] == 3)][\"gave\"].mean()\n\ndiff_2_vs_1_data = rate_2 - rate_1\ndiff_3_vs_2_data = rate_3 - rate_2\n\ncoef_2 = model.params['C(ratio, Treatment(reference=1))[T.2]']\ncoef_3 = model.params['C(ratio, Treatment(reference=1))[T.3]']\ndiff_3_vs_2_coef = coef_3 - coef_2\n\nprint(\"Donation rate differences (direct from data):\")\nprint(f\"  • 2:1 vs 1:1: {round(diff_2_vs_1_data, 4)}\")\nprint(f\"  • 3:1 vs 2:1: {round(diff_3_vs_2_data, 4)}\")\n\nprint(\"\\nEstimated difference from regression coefficients:\")\nprint(f\"  • 2:1 vs 1:1: {round(coef_2, 4)}\")\nprint(f\"  • 3:1 vs 2:1: {round(diff_3_vs_2_coef, 4)}\")\n\n\nDonation rate differences (direct from data):\n  • 2:1 vs 1:1: 0.0019\n  • 3:1 vs 2:1: 0.0001\n\nEstimated difference from regression coefficients:\n  • 2:1 vs 1:1: 0.0019\n  • 3:1 vs 2:1: 0.0001\n\n\nMoving from a 1:1 to a 2:1 match produced a small increase of 0.19%, while the difference between 3:1 and 2:1 was nearly zero (0.01%). These results were mirrored exactly in the regression estimates.\nIn practical terms, offering a match matters, but increasing the match size does not appear to further improve donation response. For fundraisers, this suggests that a basic 1:1 match may be just as effective as higher ratios, but at a lower cost.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\nI regress donation amount on the match ratio using the treatment group, with 1:1 as the reference group. The baseline average donation under 1:1 is $0.94. Compared to this:\n\nThe 2:1 group gives $0.09 more (p = 0.46)\nThe 3:1 group gives almost the same (p = 0.99)\n\nThis suggests that higher match ratios do not meaningfully affect how much people donate, even if they influence whether they donate at all.\n\n\nCode\nimport pandas as pd\ndf = pd.read_stata(\"../data/karlan_list_2007.dta\")\n\nimport statsmodels.formula.api as smf\n\namount_df = df[(df[\"treatment\"] == 1) & (df[\"ratio\"].isin([1, 2, 3]))].copy()\n\n\namount_df[\"ratio\"] = amount_df[\"ratio\"].astype(\"category\")\namount_df[\"ratio\"] = amount_df[\"ratio\"].cat.remove_unused_categories()\n\nmodel_amount = smf.ols(\"amount ~ C(ratio, Treatment(reference=1))\", data=amount_df).fit()\n\nmodel_amount.summary()\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\namount\nR-squared:\n0.000\n\n\nModel:\nOLS\nAdj. R-squared:\n-0.000\n\n\nMethod:\nLeast Squares\nF-statistic:\n0.3651\n\n\nDate:\nFri, 16 May 2025\nProb (F-statistic):\n0.694\n\n\nTime:\n16:51:44\nLog-Likelihood:\n-1.2063e+05\n\n\nNo. Observations:\n33396\nAIC:\n2.413e+05\n\n\nDf Residuals:\n33393\nBIC:\n2.413e+05\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n0.9367\n0.085\n11.026\n0.000\n0.770\n1.103\n\n\nC(ratio, Treatment(reference=1))[T.2]\n0.0895\n0.120\n0.745\n0.456\n-0.146\n0.325\n\n\nC(ratio, Treatment(reference=1))[T.3]\n0.0011\n0.120\n0.009\n0.993\n-0.234\n0.237\n\n\n\n\n\n\n\n\nOmnibus:\n64921.717\nDurbin-Watson:\n2.010\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n172634606.522\n\n\nSkew:\n15.430\nProb(JB):\n0.00\n\n\nKurtosis:\n353.872\nCond. No.\n3.73\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n I regress donation amount on the match ratio using only individuals who made a donation. The average donation in the 1:1 group is $45.14. Compared to this:\n\nThe 2:1 group gives $0.19 more (p = 0.96)\nThe 3:1 group gives $3.89 less (p = 0.31)\n\nThese differences are small and statistically insignificant. This suggests that once someone decides to donate, the size of the match does not influence how much they give.\n\n\nCode\nimport pandas as pd\ndf = pd.read_stata(\"../data/karlan_list_2007.dta\")\n\nimport statsmodels.formula.api as smf\n\namount_df = df[(df[\"treatment\"] == 1) & (df[\"amount\"] &gt; 0) & (df[\"ratio\"].isin([1, 2, 3]))].copy()\n\namount_df[\"ratio\"] = amount_df[\"ratio\"].astype(\"category\")\namount_df[\"ratio\"] = amount_df[\"ratio\"].cat.remove_unused_categories()\n\nmodel_amount = smf.ols(\"amount ~ C(ratio, Treatment(reference=1))\", data=amount_df).fit()\n\nmodel_amount.summary()\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\namount\nR-squared:\n0.002\n\n\nModel:\nOLS\nAdj. R-squared:\n-0.001\n\n\nMethod:\nLeast Squares\nF-statistic:\n0.7504\n\n\nDate:\nFri, 16 May 2025\nProb (F-statistic):\n0.473\n\n\nTime:\n16:51:44\nLog-Likelihood:\n-3794.3\n\n\nNo. Observations:\n736\nAIC:\n7595.\n\n\nDf Residuals:\n733\nBIC:\n7608.\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n45.1429\n2.765\n16.324\n0.000\n39.714\n50.572\n\n\nC(ratio, Treatment(reference=1))[T.2]\n0.1944\n3.829\n0.051\n0.960\n-7.322\n7.711\n\n\nC(ratio, Treatment(reference=1))[T.3]\n-3.8911\n3.825\n-1.017\n0.309\n-11.400\n3.618\n\n\n\n\n\n\n\n\nOmnibus:\n458.865\nDurbin-Watson:\n1.918\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n5192.758\n\n\nSkew:\n2.659\nProb(JB):\n0.00\n\n\nKurtosis:\n14.876\nCond. No.\n3.83\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n The histograms display the distribution of donation amounts among individuals who donated, separated by treatment and control groups. Both distributions are heavily right-skewed, with most donations concentrated under $100.\nThe treatment group’s average donation is $43.87, while the control group’s average is slightly higher at $45.54. Despite the visual and numerical difference, the gap is small and not statistically significant, reinforcing earlier regression results that match offers influence whether people give, but not how much they give once they’ve decided to donate.\n\n\nCode\nimport pandas as pd\ndf = pd.read_stata(\"../data/karlan_list_2007.dta\")\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndonors = df[df[\"amount\"] &gt; 0]\n\ndonors_treat = donors[donors[\"treatment\"] == 1]\ndonors_ctrl = donors[donors[\"treatment\"] == 0]\n\nmean_treat = donors_treat[\"amount\"].mean()\nmean_ctrl = donors_ctrl[\"amount\"].mean()\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\n\n# treatment group\nsns.histplot(donors_treat[\"amount\"], bins=30, ax=axes[0], color='skyblue')\naxes[0].axvline(mean_treat, color='red', linestyle='--')  # Add vertical line for mean\naxes[0].set_title(\"Treatment\")\naxes[0].set_xlabel(\"Donation Amount\")\naxes[0].set_ylabel(\"Number of Donors\")\naxes[0].text(mean_treat + 2, axes[0].get_ylim()[1]*0.9, f\"Mean = ${mean_treat:.2f}\", color='red')\n\n# control group\nsns.histplot(donors_ctrl[\"amount\"], bins=30, ax=axes[1], color='lightgreen')\naxes[1].axvline(mean_ctrl, color='red', linestyle='--')\naxes[1].set_title(\"Control\")\naxes[1].set_xlabel(\"Donation Amount\")\naxes[1].text(mean_ctrl + 2, axes[1].get_ylim()[1]*0.9, f\"Mean = ${mean_ctrl:.2f}\", color='red')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "homework/hw1_questions.html#simulation-experiment",
    "href": "homework/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nThe plot shows the cumulative average difference in donation amounts between the treatment and control groups, based on 10,000 simulated donor pairs. Initially, the cumulative average fluctuates substantially, reflecting noise in small samples. As the number of simulated pairs increases, the cumulative average steadily converges toward the true mean difference (indicated by the red dashed line).\nThis pattern illustrates the Law of Large Numbers: with enough data, the average of simulated differences approximates the population-level treatment effect. In this case, the cumulative average stabilizes around the true mean difference, providing visual confirmation that our estimate becomes more reliable as sample size increases.\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndf = pd.read_stata(\"../data/karlan_list_2007.dta\")\nnp.random.seed(123)\n\ncontrol_data = df[(df[\"treatment\"] == 0) & (df[\"amount\"] &gt; 0)][\"amount\"]\ntreat_data = df[(df[\"treatment\"] == 1) & (df[\"amount\"] &gt; 0)][\"amount\"]\n\n# simulation\ncontrol_draws = np.random.choice(control_data, size=100000, replace=True)\ntreat_draws = np.random.choice(treat_data, size=10000, replace=True)\n\ndiffs = treat_draws - control_draws[:10000]\ncum_avg_diff = np.cumsum(diffs) / np.arange(1, len(diffs) + 1)\ntrue_diff = treat_data.mean() - control_data.mean()\n\n# Plot\nplt.figure(figsize=(10, 5))\nplt.plot(cum_avg_diff, label=\"Cumulative Avg. Difference\", color='steelblue', linewidth=2)\nplt.axhline(y=true_diff, color='crimson', linestyle='--', linewidth=2, label=\"True Mean Difference\")\n\nplt.xlabel(\"Number of Simulated Pairs\", fontsize=12)\nplt.ylabel(\"Cumulative Average (Treatment - Control)\", fontsize=12)\nplt.title(\"Cumulative Average of Donation Differences\\nSimulated from Treatment and Control Groups\", fontsize=14, fontweight='bold')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.xlim(0, 10000)\nplt.ylim(-20, 20)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCentral Limit Theorem\nThese histograms show the distribution of simulated mean donation differences (treatment minus control) across 1000 replications, at four different sample sizes.\nAs the sample size increases, the distributions become narrower and more centered. This reflects a reduction in uncertainty—with small samples, estimates of the mean difference vary widely due to random noise. As the number of observations grows, the law of large numbers ensures that sample estimates converge toward the true underlying difference, and the variability across simulations shrinks.\nAt all sample sizes, the distributions tend to center slightly below zero, which is consistent with the observed data where treatment donors gave slightly less than control donors, on average. While the difference is small and not statistically significant, the simulation reinforces that larger sample sizes produce more stable and precise estimates, even when the underlying effect is near zero.\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndf = pd.read_stata(\"../data/karlan_list_2007.dta\")\nnp.random.seed(123)\n\ncontrol_data = df[(df[\"treatment\"] == 0) & (df[\"amount\"] &gt; 0)][\"amount\"].values\ntreat_data = df[(df[\"treatment\"] == 1) & (df[\"amount\"] &gt; 0)][\"amount\"].values\n\nsample_sizes = [50, 200, 500, 1000]\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\naxes = axes.flatten()\n\nfor i, n in enumerate(sample_sizes):\n    diffs = []\n    for _ in range(1000):\n        treat_sample = np.random.choice(treat_data, size=n, replace=True)\n        ctrl_sample = np.random.choice(control_data, size=n, replace=True)\n        diffs.append(treat_sample.mean() - ctrl_sample.mean())\n    \n    ax = axes[i]\n    ax.hist(diffs, bins=30, color='skyblue', edgecolor='white')\n    ax.axvline(0, color='red', linestyle='--', linewidth=2)\n    ax.set_title(f\"Sample Size = {n}\")\n    ax.set_xlabel(\"Difference in Mean Donation (Treatment - Control)\")\n    ax.set_ylabel(\"Frequency\")\n\nplt.suptitle(\"Distribution of Simulated Mean Differences\", fontsize=16, fontweight='bold')\nplt.tight_layout(rect=[0, 0, 1, 0.96])\nplt.show()"
  },
  {
    "objectID": "homework/hw4_questions.html",
    "href": "homework/hw4_questions.html",
    "title": "Unsupervised and Supervised Machine Learning: From K-Means to Key Drivers",
    "section": "",
    "text": "In this blog post, I explore two fundamental tasks in machine learning—unsupervised clustering and supervised driver analysis—using Python and real-world marketing datasets.\nThe first part focuses on implementing the K-Means algorithm from scratch, applying it to the well-known Palmer Penguins dataset. Through step-by-step visualizations, I illustrate how the algorithm identifies natural groupings based on body measurements, and evaluate the clustering quality using WCSS and silhouette scores.\nIn the second part, I turn to key driver analysis using a synthetic customer satisfaction dataset. I replicate and expand a benchmark comparison from class that evaluates variable importance using six distinct methods: Pearson correlation, standardized regression coefficients, “usefulness” (change in R²), Shapley values (LMG), Johnson’s relative weights, and Mean Decrease in Gini from a random forest model.\nTogether, these two analyses reflect both the structure-finding and insight-generating power of machine learning in marketing analytics."
  },
  {
    "objectID": "homework/hw4_questions.html#visualizing-k-means-clustering-penguins-from-scratch",
    "href": "homework/hw4_questions.html#visualizing-k-means-clustering-penguins-from-scratch",
    "title": "Unsupervised and Supervised Machine Learning: From K-Means to Key Drivers",
    "section": "Visualizing K-Means: Clustering Penguins from Scratch",
    "text": "Visualizing K-Means: Clustering Penguins from Scratch\n\nWhat is K-Means?\nK-Means is an unsupervised machine learning algorithm used for discovering clusters in data based on similarity (typically using Euclidean distance). It is widely used in marketing segmentation, image compression, and pattern recognition.\nThe goal is to partition data into K groups such that data points within each cluster are as close as possible to each other, and as far as possible from points in other clusters.\n\n\nMathematical Logic\nThis section introduces the mathematical foundation of the K-Means clustering algorithm, which is one of the most popular unsupervised learning methods for grouping similar observations. The goal of K-Means is to assign \\(n\\) observations into \\(K\\) distinct, non-overlapping clusters based on feature similarity, in such a way that the within-cluster variation is minimized.\nSuppose we have \\(n\\) observations, each represented by a \\(p\\)-dimensional feature vector \\(\\mathbf{x}_i \\in \\mathbb{R}^p\\), for \\(i = 1, \\ldots, n\\). We want to partition the observations into \\(K\\) clusters, denoted as \\(C_1, \\ldots, C_K\\), such that:\n\nEvery observation belongs to exactly one cluster: \\(\\bigcup_{k=1}^K C_k = \\{1, \\ldots, n\\}\\)\nNo overlap between clusters: \\(C_k \\cap C_{k'} = \\emptyset\\) for all \\(k \\ne k'\\)\n\nThe objective of K-Means is to minimize the total within-cluster sum of squared Euclidean distances, defined as:\n\\[\n\\text{WCSS} = \\sum_{k=1}^K \\sum_{i \\in C_k} \\| \\mathbf{x}_i - \\boldsymbol{\\mu}_k \\|^2\n\\]\nwhere: - \\(C_k\\) is the set of indices for points assigned to cluster \\(k\\) - \\(\\boldsymbol{\\mu}_k\\) is the centroid (mean vector) of cluster \\(k\\), computed as:\n\\[\n\\boldsymbol{\\mu}_k = \\frac{1}{|C_k|} \\sum_{i \\in C_k} \\mathbf{x}_i\n\\]\nIn words, the WCSS measures how tightly grouped the points in each cluster are around their respective cluster centers.\n\nK-Means Algorithm (Iterative Optimization)\nK-Means uses a simple iterative algorithm to find a local minimum of the WCSS objective:\n\nInitialize: Randomly select \\(K\\) observations as initial cluster centroids \\(\\boldsymbol{\\mu}_1, \\ldots, \\boldsymbol{\\mu}_K\\).\nAssign Step: For each observation \\(\\mathbf{x}_i\\), assign it to the nearest cluster centroid using Euclidean distance:\n\n\\[\n\\text{Cluster}(\\mathbf{x}_i) = \\arg\\min_{k \\in \\{1, \\ldots, K\\}} \\| \\mathbf{x}_i - \\boldsymbol{\\mu}_k \\|^2\n\\]\n\nUpdate Step: For each cluster \\(k\\), recompute its centroid as the mean of all points currently assigned to it:\n\n\\[\n\\boldsymbol{\\mu}_k^{\\text{new}} = \\frac{1}{|C_k|} \\sum_{\\mathbf{x}_i \\in C_k} \\mathbf{x}_i\n\\]\n\nRepeat Steps 2–3 until cluster assignments no longer change (convergence).\n\n\n\nNotes on Optimization\n\nThe K-Means algorithm is not guaranteed to find the global minimum of the WCSS, because the optimization is non-convex and depends on initialization.\nA common practical strategy is to run the algorithm multiple times with different initializations and choose the best solution (lowest WCSS).\nCluster labels are arbitrary: permuting the label numbers does not change the structure.\n\n\n\nWhy Use Euclidean Distance?\nIn standard K-Means, the measure of similarity is Euclidean distance. That’s why it’s important to standardize the features before clustering (especially when variables are on different scales), so that no single variable dominates the distance metric.\n\n\n\nVisualizing the K-Means Algorithm Step-by-Step\nTo better understand how the K-Means algorithm learns and adjusts cluster boundaries, I created an animation that visualizes each iteration of the algorithm applied to the penguins dataset.\nThe algorithm works by repeating two simple steps:\n\nAssignment: Each point is assigned to the nearest centroid.\nUpdate: Each centroid is recalculated as the mean of the points assigned to it.\n\nThis process continues until the centroids stop moving significantly—i.e., the algorithm converges.\nThe animation below illustrates this iterative process using bill_length_mm and flipper_length_mm as features, with \\(K = 3\\) clusters. Each frame represents one iteration. Data points are colored by cluster assignment, and the black “×” marks indicate the current centroid positions.\n\n\n\nCustom vs. Built-in K-Means: A Side-by-Side Comparison\nTo evaluate the correctness and effectiveness of the custom K-Means algorithm I implemented, I compare its performance against the built-in KMeans implementation from the scikit-learn library. Specifically, I use \\(K = 3\\) clusters and assess the clustering results using two metrics: Within-Cluster Sum of Squares (WCSS) and Silhouette Score.\n\n\nCode\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\npenguins = pd.read_csv(\"../data/palmer_penguins.csv\").dropna()\npenguins.head()\n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\nyear\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181\n3750\nmale\n2007\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186\n3800\nfemale\n2007\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195\n3250\nfemale\n2007\n\n\n3\nAdelie\nTorgersen\n36.7\n19.3\n193\n3450\nfemale\n2007\n\n\n4\nAdelie\nTorgersen\n39.3\n20.6\n190\n3650\nmale\n2007\n\n\n\n\n\n\n\n\n\nCode\nX = penguins[[\"bill_length_mm\", \"flipper_length_mm\"]].values\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n\nHere is the self-written function for K-Means, which initializes centroids randomly and iteratively updates them until convergence. The logic matches the standard K-Means algorithm discussed above.\n\n\nCode\nimport numpy as np\n\ndef kmeans_custom(X, k, max_iter=100):\n    n_samples, n_features = X.shape\n    np.random.seed(42)\n    centroids = X[np.random.choice(n_samples, k, replace=False)]\n    \n    for _ in range(max_iter):\n        distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)\n        labels = np.argmin(distances, axis=1)\n\n        new_centroids = np.array([X[labels == i].mean(axis=0) for i in range(k)])\n        if np.allclose(centroids, new_centroids):\n            break\n        centroids = new_centroids\n\n    return labels, centroids\n\n\nWe now apply both the custom implementation and the built-in KMeans from scikit-learn to the same standardized dataset, using 3 clusters.\n\n\nCode\nfrom sklearn.cluster import KMeans\n\nkmeans_sklearn = KMeans(n_clusters=3, n_init=10, random_state=42)\nlabels_builtin = kmeans_sklearn.fit_predict(X_scaled)\ncentroids_builtin = kmeans_sklearn.cluster_centers_\n\nlabels_custom, centroids_custom = kmeans_custom(X_scaled, k=3)\n\n\nTo visually compare the clustering results, the plots below show the cluster assignments produced by the custom K-Means implementation (left) and the built-in scikit-learn implementation (right). In both cases:\n\nPoints are colored by their assigned cluster.\nCentroids are marked with black diamonds (custom) and red diamonds (built-in).\n\nThe plots confirm that both methods yield identical clustering structures, with near-perfect agreement in cluster boundaries and centroid positions.\n\n\nCode\nimport matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5), sharex=True, sharey=True)\n\naxes[0].scatter(X_scaled[:, 0], X_scaled[:, 1], c=labels_custom, cmap='Accent', alpha=0.6)\naxes[0].scatter(*centroids_custom.T, color='black', marker='D', s=100, label='Centroid')\naxes[0].set_title(\"Custom K-Means Clustering\")\naxes[0].legend()\n\naxes[1].scatter(X_scaled[:, 0], X_scaled[:, 1], c=labels_builtin, cmap='Accent', alpha=0.6)\naxes[1].scatter(*centroids_builtin.T, color='red', marker='D', s=100, label='Centroid')\naxes[1].set_title(\"scikit-learn KMeans Clustering\")\naxes[1].legend()\n\nfor ax in axes:\n    ax.set_xlabel(\"Scaled Bill Length\")\n    ax.set_ylabel(\"Scaled Flipper Length\")\n    ax.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nWe use two metrics to compare the quality of clustering:\n\nWCSS: Measures how compact the clusters are.\nSilhouette Score: Combines cohesion and separation; higher is better.\n\n\n\nCode\nfrom sklearn.metrics import silhouette_score\n\ndef compute_wcss(X, labels):\n    return sum(np.linalg.norm(X[labels == i] - X[labels == i].mean(axis=0))**2 for i in np.unique(labels))\n\nwcss_custom = compute_wcss(X_scaled, labels_custom)\nsil_custom = silhouette_score(X_scaled, labels_custom)\n\nwcss_builtin = kmeans_sklearn.inertia_\nsil_builtin = silhouette_score(X_scaled, labels_builtin)\n\nprint(\"Comparison of Custom vs. Built-in KMeans (K=3)\")\nprint(f\"Custom KMeans     → WCSS: {wcss_custom:.2f}, Silhouette: {sil_custom:.4f}\")\nprint(f\"Built-in KMeans   → WCSS: {wcss_builtin:.2f}, Silhouette: {sil_builtin:.4f}\")\n\n\nComparison of Custom vs. Built-in KMeans (K=3)\nCustom KMeans     → WCSS: 154.85, Silhouette: 0.5189\nBuilt-in KMeans   → WCSS: 154.85, Silhouette: 0.5189\n\n\nThe comparison shows that the custom implementation of the K-Means algorithm produces results that are nearly identical to those of the built-in KMeans from scikit-learn.\n\nThe WCSS values match exactly, indicating that the overall compactness of the clusters is the same.\nThe Silhouette Scores are also identical, suggesting both methods achieve the same balance of intra-cluster cohesion and inter-cluster separation.\n\nThis confirms that the logic in the custom implementation is consistent with the standard approach, validating both the clustering assignments and the centroid updates.\n\n\nDetermining the Optimal Number of Clusters (K)\nChoosing the right number of clusters is a key decision in K-Means clustering. To determine the optimal \\(K\\), we evaluate clustering performance across different values of \\(K\\) using:\n\nWCSS: A lower WCSS indicates more compact clusters, but it always decreases with higher \\(K\\).\nSilhouette Score: Balances cohesion and separation, with values closer to 1 indicating better-defined clusters.\n\nWe test \\(K = 2\\) to \\(K = 7\\) and plot both metrics to look for the “elbow” in the WCSS curve and the peak in the silhouette scores.\n\n\nCode\nk_values = range(2, 8)\n\nwcss_list = []\nsilhouette_list = []\n\nfor k in k_values:\n    labels_k, _ = kmeans_custom(X_scaled, k)\n    wcss = compute_wcss(X_scaled, labels_k)\n    sil = silhouette_score(X_scaled, labels_k)\n    wcss_list.append(wcss)\n    silhouette_list.append(sil)\n\nimport matplotlib.pyplot as plt\n\nbest_k_sil = k_values[np.argmax(silhouette_list)]\n\nplt.figure(figsize=(12, 5))\n\n# WCSS\nplt.subplot(1, 2, 1)\nplt.plot(k_values, wcss_list, marker='o')\nplt.axvline(x=3, color='red', linestyle='--', label='Elbow (K=3)')\nplt.title(\"WCSS by Number of Clusters\")\nplt.xlabel(\"Number of Clusters (K)\")\nplt.ylabel(\"WCSS\")\nplt.legend()\n\n# Silhouette\nplt.subplot(1, 2, 2)\nplt.plot(k_values, silhouette_list, marker='o')\nplt.axvline(x=best_k_sil, color='green', linestyle='--', label=f'Peak (K={best_k_sil})')\nplt.title(\"Silhouette Score by Number of Clusters\")\nplt.xlabel(\"Number of Clusters (K)\")\nplt.ylabel(\"Silhouette Score\")\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nIn the silhouette plot, we mark the true peak at \\(K=2\\), where the score is highest. While this suggests that the data naturally splits into two well-separated clusters, the WCSS plot shows an elbow at \\(K=3\\), which offers a better balance between complexity and explanatory power.\nBoth metrics are helpful, but may suggest different optimal values depending on what aspect of clustering is prioritized."
  },
  {
    "objectID": "homework/hw4_questions.html#a.-k-means",
    "href": "homework/hw4_questions.html#a.-k-means",
    "title": "Unsupervised and Supervised Machine Learning: From K-Means to Key Drivers",
    "section": "1a. K-Means",
    "text": "1a. K-Means\ntodo: write your own code to implement the k-means algorithm. Make plots of the various steps the algorithm takes so you can “see” the algorithm working. Test your algorithm on the Palmer Penguins dataset, specifically using the bill length and flipper length variables. Compare your results to the built-in kmeans function in R or Python.\ntodo: Calculate both the within-cluster-sum-of-squares and silhouette scores (you can use built-in functions to do so) and plot the results for various numbers of clusters (ie, K=2,3,…,7). What is the “right” number of clusters as suggested by these two metrics?\nIf you want a challenge, add your plots as an animated gif on your website so that the result looks something like this."
  },
  {
    "objectID": "homework/hw4_questions.html#b.-latent-class-mnl",
    "href": "homework/hw4_questions.html#b.-latent-class-mnl",
    "title": "Unsupervised and Supervised Machine Learning: From K-Means to Key Drivers",
    "section": "1b. Latent-Class MNL",
    "text": "1b. Latent-Class MNL\ntodo: Use the Yogurt dataset to estimate a latent-class MNL model. This model was formally introduced in the paper by Kamakura & Russell (1989); you may want to read or reference page 2 of the pdf, which is described in the class slides, session 4, slides 56-57.\nThe data provides anonymized consumer identifiers (id), a vector indicating the chosen product (y1:y4), a vector indicating if any products were “featured” in the store as a form of advertising (f1:f4), and the products’ prices in price-per-ounce (p1:p4). For example, consumer 1 purchased yogurt 4 at a price of 0.079/oz and none of the yogurts were featured/advertised at the time of consumer 1’s purchase. Consumers 2 through 7 each bought yogurt 2, etc. You may want to reshape the data from its current “wide” format into a “long” format.\ntodo: Fit the standard MNL model on these data. Then fit the latent-class MNL on these data separately for 2, 3, 4, and 5 latent classes.\ntodo: How many classes are suggested by the \\(BIC = -2*\\ell_n  + k*log(n)\\)? (where \\(\\ell_n\\) is the log-likelihood, \\(n\\) is the sample size, and \\(k\\) is the number of parameters.) The Bayesian-Schwarz Information Criterion link is a metric that assess the benefit of a better log likelihood at the expense of additional parameters to estimate – akin to the adjusted R-squared for the linear regression model. Note, that a lower BIC indicates a better model fit, accounting for the number of parameters in the model.\ntodo: compare the parameter estimates between (1) the aggregate MNL, and (2) the latent-class MNL with the number of classes suggested by the BIC."
  },
  {
    "objectID": "homework/hw4_questions.html#a.-k-nearest-neighbors",
    "href": "homework/hw4_questions.html#a.-k-nearest-neighbors",
    "title": "Unsupervised and Supervised Machine Learning: From K-Means to Key Drivers",
    "section": "2a. K Nearest Neighbors",
    "text": "2a. K Nearest Neighbors\ntodo: use the following code (or the python equivalent) to generate a synthetic dataset for the k-nearest neighbors algorithm. The code generates a dataset with two features, x1 and x2, and a binary outcome variable y that is determined by whether x2 is above or below a wiggly boundary defined by a sin function.\n# gen data -----\nset.seed(42)\nn &lt;- 100\nx1 &lt;- runif(n, -3, 3)\nx2 &lt;- runif(n, -3, 3)\nx &lt;- cbind(x1, x2)\n\n# define a wiggly boundary\nboundary &lt;- sin(4*x1) + x1\ny &lt;- ifelse(x2 &gt; boundary, 1, 0) |&gt; as.factor()\ndat &lt;- data.frame(x1 = x1, x2 = x2, y = y)\ntodo: plot the data where the horizontal axis is x1, the vertical axis is x2, and the points are colored by the value of y. You may optionally draw the wiggly boundary.\ntodo: generate a test dataset with 100 points, using the same code as above but with a different seed.\ntodo: implement KNN by hand. Check you work with a built-in function – eg, class::knn() or caret::train(method=\"knn\") in R, or scikit-learn’s KNeighborsClassifier in Python.\ntodo: run your function for k=1,…,k=30, each time noting the percentage of correctly-classified points from the test dataset. Plot the results, where the horizontal axis is 1-30 and the vertical axis is the percentage of correctly-classified points. What is the optimal value of k as suggested by your plot?"
  },
  {
    "objectID": "homework/hw4_questions.html#b.-key-drivers-analysis",
    "href": "homework/hw4_questions.html#b.-key-drivers-analysis",
    "title": "Unsupervised and Supervised Machine Learning: From K-Means to Key Drivers",
    "section": "2b. Key Drivers Analysis",
    "text": "2b. Key Drivers Analysis\ntodo: replicate the table on slide 75 of the session 5 slides. Specifically, using the dataset provided in the file data_for_drivers_analysis.csv, calculate: pearson correlations, standardized regression coefficients, “usefulness”, Shapley values for a linear regression, Johnson’s relative weights, and the mean decrease in the gini coefficient from a random forest. You may use packages built into R or Python; you do not need to perform these calculations “by hand.”\nIf you want a challenge, add additional measures to the table such as the importance scores from XGBoost, from a Neural Network, or from any additional method that measures the importance of variables."
  },
  {
    "objectID": "homework/hw4_questions.html#uncovering-what-drives-outcomes-a-multimethod-feature-importance-analysis",
    "href": "homework/hw4_questions.html#uncovering-what-drives-outcomes-a-multimethod-feature-importance-analysis",
    "title": "Unsupervised and Supervised Machine Learning: From K-Means to Key Drivers",
    "section": "Uncovering What Drives Outcomes: A Multimethod Feature Importance Analysis",
    "text": "Uncovering What Drives Outcomes: A Multimethod Feature Importance Analysis\nIn this section, I use supervised machine learning methods to identify which variables are most important in explaining a target outcome.\nFollowing the framework introduced in Session 5, I compare six different variable importance measures: Pearson correlation, standardized regression coefficients, usefulness (\\(\\Delta R^2\\)), Shapley values, Johnson’s relative weights, and mean decrease in Gini from a random forest model.\nThis analysis helps uncover which features consistently drive the outcome across different model perspectives.\n\nIntroduction and Dataset Overview\nWe begin by loading the dataset and inspecting its structure. Our goal is to identify the response variable and the candidate predictors to be used in the driver analysis.\n\n\nCode\ndf = pd.read_csv(\"../data/data_for_drivers_analysis.csv\")\ndf.describe()\n\n\n\n\n\n\n\n\n\nbrand\nid\nsatisfaction\ntrust\nbuild\ndiffers\neasy\nappealing\nrewarding\npopular\nservice\nimpact\n\n\n\n\ncount\n2553.000000\n2553.000000\n2553.000000\n2553.000000\n2553.000000\n2553.000000\n2553.000000\n2553.000000\n2553.000000\n2553.000000\n2553.000000\n2553.000000\n\n\nmean\n4.857423\n8931.480611\n3.386604\n0.549550\n0.461810\n0.334508\n0.536232\n0.451234\n0.451234\n0.536232\n0.467293\n0.330983\n\n\nstd\n2.830096\n5114.287849\n1.172006\n0.497636\n0.498637\n0.471911\n0.498783\n0.497714\n0.497714\n0.498783\n0.499027\n0.470659\n\n\nmin\n1.000000\n88.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n3.000000\n4310.000000\n3.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n50%\n4.000000\n8924.000000\n4.000000\n1.000000\n0.000000\n0.000000\n1.000000\n0.000000\n0.000000\n1.000000\n0.000000\n0.000000\n\n\n75%\n6.000000\n13545.000000\n4.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n\n\nmax\n10.000000\n18088.000000\n5.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n\n\n\nThe dataset includes many perceptual variables, as well as identifiers like id and brand.\nFollowing the variable importance comparison shown in the class example (Session 5, Slide 75), I focus on the perception-based features only, excluding ID and brand metadata.\n\n\n\nStatistical Feature Importance Methods\nI begin by calculating three regression-based importance metrics:\n\nPearson correlation measures the raw linear association between each predictor and satisfaction.\nStandardized regression coefficients come from fitting a linear model on standardized data and show the relative impact per standard deviation change.\nUsefulness quantifies the drop in \\(R^2\\) when each variable is removed from a full linear model.\n\n\n\nCode\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\n\ny = df[\"satisfaction\"]\nX = df.drop(columns=[\"satisfaction\", \"id\", \"brand\"])\n\n# Pearson Correlation\ncorrelations = X.corrwith(y)\n\n# Standardized Coefficients\nscaler = StandardScaler()\nX_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\ny_scaled = (y - y.mean()) / y.std()\n\nlr = LinearRegression()\nlr.fit(X_scaled, y_scaled)\nstandardized_coefs = pd.Series(lr.coef_, index=X.columns)\n\n# Usefulness: ΔR² from dropping one variable at a time\nfull_r2 = r2_score(y, LinearRegression().fit(X, y).predict(X))\nusefulness = {}\n\nfor col in X.columns:\n    X_drop = X.drop(columns=[col])\n    y_pred_drop = LinearRegression().fit(X_drop, y).predict(X_drop)\n    r2_drop = r2_score(y, y_pred_drop)\n    usefulness[col] = full_r2 - r2_drop\n\nusefulness = pd.Series(usefulness)\n\n\nThe table below summarizes these results:\n\n\nCode\nsummary_df = pd.DataFrame({\n    \"Pearson Correlation\": correlations,\n    \"Standardized Coefficient\": standardized_coefs,\n    \"Usefulness (ΔR²)\": usefulness\n}).round(3)\n\nsummary_df.sort_values(\"Usefulness (ΔR²)\", ascending=False)\n\n\n\n\n\n\n\n\n\nPearson Correlation\nStandardized Coefficient\nUsefulness (ΔR²)\n\n\n\n\nimpact\n0.255\n0.128\n0.011\n\n\ntrust\n0.256\n0.116\n0.008\n\n\nservice\n0.251\n0.088\n0.005\n\n\ndiffers\n0.185\n0.028\n0.001\n\n\nappealing\n0.208\n0.034\n0.001\n\n\nbuild\n0.192\n0.020\n0.000\n\n\neasy\n0.213\n0.022\n0.000\n\n\nrewarding\n0.195\n0.005\n0.000\n\n\npopular\n0.171\n0.017\n0.000\n\n\n\n\n\n\n\nNote: The values in the table below differ from those shown above. This is expected, as I use a different (synthetic) dataset and Python-based implementations. The goal here is to replicate the analysis process, not to match specific numbers.\n\n\nModel-Based Variable Importance\nIn this section, I estimate variable importance using three model-driven approaches:\n\nRandom Forest Gini importance: Reflects how much each variable reduces impurity in tree-based models.\nSHAP values: A Shapley-inspired method that attributes contribution of each variable to model predictions.\nJohnson’s relative weights: A decomposition of model \\(R^2\\) based on the orthogonal projection of predictors (approximated via PCA).\n\nTo approximate Johnson’s relative weights without using unstable packages, I use a PCA-based projection method that decomposes variance and reweights predictors based on their contribution to model \\(R^2\\).\nThis gives us a stable and interpretable measure of relative variable importance, conceptually aligned with the Shapley-based decomposition.\nTogether, these three approaches reflect how variables contribute to prediction in nonlinear, interactive, and multicollinear contexts.\n\n\nCode\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.decomposition import PCA\nimport shap\n\n# Random Forest Gini\nrf = RandomForestRegressor(n_estimators=500, random_state=42)\nrf.fit(X, y)\ngini_importance = pd.Series(rf.feature_importances_, index=X.columns)\n\n# SHAP\nexplainer = shap.Explainer(rf, X)\nshap_values = explainer(X)\nshap_importance = pd.Series(\n    np.abs(shap_values.values).mean(axis=0),\n    index=X.columns\n)\n\n# Johnson's Relative Weights (PCA approximation)\npca = PCA()\nZ = pca.fit_transform(X)\n\nlr_pca = LinearRegression().fit(Z, y)\nbeta_z = lr_pca.coef_\nvar_z = np.var(Z, axis=0)\nr2 = lr_pca.score(Z, y)\n\np = X.shape[1]\nLambda = (beta_z ** 2) * var_z\nraw_weights = np.dot(pca.components_.T ** 2, Lambda)\njohnson_weight = raw_weights / raw_weights.sum() * r2\njohnson_weight = pd.Series(johnson_weight, index=X.columns)\n\n\n  7%|=                   | 166/2553 [00:11&lt;02:38]         7%|=                   | 183/2553 [00:12&lt;02:35]         8%|==                  | 197/2553 [00:13&lt;02:35]         8%|==                  | 212/2553 [00:14&lt;02:34]         9%|==                  | 226/2553 [00:15&lt;02:34]         9%|==                  | 242/2553 [00:16&lt;02:32]        10%|==                  | 258/2553 [00:17&lt;02:31]        11%|==                  | 274/2553 [00:18&lt;02:29]        11%|==                  | 289/2553 [00:19&lt;02:28]        12%|==                  | 306/2553 [00:20&lt;02:26]        13%|===                 | 320/2553 [00:21&lt;02:26]        13%|===                 | 335/2553 [00:22&lt;02:25]        14%|===                 | 353/2553 [00:23&lt;02:23]        14%|===                 | 369/2553 [00:24&lt;02:22]        15%|===                 | 385/2553 [00:25&lt;02:20]        16%|===                 | 402/2553 [00:26&lt;02:19]        16%|===                 | 418/2553 [00:27&lt;02:17]        17%|===                 | 434/2553 [00:28&lt;02:16]        18%|====                | 449/2553 [00:29&lt;02:15]        18%|====                | 468/2553 [00:30&lt;02:13]        19%|====                | 485/2553 [00:31&lt;02:12]        19%|====                | 497/2553 [00:32&lt;02:12]        20%|====                | 512/2553 [00:33&lt;02:11]        21%|====                | 526/2553 [00:34&lt;02:11]        21%|====                | 542/2553 [00:35&lt;02:09]        22%|====                | 558/2553 [00:36&lt;02:08]        22%|====                | 572/2553 [00:37&lt;02:08]        23%|=====               | 586/2553 [00:38&lt;02:07]        24%|=====               | 602/2553 [00:39&lt;02:06]        24%|=====               | 619/2553 [00:40&lt;02:04]        25%|=====               | 633/2553 [00:41&lt;02:04]        25%|=====               | 648/2553 [00:42&lt;02:03]        26%|=====               | 664/2553 [00:43&lt;02:02]        27%|=====               | 682/2553 [00:44&lt;02:00]        27%|=====               | 698/2553 [00:45&lt;01:59]        28%|======              | 713/2553 [00:46&lt;01:58]        29%|======              | 729/2553 [00:47&lt;01:57]        29%|======              | 745/2553 [00:48&lt;01:56]        30%|======              | 760/2553 [00:49&lt;01:55]        30%|======              | 774/2553 [00:50&lt;01:54]        31%|======              | 788/2553 [00:51&lt;01:54]        32%|======              | 807/2553 [00:52&lt;01:52]        32%|======              | 821/2553 [00:53&lt;01:51]        33%|=======             | 836/2553 [00:54&lt;01:50]        33%|=======             | 853/2553 [00:55&lt;01:49]        34%|=======             | 869/2553 [00:56&lt;01:48]        35%|=======             | 885/2553 [00:57&lt;01:47]        35%|=======             | 900/2553 [00:58&lt;01:46]        36%|=======             | 915/2553 [00:59&lt;01:45]        36%|=======             | 927/2553 [01:00&lt;01:45]        37%|=======             | 943/2553 [01:01&lt;01:44]        38%|========            | 958/2553 [01:02&lt;01:43]        38%|========            | 972/2553 [01:03&lt;01:42]        39%|========            | 989/2553 [01:04&lt;01:41]        39%|========            | 1003/2553 [01:05&lt;01:40]        40%|========            | 1018/2553 [01:06&lt;01:39]        41%|========            | 1034/2553 [01:07&lt;01:38]        41%|========            | 1049/2553 [01:08&lt;01:37]        42%|========            | 1064/2553 [01:09&lt;01:36]        42%|========            | 1079/2553 [01:10&lt;01:35]        43%|=========           | 1094/2553 [01:11&lt;01:34]        43%|=========           | 1109/2553 [01:12&lt;01:33]        44%|=========           | 1125/2553 [01:13&lt;01:32]        45%|=========           | 1140/2553 [01:14&lt;01:31]        45%|=========           | 1156/2553 [01:15&lt;01:30]        46%|=========           | 1172/2553 [01:16&lt;01:29]        47%|=========           | 1188/2553 [01:17&lt;01:28]        47%|=========           | 1205/2553 [01:18&lt;01:27]        48%|==========          | 1222/2553 [01:19&lt;01:26]        48%|==========          | 1235/2553 [01:20&lt;01:25]        49%|==========          | 1251/2553 [01:21&lt;01:24]        50%|==========          | 1266/2553 [01:22&lt;01:23]        50%|==========          | 1282/2553 [01:23&lt;01:22]        51%|==========          | 1298/2553 [01:24&lt;01:21]        52%|==========          | 1315/2553 [01:25&lt;01:20]        52%|==========          | 1331/2553 [01:26&lt;01:18]        53%|===========         | 1347/2553 [01:27&lt;01:17]        53%|===========         | 1362/2553 [01:28&lt;01:16]        54%|===========         | 1380/2553 [01:29&lt;01:15]        55%|===========         | 1396/2553 [01:30&lt;01:14]        55%|===========         | 1414/2553 [01:31&lt;01:13]        56%|===========         | 1430/2553 [01:32&lt;01:12]        57%|===========         | 1448/2553 [01:33&lt;01:10]        57%|===========         | 1465/2553 [01:34&lt;01:09]        58%|============        | 1483/2553 [01:35&lt;01:08]        59%|============        | 1500/2553 [01:36&lt;01:07]        59%|============        | 1516/2553 [01:37&lt;01:06]        60%|============        | 1534/2553 [01:38&lt;01:05]        61%|============        | 1553/2553 [01:39&lt;01:03]        62%|============        | 1572/2553 [01:40&lt;01:02]        62%|============        | 1590/2553 [01:41&lt;01:01]        63%|=============       | 1606/2553 [01:42&lt;01:00]        64%|=============       | 1622/2553 [01:43&lt;00:59]        64%|=============       | 1640/2553 [01:44&lt;00:57]        65%|=============       | 1657/2553 [01:45&lt;00:56]        66%|=============       | 1675/2553 [01:46&lt;00:55]        66%|=============       | 1692/2553 [01:47&lt;00:54]        67%|=============       | 1711/2553 [01:48&lt;00:53]        68%|==============      | 1728/2553 [01:49&lt;00:52]        68%|==============      | 1744/2553 [01:50&lt;00:51]        69%|==============      | 1763/2553 [01:51&lt;00:49]        70%|==============      | 1779/2553 [01:52&lt;00:48]        70%|==============      | 1796/2553 [01:53&lt;00:47]        71%|==============      | 1812/2553 [01:54&lt;00:46]        72%|==============      | 1830/2553 [01:55&lt;00:45]        72%|==============      | 1847/2553 [01:56&lt;00:44]        73%|===============     | 1863/2553 [01:57&lt;00:43]        74%|===============     | 1881/2553 [01:58&lt;00:42]        74%|===============     | 1896/2553 [01:59&lt;00:41]        75%|===============     | 1911/2553 [02:00&lt;00:40]        75%|===============     | 1926/2553 [02:01&lt;00:39]        76%|===============     | 1945/2553 [02:02&lt;00:38]        77%|===============     | 1960/2553 [02:03&lt;00:37]        77%|===============     | 1977/2553 [02:04&lt;00:36]        78%|================    | 1992/2553 [02:05&lt;00:35]        79%|================    | 2011/2553 [02:06&lt;00:33]        79%|================    | 2026/2553 [02:07&lt;00:33]        80%|================    | 2043/2553 [02:08&lt;00:31]        81%|================    | 2063/2553 [02:09&lt;00:30]        82%|================    | 2083/2553 [02:10&lt;00:29]        82%|================    | 2103/2553 [02:11&lt;00:28]        83%|=================   | 2122/2553 [02:12&lt;00:26]        84%|=================   | 2137/2553 [02:13&lt;00:25]        84%|=================   | 2156/2553 [02:14&lt;00:24]        85%|=================   | 2173/2553 [02:15&lt;00:23]        86%|=================   | 2192/2553 [02:16&lt;00:22]        87%|=================   | 2213/2553 [02:17&lt;00:21]        87%|=================   | 2230/2553 [02:18&lt;00:19]        88%|==================  | 2249/2553 [02:19&lt;00:18]        89%|==================  | 2268/2553 [02:20&lt;00:17]        90%|==================  | 2286/2553 [02:21&lt;00:16]        90%|==================  | 2304/2553 [02:22&lt;00:15]        91%|==================  | 2324/2553 [02:23&lt;00:14]        92%|==================  | 2341/2553 [02:24&lt;00:13]        92%|==================  | 2361/2553 [02:25&lt;00:11]        93%|=================== | 2381/2553 [02:26&lt;00:10]        94%|=================== | 2400/2553 [02:27&lt;00:09]        95%|=================== | 2419/2553 [02:28&lt;00:08]        96%|=================== | 2439/2553 [02:29&lt;00:06]        96%|=================== | 2458/2553 [02:30&lt;00:05]        97%|=================== | 2476/2553 [02:31&lt;00:04]        98%|===================| 2495/2553 [02:32&lt;00:03]        98%|===================| 2511/2553 [02:33&lt;00:02]        99%|===================| 2526/2553 [02:34&lt;00:01]       100%|===================| 2544/2553 [02:35&lt;00:00]       \n\n\nThe table below summarizes these results:\n\n\nCode\nmodel_df = pd.DataFrame({\n    \"Gini (RF)\": gini_importance,\n    \"SHAP (mean abs)\": shap_importance,\n    \"Johnson Weight\": johnson_weight\n}).round(4).sort_values(\"SHAP (mean abs)\", ascending=False)\n\nmodel_df\n\n\n\n\n\n\n\n\n\nGini (RF)\nSHAP (mean abs)\nJohnson Weight\n\n\n\n\nimpact\n0.1396\n0.1418\n0.0105\n\n\nservice\n0.1361\n0.1235\n0.0143\n\n\ntrust\n0.1523\n0.1148\n0.0130\n\n\nappealing\n0.0847\n0.0765\n0.0132\n\n\npopular\n0.0953\n0.0614\n0.0091\n\n\neasy\n0.0973\n0.0594\n0.0133\n\n\nbuild\n0.0997\n0.0567\n0.0119\n\n\nrewarding\n0.1055\n0.0549\n0.0150\n\n\ndiffers\n0.0896\n0.0531\n0.0087\n\n\n\n\n\n\n\nNote: The values in the table below differ from those shown above. This is expected, as I use a different (synthetic) dataset and Python-based implementations. The goal here is to replicate the analysis process, not to match specific numbers.\n\n\nSummary Table of Results\nTo wrap up the analysis, I combine all six importance metrics into a single summary table. These metrics span both traditional statistical and modern model-based approaches, offering different perspectives on what drives customer satisfaction.\nThe six methods include:\n\nPearson correlation\nStandardized regression coefficient\nUsefulness (\\(\\Delta R^2\\))\nRandom Forest Gini importance\nSHAP value (mean absolute contribution)\nJohnson’s relative weights\n\nBy viewing all metrics side-by-side, we can identify variables that consistently rank highly and those that are more model-sensitive.\n\n\nCode\nfull_df = pd.concat([summary_df, model_df], axis=1).round(3)\n\nfull_df = full_df.sort_values(\"SHAP (mean abs)\", ascending=False)\nfull_df\n\n\n\n\n\n\n\n\n\nPearson Correlation\nStandardized Coefficient\nUsefulness (ΔR²)\nGini (RF)\nSHAP (mean abs)\nJohnson Weight\n\n\n\n\nimpact\n0.255\n0.128\n0.011\n0.140\n0.142\n0.010\n\n\nservice\n0.251\n0.088\n0.005\n0.136\n0.124\n0.014\n\n\ntrust\n0.256\n0.116\n0.008\n0.152\n0.115\n0.013\n\n\nappealing\n0.208\n0.034\n0.001\n0.085\n0.076\n0.013\n\n\npopular\n0.171\n0.017\n0.000\n0.095\n0.061\n0.009\n\n\neasy\n0.213\n0.022\n0.000\n0.097\n0.059\n0.013\n\n\nbuild\n0.192\n0.020\n0.000\n0.100\n0.057\n0.012\n\n\nrewarding\n0.195\n0.005\n0.000\n0.106\n0.055\n0.015\n\n\ndiffers\n0.185\n0.028\n0.001\n0.090\n0.053\n0.009\n\n\n\n\n\n\n\n\n\nCode\ntop_vars = full_df.sort_values(\"SHAP (mean abs)\", ascending=False).head(6)\n\nmethods_to_plot = [\n    \"Pearson Correlation\",\n    \"Standardized Coefficient\",\n    \"Usefulness (ΔR²)\",\n    \"Gini (RF)\",\n    \"SHAP (mean abs)\",\n    \"Johnson Weight\"\n]\n\ntop_vars[methods_to_plot].plot(kind='bar', figsize=(10, 6))\nplt.title(\"Variable Importance Across Methods (Top 6 Variables)\")\nplt.ylabel(\"Importance Score\")\nplt.xlabel(\"Variable\")\nplt.xticks(rotation=45)\nplt.legend(loc=\"upper right\", fontsize=8)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nAcross all six methods, impact, trust, and service consistently rank as the top drivers of satisfaction. These variables score highly not only in correlation-based methods but also in model-driven measures like SHAP and Gini importance.\nOther features such as popular, appealing, and easy show more variation in their rankings depending on the method used—highlighting how different models capture different aspects of influence.\nThe combined analysis confirms a small set of robust predictors, while also revealing that feature importance can be sensitive to the assumptions and structure of the chosen method.\n\n\nExtended Comparison with Advanced Models\nTo complement the earlier methods, I explore additional model-based techniques for estimating feature importance. These include:\n\nXGBoost: A gradient-boosted tree ensemble model that outputs multiple importance metrics, such as gain and cover.\nMultilayer Perceptron (MLP): A fully connected neural network, for which we calculate permutation-based importance to assess how sensitive the model is to each input feature.\n\nThese advanced models provide robustness checks and capture complex non-linear interactions that traditional methods may overlook.\n\n\nCode\nimport xgboost as xgb\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.inspection import permutation_importance\n\nxgb_model = xgb.XGBRegressor(n_estimators=500, random_state=42)\nxgb_model.fit(X, y)\n\nxgb_importance = pd.Series(xgb_model.feature_importances_, index=X.columns)\n\nxgb_importance_df = xgb_importance.sort_values(ascending=False).to_frame(\"XGBoost Gain\")\n\nmlp = MLPRegressor(hidden_layer_sizes=(100,), max_iter=1000, random_state=42)\nmlp.fit(X, y)\n\nperm = permutation_importance(mlp, X, y, n_repeats=10, random_state=42)\nmlp_importance = pd.Series(perm.importances_mean, index=X.columns).sort_values(ascending=False)\nmlp_importance_df = mlp_importance.to_frame(\"MLP Permutation Importance\")\n\n\nThe table below summarizes these results:\n\n\nCode\nadvanced_df = pd.concat([xgb_importance_df, mlp_importance_df], axis=1).round(4)\nadvanced_df\n\n\n\n\n\n\n\n\n\nXGBoost Gain\nMLP Permutation Importance\n\n\n\n\ntrust\n0.2529\n0.1116\n\n\nimpact\n0.1741\n0.0957\n\n\nservice\n0.1064\n0.0876\n\n\nbuild\n0.0927\n0.0676\n\n\neasy\n0.0916\n0.0673\n\n\npopular\n0.0768\n0.0681\n\n\ndiffers\n0.0695\n0.0672\n\n\nappealing\n0.0683\n0.0753\n\n\nrewarding\n0.0677\n0.0781\n\n\n\n\n\n\n\nAdvanced models further reinforce the role of a few key predictors. In the XGBoost model, trust stands out with the highest gain score, followed by impact and service. These same features are also top-ranked in the MLP model, based on permutation importance.\nWhile there are slight variations in ordering, the consistency across both linear and nonlinear methods—including tree ensembles and neural networks—underscores the robustness of these drivers in explaining satisfaction.\n\n\nConclusion\n\n\nCode\nimport seaborn as sns\n\nall_metrics_df = pd.concat([summary_df, model_df, advanced_df], axis=1)\n\nnormalized_df = all_metrics_df.copy()\nnormalized_df = normalized_df.apply(lambda col: (col - col.min()) / (col.max() - col.min()))\n\nplt.figure(figsize=(12, 7))\nsns.heatmap(normalized_df.T, annot=True, cmap=\"YlGnBu\", fmt=\".2f\", linewidths=0.5)\n\nplt.title(\"Normalized Feature Importance Across 8 Methods\")\nplt.xlabel(\"Variable\")\nplt.ylabel(\"Importance Metric\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nAcross all eight importance metrics—from classical correlation to modern machine learning techniques—trust and impact consistently emerge as the most influential predictors of customer satisfaction. These two variables dominate not only in statistical methods (e.g., Pearson correlation, standardized coefficients) but also in model-driven metrics like SHAP, Gini importance, and XGBoost gain.\nService also ranks highly in several methods, further supporting its role as a key driver. Meanwhile, features such as differs, rewarding, and popular receive little to no importance across nearly all methods, suggesting minimal predictive value in this context.\nNotably, the heatmap also reveals areas of model sensitivity. For instance, rewarding scores very low in most methods but peaks under Johnson’s relative weights—highlighting how different models capture different aspects of importance.\nOverall, the triangulation of insights across diverse methods reinforces confidence in the top features while also providing transparency around model-specific variance. This multi-method approach not only strengthens interpretability but also increases robustness in actionable recommendations."
  }
]