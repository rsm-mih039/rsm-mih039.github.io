[
  {
    "objectID": "homework/hw3_questions.html",
    "href": "homework/hw3_questions.html",
    "title": "Multinomial Logit Model",
    "section": "",
    "text": "This assignment expores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm."
  },
  {
    "objectID": "homework/hw3_questions.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "href": "homework/hw3_questions.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "title": "Multinomial Logit Model",
    "section": "1. Likelihood for the Multi-nomial Logit (MNL) Model",
    "text": "1. Likelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 3 products, then either \\(y=3\\) or \\(y=(0,0,1)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, brand, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 3 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]"
  },
  {
    "objectID": "homework/hw3_questions.html#simulate-conjoint-data",
    "href": "homework/hw3_questions.html#simulate-conjoint-data",
    "title": "Multinomial Logit Model",
    "section": "2. Simulate Conjoint Data",
    "text": "2. Simulate Conjoint Data\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a “no choice” option; each simulated respondent must select one of the 3 alternatives.\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from $4 to $32 in increments of $4.\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer \\(i\\) for hypothethical streaming service \\(j\\) is\n\\[\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n\\]\nwhere the variables are binary indicators and \\(\\varepsilon\\) is Type 1 Extreme Value (ie, Gumble) distributed.\nThe following code provides the simulation of the conjoint data.\n\nimport numpy as np\nimport pandas as pd\n\n# Set random seed\nnp.random.seed(123)\n\n# Define attribute levels\nbrands = [\"N\", \"P\", \"H\"]  # Netflix, Prime, Hulu\nads = [\"Yes\", \"No\"]\nprices = list(range(8, 33, 4))  # from 8 to 32 by 4\n\n# Generate full factorial design\nimport itertools\nprofiles = pd.DataFrame(\n    list(itertools.product(brands, ads, prices)), \n    columns=[\"brand\", \"ad\", \"price\"])\n\n# Utility functions (true parameters)\nbrand_utils = {\"N\": 1.0, \"P\": 0.5, \"H\": 0.0}\nad_utils = {\"Yes\": -0.8, \"No\": 0.0}\nprice_util = lambda p: -0.1 * p\n\n# Settings\nn_peeps = 100\nn_tasks = 10\nn_alts = 3\n\n# Function to simulate one respondent\ndef simulate_respondent(pid):\n    respondent_data = []\n    for task in range(1, n_tasks + 1):\n        alts = profiles.sample(n=n_alts).copy()\n        alts[\"resp\"] = pid\n        alts[\"task\"] = task\n        \n        # Calculate deterministic utility\n        alts[\"v\"] = (\n            alts[\"brand\"].map(brand_utils) +\n            alts[\"ad\"].map(ad_utils) +\n            alts[\"price\"].apply(price_util)\n        ).round(10)\n        \n        # Add Gumbel-distributed error (Type I Extreme Value)\n        gumbel_noise = -np.log(-np.log(np.random.rand(n_alts)))\n        alts[\"u\"] = alts[\"v\"] + gumbel_noise\n        \n        # Choose the alternative with max utility\n        alts[\"choice\"] = (alts[\"u\"] == alts[\"u\"].max()).astype(int)\n        \n        respondent_data.append(alts)\n    \n    return pd.concat(respondent_data, ignore_index=True)\n\n# Simulate for all respondents\ndf_list = [simulate_respondent(i) for i in range(1, n_peeps + 1)]\nconjoint_data = pd.concat(df_list, ignore_index=True)\n\n# Keep only relevant columns (as if unobservable utility components are hidden)\nconjoint_data = conjoint_data[[\"resp\", \"task\", \"brand\", \"ad\", \"price\", \"choice\"]]"
  },
  {
    "objectID": "homework/hw3_questions.html#preparing-the-data-for-estimation",
    "href": "homework/hw3_questions.html#preparing-the-data-for-estimation",
    "title": "Multinomial Logit Model",
    "section": "3. Preparing the Data for Estimation",
    "text": "3. Preparing the Data for Estimation\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables.\nTo estimate the MNL likelihood, we must reshape the data to panel (long) format, and encode categorical variables (brand and ad exposure) into binary indicators. Each row represents one alternative within a choice task.\n\nimport pandas as pd\nimport numpy as np\n\nconjoint_data = pd.read_csv(\"../data/conjoint_data.csv\")\n\n# One-hot encode\nX = pd.get_dummies(conjoint_data, columns=[\"brand\", \"ad\"], drop_first=True)\n\nX.rename(columns={\n    \"brand_P\": \"Prime\",\n    \"brand_N\": \"Netflix\",\n    \"ad_Yes\": \"Ads\"\n}, inplace=True)\n\nfor col in ['resp', 'task', 'choice']:\n    X[col] = X[col].astype('int')\n\nX.head()\n\n\n\n\n\n\n\n\nresp\ntask\nchoice\nprice\nNetflix\nPrime\nAds\n\n\n\n\n0\n1\n1\n1\n28\nTrue\nFalse\nTrue\n\n\n1\n1\n1\n0\n16\nFalse\nFalse\nTrue\n\n\n2\n1\n1\n0\n16\nFalse\nTrue\nTrue\n\n\n3\n1\n2\n0\n32\nTrue\nFalse\nTrue\n\n\n4\n1\n2\n1\n16\nFalse\nTrue\nTrue\n\n\n\n\n\n\n\nNow we have the data in a suitable format for estimating the likelihood. We will treat “Hulu” and “Ad-Free” as the base categories."
  },
  {
    "objectID": "homework/hw3_questions.html#estimation-via-maximum-likelihood",
    "href": "homework/hw3_questions.html#estimation-via-maximum-likelihood",
    "title": "Multinomial Logit Model",
    "section": "4. Estimation via Maximum Likelihood",
    "text": "4. Estimation via Maximum Likelihood\nWe estimate the multinomial logit model by maximizing the joint log-likelihood across all respondents and tasks. The 4 parameters are:\n- \\(\\beta_\\text{netflix}\\)\n- \\(\\beta_\\text{prime}\\)\n- \\(\\beta_\\text{ads}\\)\n- \\(\\beta_\\text{price}\\)\n\nfrom scipy.optimize import minimize\n\nfeatures = [\"Netflix\", \"Prime\", \"Ads\", \"price\"]\nX[features] = X[features].astype(float)\n\ngroups = X.groupby([\"resp\", \"task\"])\n\nX_mat = X[features].to_numpy()\ny = X[\"choice\"].to_numpy()\nn_rows = X.shape[0]\nn_params = len(features)\n\nWe now proceed to implement the log-likelihood function for the MNL model introduced earlier. For clarity, we restate the key expressions used in estimation below.\n\nLog-Likelihood Function for the Multinomial Logit Model\nTo estimate the parameters of the multinomial logit (MNL) model, we maximize the log-likelihood function based on the observed choices. For each respondent \\(i\\) and choice task \\(t\\), let \\(\\mathcal{J}_{it}\\) be the set of alternatives available (typically 3). Let \\(x_{ijt}\\) be the feature vector of alternative \\(j\\), and \\(\\beta\\) the parameter vector. The utility of alternative \\(j\\) is modeled as:\n\\[\nU_{ijt} = x_{ijt}^\\top \\beta + \\varepsilon_{ijt}\n\\]\nAssuming \\(\\varepsilon_{ijt}\\) follows an i.i.d. Type I Extreme Value distribution, the probability that individual \\(i\\) chooses option \\(j\\) in task \\(t\\) is:\n\\[\n\\mathbb{P}_{ijt} = \\frac{\\exp(x_{ijt}^\\top \\beta)}{\\sum_{k \\in \\mathcal{J}_{it}} \\exp(x_{ikt}^\\top \\beta)}\n\\]\nLet \\(\\delta_{ijt}\\) be an indicator variable equal to 1 if alternative \\(j\\) was chosen by respondent \\(i\\) in task \\(t\\), and 0 otherwise. The log-likelihood across all respondents and tasks is:\n\\[\n\\ell(\\beta) = \\sum_{i} \\sum_{t} \\sum_{j \\in \\mathcal{J}_{it}} \\delta_{ijt} \\cdot \\log \\mathbb{P}_{ijt}\n\\]\nThis is the function we aim to maximize with respect to \\(\\beta\\) using numerical optimization.\n\ndef neg_log_likelihood(beta, X=X_mat, y=y, groups=groups):\n    utilities = X @ beta\n    log_likelihood = 0\n    start = 0\n\n    for _, group in groups:\n        n = group.shape[0]\n        util_slice = utilities[start:start + n]\n        choice_slice = y[start:start + n]\n        denom = np.sum(np.exp(util_slice))\n        probs = np.exp(util_slice) / denom\n        log_likelihood += np.log(probs @ choice_slice)\n        start += n\n\n    return -log_likelihood\n\nThe table below presents the MLEs for each parameter in the MNL model, along with standard errors and 95% confidence intervals. These values are estimated using scipy.optimize.minimize() with the BFGS method.\n\n# Initial guess\ninit_beta = np.zeros(n_params)\n\n# Optimize\nresult = minimize(neg_log_likelihood, init_beta, method='BFGS')\n\n# Estimated betas\nbeta_hat = result.x\nhessian_inv = result.hess_inv\n\n# Standard errors\nse = np.sqrt(np.diag(hessian_inv))\n\n# 95% confidence intervals\nz = 1.96\nci_lower = beta_hat - z * se\nci_upper = beta_hat + z * se\n\n# Combine results\nresults = pd.DataFrame({\n    \"parameter\": features,\n    \"estimate\": beta_hat,\n    \"std_error\": se,\n    \"ci_lower\": ci_lower,\n    \"ci_upper\": ci_upper\n})\n\nfrom IPython.display import display\ndisplay(results.round(4))\n\n\n\n\n\n\n\n\nparameter\nestimate\nstd_error\nci_lower\nci_upper\n\n\n\n\n0\nNetflix\n0.9412\n0.0335\n0.8756\n1.0068\n\n\n1\nPrime\n0.5016\n0.1180\n0.2704\n0.7329\n\n\n2\nAds\n-0.7320\n0.0898\n-0.9080\n-0.5560\n\n\n3\nprice\n-0.0995\n0.0064\n-0.1121\n-0.0868"
  },
  {
    "objectID": "homework/hw3_questions.html#estimation-via-bayesian-methods",
    "href": "homework/hw3_questions.html#estimation-via-bayesian-methods",
    "title": "Multinomial Logit Model",
    "section": "5. Estimation via Bayesian Methods",
    "text": "5. Estimation via Bayesian Methods\nWe now estimate the MNL model using Bayesian inference, via the Metropolis-Hastings (MH) algorithm. We use the same log-likelihood function from the MLE section and combine it with log-priors to compute the unnormalized log-posterior.\nThe priors are: - \\(\\beta_{\\text{Netflix}}, \\beta_{\\text{Prime}}, \\beta_{\\text{Ads}} \\sim \\mathcal{N}(0, 5^2)\\)\n- \\(\\beta_{\\text{price}} \\sim \\mathcal{N}(0, 1^2)\\)\nThe proposal distribution is a multivariate normal with zero covariance, i.e., independent proposals: - First 3 dimensions: \\(\\mathcal{N}(0, 0.05)\\)\n- Last dimension (price): \\(\\mathcal{N}(0, 0.005)\\)\n\n# Prior log densities\ndef log_prior(beta):\n    lp = -0.5 * (beta[0:3]**2 / 25).sum()\n    lp += -0.5 * (beta[3]**2 / 1)\n    return lp\n\n# Log posterior = log likelihood + log prior\ndef log_posterior(beta):\n    return -neg_log_likelihood(beta) + log_prior(beta)\n\n\n# MH sampler\ndef metropolis_sampler(log_post_fn, start, steps=11000):\n    draws = np.zeros((steps, len(start)))\n    draws[0] = start\n    current_lp = log_post_fn(start)\n\n    for t in range(1, steps):\n        # Propose: independent normal steps\n        proposal = draws[t-1] + np.array([\n            np.random.normal(0, np.sqrt(0.05)),\n            np.random.normal(0, np.sqrt(0.05)),\n            np.random.normal(0, np.sqrt(0.05)),\n            np.random.normal(0, np.sqrt(0.005))\n        ])\n\n        proposal_lp = log_post_fn(proposal)\n\n        # MH acceptance\n        log_accept_ratio = proposal_lp - current_lp\n        if np.log(np.random.rand()) &lt; log_accept_ratio:\n            draws[t] = proposal\n            current_lp = proposal_lp\n        else:\n            draws[t] = draws[t-1]\n\n    return draws\n\n\nnp.random.seed(42)\n\nstart_beta = np.zeros(4)\nsamples = metropolis_sampler(log_posterior, start=start_beta, steps=11000)\n\nposterior = samples[1000:]  # remove burn-in\n\n# Compute MCMC acceptance rate\naccepted = np.sum(np.any(samples[1:] != samples[:-1], axis=1))\naccept_rate = accepted / (samples.shape[0] - 1)\nprint(f\"Acceptance rate: {accept_rate:.3f}\")\n\nAcceptance rate: 0.018\n\n\n\nimport matplotlib.pyplot as plt\n\nparam_names = [\"$\\\\beta_{netflix}$\", \"$\\\\beta_{prime}$\", \"$\\\\beta_{ads}$\", \"$\\\\beta_{price}$\"]\n\nfig, axes = plt.subplots(4, 2, figsize=(12, 10))\n\nfor i in range(4):\n    # Trace plot\n    axes[i, 0].plot(posterior[:, i], linewidth=0.7)\n    axes[i, 0].set_title(f\"Trace plot: {param_names[i]}\")\n    axes[i, 0].set_ylabel(\"Value\")\n    axes[i, 0].grid(alpha=0.3)\n\n    # Histogram\n    axes[i, 1].hist(posterior[:, i], bins=30, density=True)\n    axes[i, 1].axvline(posterior[:, i].mean(), color=\"red\", linestyle=\"--\", label=\"Mean\")\n    axes[i, 1].set_title(f\"Posterior histogram: {param_names[i]}\")\n    axes[i, 1].set_xlabel(\"Parameter value\")\n    axes[i, 1].set_ylabel(\"Density\")\n    axes[i, 1].grid(alpha=0.3)\n    axes[i, 1].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nPosterior Diagnostics\nThe trace plots and posterior histograms for all four parameters are shown below. Visually, the trace plots indicate that the Markov chains mix reasonably well and remain stable over time, suggesting adequate convergence. The chains explore the parameter space with some local variability but no major trends or drifts, which supports the credibility of the resulting posterior estimates.\nThe posterior histograms further confirm the patterns observed in the trace plots:\n\n\\(\\beta_{netflix}\\) shows a slightly right-skewed distribution centered near 0.93, with reasonable spread and no multimodality.\n\\(\\beta_{prime}\\) exhibits greater dispersion but maintains a unimodal shape around 0.49.\n\\(\\beta_{ads}\\) is centered tightly around -0.73, with most posterior mass between -0.90 and -0.57.\n\\(\\beta_{price}\\) is the most precise, with a narrow posterior centered around -0.099, confirming strong evidence for price sensitivity.\n\nThese diagnostics suggest that the Metropolis-Hastings sampler successfully captured the posterior distributions for all parameters, with trace plots supporting convergence and histograms indicating stable inference. No signs of severe autocorrelation or non-convergence are evident.\n\nposterior_summary = pd.DataFrame({\n    \"parameter\": features,\n    \"mean\": posterior.mean(axis=0),\n    \"std_dev\": posterior.std(axis=0),\n    \"ci_lower\": np.percentile(posterior, 2.5, axis=0),\n    \"ci_upper\": np.percentile(posterior, 97.5, axis=0)\n}).round(4)\n\ndisplay(posterior_summary)\n\n\n\n\n\n\n\n\nparameter\nmean\nstd_dev\nci_lower\nci_upper\n\n\n\n\n0\nNetflix\n0.9325\n0.1000\n0.7136\n1.1181\n\n\n1\nPrime\n0.4934\n0.0917\n0.3041\n0.6559\n\n\n2\nAds\n-0.7307\n0.0882\n-0.9066\n-0.5691\n\n\n3\nprice\n-0.0994\n0.0061\n-0.1109\n-0.0885\n\n\n\n\n\n\n\nThe table above summarizes the posterior means, standard deviations, and 95% credible intervals for each parameter, based on 10,000 retained samples from the Metropolis-Hastings sampler.\nThese can be directly compared with the MLE results from Section 4 to assess similarity in point estimates and uncertainty.\n\n\nComparison of MLE and Bayesian Estimates\nThe table below compares the parameter estimates obtained via Maximum Likelihood Estimation (MLE) and Bayesian inference (via Metropolis-Hastings MCMC). Overall, the point estimates from both methods are very similar, with only minor differences across parameters. This consistency suggests that the data are informative and the priors used in the Bayesian method are relatively non-influential.\n\nFor \\(\\beta_{netflix}\\), the MLE estimate is 0.9412 (95% CI: [0.8756, 1.0068]), while the Bayesian posterior mean is 0.9325 (95% CI: [0.7136, 1.1181]). Both suggest a strong positive preference for Netflix, and their intervals largely overlap.\nFor \\(\\beta_{prime}\\), both approaches yield moderate positive estimates (MLE: 0.5016; Bayes: 0.4934), with the Bayesian interval slightly narrower due to the influence of the prior.\nFor \\(\\beta_{ads}\\), both estimates indicate a strong negative effect of ads on utility (MLE: -0.7320; Bayes: -0.7307), with virtually identical standard errors and credible/confidence intervals.\nThe coefficient on price is also consistent (MLE: -0.0995; Bayes: -0.0994), confirming that higher prices reduce the probability of choice.\n\nThe Bayesian credible intervals are slightly wider for some parameters (e.g., Netflix), likely reflecting greater uncertainty due to the prior. Overall, both methods tell a coherent story: consumers prefer Netflix and Prime over Hulu, dislike ads, and are price-sensitive."
  },
  {
    "objectID": "homework/hw3_questions.html#discussion",
    "href": "homework/hw3_questions.html#discussion",
    "title": "Multinomial Logit Model",
    "section": "6. Discussion",
    "text": "6. Discussion\n\nInterpretation of Parameter Estimates\nIf we did not know the data were simulated, we would still conclude from the estimates that respondents generally:\n\nPrefer Netflix over Amazon Prime, and both over Hulu (the omitted reference level). This is reflected in the fact that \\(\\beta_{\\text{Netflix}} &gt; \\beta_{\\text{Prime}} &gt; 0\\).\nDislike advertisements, as indicated by the consistently negative \\(\\beta_{\\text{ads}}\\).\nAre price-sensitive, as \\(\\beta_{\\text{price}} &lt; 0\\) implies that the likelihood of choosing an alternative decreases as its price increases.\n\nSpecifically, \\(\\beta_{\\text{Netflix}} &gt; \\beta_{\\text{Prime}}\\) means that, holding other attributes constant, the utility (and therefore the probability of being chosen) is higher for Netflix than Prime. This aligns with common consumer preferences in the streaming market. Likewise, a negative \\(\\beta_{\\text{price}}\\) is consistent with economic theory: higher prices reduce demand.\n\n\nToward a Hierarchical (Multi-level) Model\nIn our current model, all respondents share the same set of preference parameters \\(\\beta\\) — that is, we assume homogeneous preferences across individuals. However, in real-world conjoint studies, different people often have different tastes.\nTo model this heterogeneity, we can move to a hierarchical (random-parameter) model. In this framework, each respondent \\(i\\) has their own parameter vector \\(\\beta_i\\), which is drawn from a population-level distribution:\n\\[\n\\beta_i \\sim \\mathcal{N}(\\mu, \\Sigma)\n\\]\nHere, \\(\\mu\\) represents the average preference across the population, and \\(\\Sigma\\) captures the variation between individuals.\nTo simulate such data, we would: 1. Draw a unique \\(\\beta_i\\) for each respondent from the population distribution. 2. Use that \\(\\beta_i\\) to simulate choices for each of their tasks.\nTo estimate the parameters, we would need to use Bayesian hierarchical methods or maximum simulated likelihood, such as: - MCMC with Gibbs or Hamiltonian sampling - Hierarchical Bayes via Stan or PyMC - Mixed logit estimation (if using frequentist methods)\nThese models are more complex, but they better capture real consumer behavior by accounting for individual-level preference variation.\n\n\nSampler Performance Note\nThe Metropolis-Hastings algorithm yielded an acceptance rate of approximately 1.8%, which is substantially lower than the typical recommended range (20%–40%). This suggests that the proposal distribution may not have been well-tuned for the posterior geometry, potentially resulting in poor mixing and inefficient exploration of the parameter space. While the trace plots do show some movement across the support, future implementations could benefit from increasing the proposal variance or adopting adaptive MCMC methods to improve efficiency."
  },
  {
    "objectID": "homework/hw2_questions.html",
    "href": "homework/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n\nCode\nimport pandas as pd\ndf = pd.read_csv(\"../data/blueprinty.csv\")\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(1, 2, figsize=(10, 4), sharey=True)\n\ndf[df['iscustomer'] == 1]['patents'].hist(ax=ax[0], bins=10, color='skyblue')\nax[0].set_title('Customers')\nax[0].set_xlabel('Number of Patents')\n\ndf[df['iscustomer'] == 0]['patents'].hist(ax=ax[1], bins=10, color='salmon')\nax[1].set_title('Non-Customers')\nax[1].set_xlabel('Number of Patents')\n\nfig.suptitle('Histogram of Patents by Customer Status')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf.groupby('iscustomer')['patents'].mean()\n\n\niscustomer\n0    3.473013\n1    4.133056\nName: patents, dtype: float64\n\n\nCustomers tend to have more patents on average (4.13 vs. 3.47), and their distribution is more spread out, with more firms holding a higher number of patents. Non-customers are more concentrated at lower patent counts.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n\nCode\nimport seaborn as sns\n\nfig, axs = plt.subplots(1, 2, figsize=(12, 4))\n\nsns.countplot(data=df, x='region', hue='iscustomer', ax=axs[0])\naxs[0].set_title('Region by Customer Status')\naxs[0].set_xlabel('Region')\naxs[0].set_ylabel('Count')\naxs[0].legend(title='Customer')\n\nsns.kdeplot(data=df, x='age', hue='iscustomer', ax=axs[1], fill=True)\naxs[1].set_title('Age Distribution by Customer Status')\naxs[1].set_xlabel('Age')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf.groupby('iscustomer')['age'].mean()\n\n\niscustomer\n0    26.101570\n1    26.900208\nName: age, dtype: float64\n\n\nCustomers tend to be slightly older (average age 26.9 vs. 26.1). The regional distribution shows a clear difference: a much higher number of customers come from the Northeast, while other regions like the Midwest and Southwest are dominated by non-customers.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\n\nWe assume the number of patents \\(Y \\sim \\text{Poisson}(\\lambda)\\), with density:\n\\[\nf(Y \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^Y}{Y!}\n\\]\nFor a sample \\(Y_1, Y_2, \\dots, Y_n\\), the log-likelihood is:\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^n \\left( -\\lambda + Y_i \\log \\lambda - \\log(Y_i!) \\right)\n\\]\nLog-Likelihood Function in Python\n\n\nCode\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef poisson_log_likelihood(lmbda, y):\n    y = np.asarray(y)\n    return np.sum(-lmbda + y * np.log(lmbda) - gammaln(y + 1))\n\n\nPlotting the Log-Likelihood Curve\n\n\nCode\nimport matplotlib.pyplot as plt\n\ny = df['patents'].values\nlambdas = np.linspace(0.1, 10, 100)\nlogliks = [poisson_log_likelihood(lmbda, y) for lmbda in lambdas]\n\nplt.plot(lambdas, logliks)\nplt.xlabel(\"λ (lambda)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.title(\"Poisson Log-Likelihood for Different λ\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nThe analytical MLE for \\(\\lambda\\) is simply the sample mean of \\(Y\\), since:\n\\[\n\\hat{\\lambda}_{\\text{MLE}} = \\bar{Y}\n\\] Sample Mean\n\n\nCode\nybar = np.mean(y)\nybar\n\n\n3.6846666666666668\n\n\nNumerical Maximization Using scipy.optimize\n\n\nCode\nfrom scipy.optimize import minimize\n\ndef neg_log_likelihood(lmbda):\n    return -poisson_log_likelihood(lmbda[0], y)\n\nresult = minimize(neg_log_likelihood, x0=[1.0], bounds=[(0.001, None)])\nresult.x[0]\n\n\n3.6846662953477973\n\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n\nWe now extend our Poisson model to allow the rate of patent awards to depend on firm characteristics via:\n\\[\nY_i \\sim \\text{Poisson}(\\lambda_i), \\quad \\text{where } \\lambda_i = \\exp(X_i^\\top \\beta)\n\\]\nCovariates include: age, age squared, region (as dummies), and whether the firm is a Blueprinty customer.\n\n\n\n\nCode\nimport pandas as pd\nimport statsmodels.api as sm\n\ndf['age_squared'] = df['age'] ** 2\nX_df = pd.get_dummies(df[['age', 'age_squared', 'region', 'iscustomer']], drop_first=True)\nX_df = sm.add_constant(X_df)\n\nX_df = X_df.astype(float)\nX = X_df\ny = df['patents'].values\n\n\n\n\n\n\n\nCode\nimport statsmodels.api as sm\n\nmodel = sm.GLM(y, X, family=sm.families.Poisson())\nglm_result = model.fit()\n\nglm_result.summary()\n\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\ny\nNo. Observations:\n1500\n\n\nModel:\nGLM\nDf Residuals:\n1492\n\n\nModel Family:\nPoisson\nDf Model:\n7\n\n\nLink Function:\nLog\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-3258.1\n\n\nDate:\nFri, 16 May 2025\nDeviance:\n2143.3\n\n\nTime:\n16:51:38\nPearson chi2:\n2.07e+03\n\n\nNo. Iterations:\n5\nPseudo R-squ. (CS):\n0.1360\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n-0.5089\n0.183\n-2.778\n0.005\n-0.868\n-0.150\n\n\nage\n0.1486\n0.014\n10.716\n0.000\n0.121\n0.176\n\n\nage_squared\n-0.0030\n0.000\n-11.513\n0.000\n-0.003\n-0.002\n\n\niscustomer\n0.2076\n0.031\n6.719\n0.000\n0.147\n0.268\n\n\nregion_Northeast\n0.0292\n0.044\n0.669\n0.504\n-0.056\n0.115\n\n\nregion_Northwest\n-0.0176\n0.054\n-0.327\n0.744\n-0.123\n0.088\n\n\nregion_South\n0.0566\n0.053\n1.074\n0.283\n-0.047\n0.160\n\n\nregion_Southwest\n0.0506\n0.047\n1.072\n0.284\n-0.042\n0.143\n\n\n\n\n\n\n\n\n\n\nCode\nsummary_df = pd.DataFrame({\n    \"Coefficient\": glm_result.params,\n    \"Std. Error\": glm_result.bse,\n    \"z-value\": glm_result.tvalues,\n    \"p-value\": glm_result.pvalues\n})\n\nsummary_df\n\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\nz-value\np-value\n\n\n\n\nconst\n-0.508920\n0.183179\n-2.778269\n5.464935e-03\n\n\nage\n0.148619\n0.013869\n10.716250\n8.539597e-27\n\n\nage_squared\n-0.002970\n0.000258\n-11.513237\n1.131496e-30\n\n\niscustomer\n0.207591\n0.030895\n6.719179\n1.827509e-11\n\n\nregion_Northeast\n0.029170\n0.043625\n0.668647\n5.037205e-01\n\n\nregion_Northwest\n-0.017575\n0.053781\n-0.326782\n7.438327e-01\n\n\nregion_South\n0.056561\n0.052662\n1.074036\n2.828066e-01\n\n\nregion_Southwest\n0.050576\n0.047198\n1.071568\n2.839141e-01\n\n\n\n\n\n\n\n\n\n\nFirms that are Blueprinty customers are expected to produce about 23%(i.e. exp(0.2076)-1) more patents than non-customers, holding other factors constant. This effect is statistically significant.\nPatent output increases with firm age, but at a decreasing rate—suggesting older firms patent more, but the effect tapers off.\nRegional differences are not statistically significant, indicating little variation in patenting across regions once other firm characteristics are controlled for.\n\n\n\nWe create two fake datasets: - X_0 with iscustomer = 0 for all firms (as if no firm were a customer) - X_1 with iscustomer = 1 for all firms (as if all firms were customers)\nWe use the fitted model to compute predicted number of patents for each case, then take the difference.\n\n\nCode\nX_0 = X.copy()\nX_1 = X.copy()\n\nX_0.loc[:, X.columns.str.contains(\"iscustomer\")] = 0\nX_1.loc[:, X.columns.str.contains(\"iscustomer\")] = 1\n\n# Predict expected patent counts\ny_pred_0 = glm_result.predict(X_0)\ny_pred_1 = glm_result.predict(X_1)\n\n# Difference in predicted patents\ndiff = y_pred_1 - y_pred_0\ndiff.mean()\n\n\n0.7927680710452626\n\n\n\n\n\nOn average, firms are predicted to receive 0.79(i.e. 3.47 * 0.23) more patents over five years if they are Blueprinty customers, compared to if they are not—holding all other firm characteristics constant. This suggests a meaningful positive effect of the software on patenting success."
  },
  {
    "objectID": "homework/hw2_questions.html#blueprinty-case-study",
    "href": "homework/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n\nCode\nimport pandas as pd\ndf = pd.read_csv(\"../data/blueprinty.csv\")\n\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(1, 2, figsize=(10, 4), sharey=True)\n\ndf[df['iscustomer'] == 1]['patents'].hist(ax=ax[0], bins=10, color='skyblue')\nax[0].set_title('Customers')\nax[0].set_xlabel('Number of Patents')\n\ndf[df['iscustomer'] == 0]['patents'].hist(ax=ax[1], bins=10, color='salmon')\nax[1].set_title('Non-Customers')\nax[1].set_xlabel('Number of Patents')\n\nfig.suptitle('Histogram of Patents by Customer Status')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf.groupby('iscustomer')['patents'].mean()\n\n\niscustomer\n0    3.473013\n1    4.133056\nName: patents, dtype: float64\n\n\nCustomers tend to have more patents on average (4.13 vs. 3.47), and their distribution is more spread out, with more firms holding a higher number of patents. Non-customers are more concentrated at lower patent counts.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n\nCode\nimport seaborn as sns\n\nfig, axs = plt.subplots(1, 2, figsize=(12, 4))\n\nsns.countplot(data=df, x='region', hue='iscustomer', ax=axs[0])\naxs[0].set_title('Region by Customer Status')\naxs[0].set_xlabel('Region')\naxs[0].set_ylabel('Count')\naxs[0].legend(title='Customer')\n\nsns.kdeplot(data=df, x='age', hue='iscustomer', ax=axs[1], fill=True)\naxs[1].set_title('Age Distribution by Customer Status')\naxs[1].set_xlabel('Age')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf.groupby('iscustomer')['age'].mean()\n\n\niscustomer\n0    26.101570\n1    26.900208\nName: age, dtype: float64\n\n\nCustomers tend to be slightly older (average age 26.9 vs. 26.1). The regional distribution shows a clear difference: a much higher number of customers come from the Northeast, while other regions like the Midwest and Southwest are dominated by non-customers.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\n\nWe assume the number of patents \\(Y \\sim \\text{Poisson}(\\lambda)\\), with density:\n\\[\nf(Y \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^Y}{Y!}\n\\]\nFor a sample \\(Y_1, Y_2, \\dots, Y_n\\), the log-likelihood is:\n\\[\n\\ell(\\lambda) = \\sum_{i=1}^n \\left( -\\lambda + Y_i \\log \\lambda - \\log(Y_i!) \\right)\n\\]\nLog-Likelihood Function in Python\n\n\nCode\nimport numpy as np\nfrom scipy.special import gammaln\n\ndef poisson_log_likelihood(lmbda, y):\n    y = np.asarray(y)\n    return np.sum(-lmbda + y * np.log(lmbda) - gammaln(y + 1))\n\n\nPlotting the Log-Likelihood Curve\n\n\nCode\nimport matplotlib.pyplot as plt\n\ny = df['patents'].values\nlambdas = np.linspace(0.1, 10, 100)\nlogliks = [poisson_log_likelihood(lmbda, y) for lmbda in lambdas]\n\nplt.plot(lambdas, logliks)\nplt.xlabel(\"λ (lambda)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.title(\"Poisson Log-Likelihood for Different λ\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nThe analytical MLE for \\(\\lambda\\) is simply the sample mean of \\(Y\\), since:\n\\[\n\\hat{\\lambda}_{\\text{MLE}} = \\bar{Y}\n\\] Sample Mean\n\n\nCode\nybar = np.mean(y)\nybar\n\n\n3.6846666666666668\n\n\nNumerical Maximization Using scipy.optimize\n\n\nCode\nfrom scipy.optimize import minimize\n\ndef neg_log_likelihood(lmbda):\n    return -poisson_log_likelihood(lmbda[0], y)\n\nresult = minimize(neg_log_likelihood, x0=[1.0], bounds=[(0.001, None)])\nresult.x[0]\n\n\n3.6846662953477973\n\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n\nWe now extend our Poisson model to allow the rate of patent awards to depend on firm characteristics via:\n\\[\nY_i \\sim \\text{Poisson}(\\lambda_i), \\quad \\text{where } \\lambda_i = \\exp(X_i^\\top \\beta)\n\\]\nCovariates include: age, age squared, region (as dummies), and whether the firm is a Blueprinty customer.\n\n\n\n\nCode\nimport pandas as pd\nimport statsmodels.api as sm\n\ndf['age_squared'] = df['age'] ** 2\nX_df = pd.get_dummies(df[['age', 'age_squared', 'region', 'iscustomer']], drop_first=True)\nX_df = sm.add_constant(X_df)\n\nX_df = X_df.astype(float)\nX = X_df\ny = df['patents'].values\n\n\n\n\n\n\n\nCode\nimport statsmodels.api as sm\n\nmodel = sm.GLM(y, X, family=sm.families.Poisson())\nglm_result = model.fit()\n\nglm_result.summary()\n\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\ny\nNo. Observations:\n1500\n\n\nModel:\nGLM\nDf Residuals:\n1492\n\n\nModel Family:\nPoisson\nDf Model:\n7\n\n\nLink Function:\nLog\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-3258.1\n\n\nDate:\nFri, 16 May 2025\nDeviance:\n2143.3\n\n\nTime:\n16:51:38\nPearson chi2:\n2.07e+03\n\n\nNo. Iterations:\n5\nPseudo R-squ. (CS):\n0.1360\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n-0.5089\n0.183\n-2.778\n0.005\n-0.868\n-0.150\n\n\nage\n0.1486\n0.014\n10.716\n0.000\n0.121\n0.176\n\n\nage_squared\n-0.0030\n0.000\n-11.513\n0.000\n-0.003\n-0.002\n\n\niscustomer\n0.2076\n0.031\n6.719\n0.000\n0.147\n0.268\n\n\nregion_Northeast\n0.0292\n0.044\n0.669\n0.504\n-0.056\n0.115\n\n\nregion_Northwest\n-0.0176\n0.054\n-0.327\n0.744\n-0.123\n0.088\n\n\nregion_South\n0.0566\n0.053\n1.074\n0.283\n-0.047\n0.160\n\n\nregion_Southwest\n0.0506\n0.047\n1.072\n0.284\n-0.042\n0.143\n\n\n\n\n\n\n\n\n\n\nCode\nsummary_df = pd.DataFrame({\n    \"Coefficient\": glm_result.params,\n    \"Std. Error\": glm_result.bse,\n    \"z-value\": glm_result.tvalues,\n    \"p-value\": glm_result.pvalues\n})\n\nsummary_df\n\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\nz-value\np-value\n\n\n\n\nconst\n-0.508920\n0.183179\n-2.778269\n5.464935e-03\n\n\nage\n0.148619\n0.013869\n10.716250\n8.539597e-27\n\n\nage_squared\n-0.002970\n0.000258\n-11.513237\n1.131496e-30\n\n\niscustomer\n0.207591\n0.030895\n6.719179\n1.827509e-11\n\n\nregion_Northeast\n0.029170\n0.043625\n0.668647\n5.037205e-01\n\n\nregion_Northwest\n-0.017575\n0.053781\n-0.326782\n7.438327e-01\n\n\nregion_South\n0.056561\n0.052662\n1.074036\n2.828066e-01\n\n\nregion_Southwest\n0.050576\n0.047198\n1.071568\n2.839141e-01\n\n\n\n\n\n\n\n\n\n\nFirms that are Blueprinty customers are expected to produce about 23%(i.e. exp(0.2076)-1) more patents than non-customers, holding other factors constant. This effect is statistically significant.\nPatent output increases with firm age, but at a decreasing rate—suggesting older firms patent more, but the effect tapers off.\nRegional differences are not statistically significant, indicating little variation in patenting across regions once other firm characteristics are controlled for.\n\n\n\nWe create two fake datasets: - X_0 with iscustomer = 0 for all firms (as if no firm were a customer) - X_1 with iscustomer = 1 for all firms (as if all firms were customers)\nWe use the fitted model to compute predicted number of patents for each case, then take the difference.\n\n\nCode\nX_0 = X.copy()\nX_1 = X.copy()\n\nX_0.loc[:, X.columns.str.contains(\"iscustomer\")] = 0\nX_1.loc[:, X.columns.str.contains(\"iscustomer\")] = 1\n\n# Predict expected patent counts\ny_pred_0 = glm_result.predict(X_0)\ny_pred_1 = glm_result.predict(X_1)\n\n# Difference in predicted patents\ndiff = y_pred_1 - y_pred_0\ndiff.mean()\n\n\n0.7927680710452626\n\n\n\n\n\nOn average, firms are predicted to receive 0.79(i.e. 3.47 * 0.23) more patents over five years if they are Blueprinty customers, compared to if they are not—holding all other firm characteristics constant. This suggests a meaningful positive effect of the software on patenting success."
  },
  {
    "objectID": "homework/hw2_questions.html#airbnb-case-study",
    "href": "homework/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\n\nLoad and Clean Data\n\n\nCode\nimport pandas as pd\n\ndf = pd.read_csv(\"../data/airbnb.csv\")\n\n# Drop rows with missing values in relevant columns\ndf = df[['days', 'room_type', 'bathrooms', 'bedrooms', 'price', 'number_of_reviews',\n         'review_scores_cleanliness', 'review_scores_location', 'review_scores_value',\n         'instant_bookable']].dropna()\n\n# Convert instant_bookable to binary\ndf['instant_bookable'] = df['instant_bookable'].map({'t': 1, 'f': 0})\n\n\n\n\nExploratory Data Analysis\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Correlation heatmap for numeric features\nnumeric_cols = df.select_dtypes(include='number').columns\ncorr = df[numeric_cols].corr()\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(corr, annot=True, cmap='coolwarm')\nplt.title(\"Correlation Heatmap of Numeric Variables\")\nplt.show()\n\n# Dashboard 1: Scatter plots of numeric predictors vs number_of_reviews\nfig, axs = plt.subplots(1, 3, figsize=(18, 5))\nsns.scatterplot(x=df['days'], y=df['number_of_reviews'], ax=axs[0])\naxs[0].set_title(\"Reviews vs Days\")\n\nsns.scatterplot(x=df['price'], y=df['number_of_reviews'], ax=axs[1])\naxs[1].set_title(\"Reviews vs Price\")\n\nsns.scatterplot(x=df['review_scores_location'], y=df['number_of_reviews'], ax=axs[2])\naxs[2].set_title(\"Reviews vs Location Score\")\n\nplt.tight_layout()\nplt.show()\n\n# Dashboard 2: Boxplot of reviews by room type\nplt.figure(figsize=(6, 5))\nsns.boxplot(data=df, x='room_type', y='number_of_reviews')\nplt.title(\"Number of Reviews by Room Type\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nReview score variables are correlated; don’t include all of them together.\nNumber of reviews is highly skewed — Poisson is reasonable, but check overdispersion.\nHigher prices tend to get fewer reviews; log-transforming price is helpful.\nReviews increase with location score, especially at high values.\nShared rooms get fewer reviews; room type matters and may interact with other variables.\n\n\n\nFit Poisson Regression Model\n\n\nCode\nimport statsmodels.api as sm\n\n# Log and squared transforms\ndf['log_price'] = np.log1p(df['price'])\ndf['log_days'] = np.log1p(df['days'])\n\n# Interaction terms\ndf['price_x_days'] = df['price'] * df['days']\ndf['bookable_x_room'] = df['instant_bookable'] * (df['room_type'] == 'Entire home/apt').astype(int)\n\n\n# Define predictors\nX = df[['log_days','log_price',\n        'review_scores_value', 'instant_bookable',\n        'price_x_days', 'bookable_x_room']]\n\n# Add room type dummies\nroom_dummies = pd.get_dummies(df[['room_type','bedrooms', 'bathrooms']], drop_first=True)\nX = pd.concat([X, room_dummies], axis=1)\n\n# Add intercept\nX = sm.add_constant(X)\nX = X.astype(float)\ny = df['number_of_reviews']\n\nmodel = sm.GLM(y, X, family=sm.families.Poisson())\nglm_result = model.fit()\nglm_result.summary()\n\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\nnumber_of_reviews\nNo. Observations:\n30160\n\n\nModel:\nGLM\nDf Residuals:\n30149\n\n\nModel Family:\nPoisson\nDf Model:\n10\n\n\nLink Function:\nLog\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-4.8964e+05\n\n\nDate:\nFri, 16 May 2025\nDeviance:\n8.5782e+05\n\n\nTime:\n16:51:39\nPearson chi2:\n1.22e+06\n\n\nNo. Iterations:\n7\nPseudo R-squ. (CS):\n0.9680\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n-1.0944\n0.027\n-40.254\n0.000\n-1.148\n-1.041\n\n\nlog_days\n0.5646\n0.002\n249.950\n0.000\n0.560\n0.569\n\n\nlog_price\n0.1275\n0.003\n39.293\n0.000\n0.121\n0.134\n\n\nreview_scores_value\n-0.0465\n0.001\n-33.716\n0.000\n-0.049\n-0.044\n\n\ninstant_bookable\n0.4976\n0.004\n124.468\n0.000\n0.490\n0.505\n\n\nprice_x_days\n-2.755e-07\n6.81e-09\n-40.463\n0.000\n-2.89e-07\n-2.62e-07\n\n\nbookable_x_room\n0.0230\n0.006\n3.971\n0.000\n0.012\n0.034\n\n\nbedrooms\n0.0774\n0.002\n37.365\n0.000\n0.073\n0.081\n\n\nbathrooms\n-0.1074\n0.004\n-28.239\n0.000\n-0.115\n-0.100\n\n\nroom_type_Private room\n0.0862\n0.004\n23.458\n0.000\n0.079\n0.093\n\n\nroom_type_Shared room\n0.0090\n0.009\n0.978\n0.328\n-0.009\n0.027\n\n\n\n\n\n\n\nShow Coefficients and Exponentiated Effects\n\n\nCode\ncoef_df = pd.DataFrame({\n    'Coefficient': glm_result.params,\n    'Exp(Coefficient)': np.exp(glm_result.params),\n    'p-value': glm_result.pvalues\n})\n\ncoef_df.sort_values('p-value')  # sort by significance\n\n\n\n\n\n\n\n\n\nCoefficient\nExp(Coefficient)\np-value\n\n\n\n\nconst\n-1.094405e+00\n0.334739\n0.000000e+00\n\n\nlog_days\n5.645793e-01\n1.758708\n0.000000e+00\n\n\nlog_price\n1.274563e-01\n1.135935\n0.000000e+00\n\n\ninstant_bookable\n4.975882e-01\n1.644750\n0.000000e+00\n\n\nprice_x_days\n-2.754803e-07\n1.000000\n0.000000e+00\n\n\nbedrooms\n7.738920e-02\n1.080463\n1.470202e-305\n\n\nreview_scores_value\n-4.651199e-02\n0.954553\n3.400566e-249\n\n\nbathrooms\n-1.074441e-01\n0.898127\n1.967010e-175\n\n\nroom_type_Private room\n8.620330e-02\n1.090028\n1.092157e-121\n\n\nbookable_x_room\n2.297251e-02\n1.023238\n7.168259e-05\n\n\nroom_type_Shared room\n9.039075e-03\n1.009080\n3.280893e-01\n\n\n\n\n\n\n\n\n\nInterpretation\nThe Poisson regression results show that several listing features significantly affect the expected number of reviews:\n\nLog Days has the strongest effect — older listings get substantially more reviews.\nLog Price is positively associated with review count, suggesting that higher-priced listings may attract more engagement.\nInstant Bookable listings receive about 64% more reviews, highlighting convenience as a key factor.\nBedrooms have a moderate positive effect, while bathrooms show a small but negative association.\nReview Score (Value) is negatively associated with reviews, which may reflect multicollinearity with other score metrics.\nRoom Type matters: private rooms receive slightly more reviews, but shared rooms are not significantly different from the baseline (entire home/apt).\nThe interaction terms (price × days, bookable × room) have statistically significant but small effects.\n\nOverall, the model fits well, and the most impactful predictors are listing age, instant bookability, and price."
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Miya(Mia) Huang",
    "section": "",
    "text": "Welcome!"
  },
  {
    "objectID": "homework/hw1_questions.html",
    "href": "homework/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nIn this field experiment, each donor in the sample was randomly assigned to receive one of several versions of a fundraising letter. The control group received the nonprofit’s standard appeal—a typical letter asking for support, with no mention of any special incentives.\nThe treatment group, on the other hand, received letters that included a matching grant offer. These letters stated that a concerned member of the organization would match the recipient’s donation at a fixed rate. Every dollar donated would be matched immediately, increasing the total contribution the organization would receive.\nThe treatment letters varied across three key dimensions:\n\nMatching ratio\nDonors were randomly assigned to receive a match offer of 1:1, 2:1, or 3:1, meaning the organization would receive $2, $3, or $4 for every $1 donated.\nMaximum match amount\nThe total available match funding was stated as either $25,000, $50,000, $100,000, or left unstated.\nSuggested donation amount\nEach letter included a reply card with three suggested gift levels based on the donor’s previous highest contribution: the same amount, 1.25×, or 1.5×. One of these was used as an example in the match statement.\n\nAside from these randomized variations, all letters were identical in format, tone, and content. This design allowed the researchers to isolate the effect of the matching grant mechanism and its specific features on donor behavior.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "homework/hw1_questions.html#introduction",
    "href": "homework/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nIn this field experiment, each donor in the sample was randomly assigned to receive one of several versions of a fundraising letter. The control group received the nonprofit’s standard appeal—a typical letter asking for support, with no mention of any special incentives.\nThe treatment group, on the other hand, received letters that included a matching grant offer. These letters stated that a concerned member of the organization would match the recipient’s donation at a fixed rate. Every dollar donated would be matched immediately, increasing the total contribution the organization would receive.\nThe treatment letters varied across three key dimensions:\n\nMatching ratio\nDonors were randomly assigned to receive a match offer of 1:1, 2:1, or 3:1, meaning the organization would receive $2, $3, or $4 for every $1 donated.\nMaximum match amount\nThe total available match funding was stated as either $25,000, $50,000, $100,000, or left unstated.\nSuggested donation amount\nEach letter included a reply card with three suggested gift levels based on the donor’s previous highest contribution: the same amount, 1.25×, or 1.5×. One of these was used as an example in the match statement.\n\nAside from these randomized variations, all letters were identical in format, tone, and content. This design allowed the researchers to isolate the effect of the matching grant mechanism and its specific features on donor behavior.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "homework/hw1_questions.html#data",
    "href": "homework/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\nThe dataset consists of 50,083 observations and 51 variables, representing prior donors who were part of a large-scale direct mail fundraising experiment. Each row corresponds to an individual who received a solicitation letter, and the variables capture both treatment assignments and individual characteristics.\nThe bar plots show the distribution of key categorical variables in the dataset. The sample is unbalanced across treatment groups by design, with approximately two-thirds assigned to treatment. Only 2.1% of individuals donated, indicating a low response rate. Most of the sample resides in blue states (59.5%), and the majority of donors are male (70.6%) and not donating as a couple (88.7%).\nThe histograms of numeric variables show that both donation-related and historical giving behaviors are highly skewed to the right. Most donation amounts are under $100, and both highest past donation and frequency of past donations exhibit long right tails, suggesting that a small subset of donors account for a disproportionately large share of past giving. The recency variable (mrm2) also shows many donors who haven’t contributed recently, with the most frequent values clustered between 0 and 12 months.\n\n\nCode\nimport pandas as pd\ndf = pd.read_stata(\"../data/karlan_list_2007.dta\")\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# categorical\ncategorical_vars = ['treatment', 'gave', 'red0', 'female', 'couple']\nlabel_map = {\n    'treatment': {0: 'Control', 1: 'Treatment'},\n    'gave': {0: 'Did Not Give', 1: 'Gave'},\n    'red0': {0: 'Blue State', 1: 'Red State'},\n    'female': {0: 'Male', 1: 'Female'},\n    'couple': {0: 'Single', 1: 'Couple'}\n}\n\nfig, axes = plt.subplots(2, 3, figsize=(16, 8))\naxes = axes.flatten()\n\nfor i, var in enumerate(categorical_vars):\n    ax = axes[i]\n    counts = df[var].value_counts().sort_index()\n    labels = list(label_map[var].values())\n    bars = ax.bar(labels, counts.values, color=sns.color_palette(\"pastel\"))\n\n    for bar, count in zip(bars, counts.values):\n        pct = count / df.shape[0]\n        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height(), f'{pct:.1%}',\n                ha='center', va='bottom', fontsize=9)\n\n    ax.set_title(f\"{var.capitalize()} Distribution\")\n    ax.set_ylabel(\"Count\")\n\nif len(categorical_vars) &lt; len(axes):\n    fig.delaxes(axes[-1])\n\nplt.tight_layout()\nplt.show()\n\n# numeric\nnumeric_vars = ['amount', 'hpa', 'mrm2', 'freq']\ntitles = {\n    'amount': 'Donation Amount',\n    'hpa': 'Highest Past Donation',\n    'mrm2': 'Months Since Last Donation',\n    'freq': 'Donation Frequency'\n}\n\nxlims = {\n    'amount': (0, 200),\n    'hpa': (0, 250),\n    'mrm2': (0, 60),\n    'freq': (0, 50)\n}\n\nfig, axes = plt.subplots(2, 2, figsize=(12, 6))\naxes = axes.flatten()\n\nfor i, var in enumerate(numeric_vars):\n    ax = axes[i]\n    data = df[df['gave'] == 1][var] if var == 'amount' else df[var]\n    \n    sns.histplot(data, bins=30, kde=True, ax=ax, color='skyblue')\n    ax.set_title(titles[var])\n    ax.set_xlabel(var)\n    ax.set_ylabel(\"Frequency\")\n    ax.set_xlim(xlims[var])\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\n\nCode\nimport pandas as pd\ndf = pd.read_stata(\"../data/karlan_list_2007.dta\")  \n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport statsmodels.api as sm\nfrom scipy.stats import t as t_dist\n\n# Variables\ntest_vars = ['mrm2', 'hpa', 'freq']\nvar_labels = {\n    'mrm2': 'Months Since Last Donation',\n    'hpa': 'Highest Past Donation',\n    'freq': 'Donation Frequency'\n}\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\naxes = axes.flatten()\n\nfor i, var in enumerate(test_vars):\n    means = [\n        df[df['treatment'] == 0][var].mean(),\n        df[df['treatment'] == 1][var].mean()\n    ]\n    labels = ['Control', 'Treatment']\n    colors = ['#6baed6', '#fd8d3c']\n\n    bars = axes[i].bar(labels, means, color=colors)\n    axes[i].set_title(var_labels[var])\n    axes[i].set_ylabel(\"Mean\")\n\n    # Add value labels\n    for bar, mean in zip(bars, means):\n        axes[i].text(bar.get_x() + bar.get_width()/2, bar.get_height(),\n                     f\"{mean:.2f}\", ha='center', va='bottom', fontsize=9)\n\nplt.tight_layout()\nplt.show()\n\n# t-test and regression comparison\nresults = []\n\nfor var in test_vars:\n    x1 = df[df[\"treatment\"] == 1][var].dropna()\n    x0 = df[df[\"treatment\"] == 0][var].dropna()\n\n    mean1 = x1.mean()\n    mean0 = x0.mean()\n    diff = mean1 - mean0\n    var1 = x1.var(ddof=1)\n    var0 = x0.var(ddof=1)\n    n1 = len(x1)\n    n0 = len(x0)\n\n    se = np.sqrt(var1 / n1 + var0 / n0)\n    t_stat = diff / se\n\n    df_denom = (var1/n1 + var0/n0)**2 / ((var1**2 / (n1**2 * (n1 - 1))) + (var0**2 / (n0**2 * (n0 - 1))))\n    ttest_p = 2 * (1 - t_dist.cdf(abs(t_stat), df=df_denom))\n\n    X = sm.add_constant(df[['treatment']])\n    model = sm.OLS(df[var], X, missing='drop').fit()\n    reg_t = model.tvalues['treatment']\n    reg_p = model.pvalues['treatment']\n\n    results.append({\n        'Variable': var,\n        'Mean_Treatment': round(mean1, 3),\n        'Mean_Control': round(mean0, 3),\n        'Difference': round(diff, 3),\n        'T-test t': round(t_stat, 3),\n        'T-test p': round(ttest_p, 4),\n        'Reg t': round(reg_t, 3),\n        'Reg p-value': round(reg_p, 4)\n    })\n\npd.DataFrame(results)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nMean_Treatment\nMean_Control\nDifference\nT-test t\nT-test p\nReg t\nReg p-value\n\n\n\n\n0\nmrm2\n13.012\n12.998000\n0.014\n0.120\n0.9049\n0.119\n0.9049\n\n\n1\nhpa\n59.597\n58.959999\n0.637\n0.970\n0.3318\n0.944\n0.3451\n\n\n2\nfreq\n8.035\n8.047000\n-0.012\n-0.111\n0.9117\n-0.111\n0.9117\n\n\n\n\n\n\n\nThe bar plots visually confirm that the treatment and control groups are nearly identical in their pre-treatment characteristics. There is no meaningful difference in the average months since last donation, highest past donation, or donation frequency.\nStatistical tests further support this conclusion. Both t-tests (using the class formula) and simple regressions yield non-significant results at the 95% confidence level, with very small t-statistics and large p-values across all variables.\nThese findings align with Table 1 in Karlan and List (2007), providing strong evidence that the random assignment was successfully implemented and that treatment and control groups are balanced in observable covariates. This supports the internal validity of any subsequent causal analysis."
  },
  {
    "objectID": "homework/hw1_questions.html#experimental-results",
    "href": "homework/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf = pd.read_stata(\"../data/karlan_list_2007.dta\")\n\ndonation_rates = df.groupby('treatment')['gave'].mean()\nlabels = ['Control', 'Treatment']\ncolors = ['#6baed6', '#fd8d3c']\n\n# Plot\nfig, ax = plt.subplots(figsize=(6, 4))\nbars = ax.bar(labels, donation_rates.values, color=colors)\n\nfor bar, rate in zip(bars, donation_rates.values):\n    ax.text(bar.get_x() + bar.get_width() / 2,\n            bar.get_height() + 0.002,\n            f\"{rate:.2%}\", ha='center', va='bottom', fontsize=10)\n\nax.set_ylabel(\"Proportion Donated\")\nax.set_title(\"Donation Rate by Treatment Group\")\nplt.ylim(0, max(donation_rates.values) + 0.01)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nThe treatment group has a higher donation rate (2.20%) than the control group (1.79%), suggesting a positive effect of the matching offer on donor response.\n\n\nCode\nimport pandas as pd\ndf = pd.read_stata(\"../data/karlan_list_2007.dta\")\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom scipy.stats import t as t_dist\n\ny_treat = df[df[\"treatment\"] == 1][\"gave\"]\ny_ctrl = df[df[\"treatment\"] == 0][\"gave\"]\n\nmean_treat = y_treat.mean()\nmean_ctrl = y_ctrl.mean()\ndiff = mean_treat - mean_ctrl\nn_treat = len(y_treat)\nn_ctrl = len(y_ctrl)\nvar_treat = y_treat.var(ddof=1)\nvar_ctrl = y_ctrl.var(ddof=1)\n\nse = np.sqrt(var_treat / n_treat + var_ctrl / n_ctrl)\nt_stat = diff / se\ndf_denom = (var_treat/n_treat + var_ctrl/n_ctrl)**2 / (\n    (var_treat**2 / (n_treat**2 * (n_treat - 1))) +\n    (var_ctrl**2 / (n_ctrl**2 * (n_ctrl - 1)))\n)\np_val = 2 * (1 - t_dist.cdf(abs(t_stat), df=df_denom))\n\nX = sm.add_constant(df[\"treatment\"])\nmodel = sm.OLS(df[\"gave\"], X, missing ='drop').fit()\n\n# results\nprint(\"T-test result:\")\nprint(f\"  • Mean difference in donation rate (Treatment - Control): { round(diff, 4)}\")\nprint(f\"  • t-statistic: {round(t_stat, 3)}\")\nprint(f\"  • p-value: {round(p_val, 4)}\")\n\nprint(\"\\nLinear regression result:\")\nprint(f\"  • Coefficient on treatment: {round(model.params['treatment'], 4)}\")\nprint(f\"  • t-statistic: {round(model.tvalues['treatment'], 3)}\")\nprint(f\"  • p-value: {round(model.pvalues['treatment'], 4)}\")\n\n\nT-test result:\n  • Mean difference in donation rate (Treatment - Control): 0.0042\n  • t-statistic: 3.209\n  • p-value: 0.0013\n\nLinear regression result:\n  • Coefficient on treatment: 0.0042\n  • t-statistic: 3.101\n  • p-value: 0.0019\n\n\nBoth the t-test and the linear regression show that the treatment group had a higher donation rate than the control group, and this difference is statistically significant at the 1% level. Specifically, receiving a matching grant offer increases the probability of donating by about 0.42 percentage points—rising from roughly 1.79% in the control group to 2.21% in the treatment group. While this absolute change may appear small, it represents a relative increase of over 20%, which is substantial given the typically low baseline response rate in charitable giving.\nTo further support this finding, I estimate a probit model where the outcome is whether a donation was made and the explanatory variable is treatment assignment. The coefficient on treatment is 0.087, which is also statistically significant at the 1% level. Although the coefficient cannot be interpreted directly as a probability, it indicates an increase in the latent propensity to donate among individuals who received the matching message.\nTaken together, the results provide consistent evidence that even modest psychological framing—such as mentioning that a donation will be matched—can meaningfully increase donor response.\n\n\nCode\nimport pandas as pd\ndf = pd.read_stata(\"../data/karlan_list_2007.dta\")\n\nfrom statsmodels.discrete.discrete_model import Probit\nimport statsmodels.api as sm\n\nX = sm.add_constant(df['treatment'])\nprobit_model = Probit(df['gave'], X).fit()\n\nprint(probit_model.summary())\n\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n                          Probit Regression Results                           \n==============================================================================\nDep. Variable:                   gave   No. Observations:                50083\nModel:                         Probit   Df Residuals:                    50081\nMethod:                           MLE   Df Model:                            1\nDate:                Fri, 16 May 2025   Pseudo R-squ.:               0.0009783\nTime:                        16:51:44   Log-Likelihood:                -5030.5\nconverged:                       True   LL-Null:                       -5035.4\nCovariance Type:            nonrobust   LLR p-value:                  0.001696\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -2.1001      0.023    -90.073      0.000      -2.146      -2.054\ntreatment      0.0868      0.028      3.113      0.002       0.032       0.141\n==============================================================================\n\n\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\nI conduct three pairwise t-tests comparing donation rates between match ratios (2:1 vs. 1:1, 3:1 vs. 1:1, and 3:1 vs. 2:1). The differences in mean donation rates across these groups are all very small and none are statistically significant at the 5% level. All p-values are well above 0.30. This suggests that the presence of a match itself may be sufficient to motivate behavior, while increasing the match ratio beyond 1:1 offers little incremental benefit.\n\n\nCode\nimport pandas as pd\ndf = pd.read_stata(\"../data/karlan_list_2007.dta\")\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import t as t_dist\n\ntreatment_df = df[df[\"treatment\"] == 1]\n\nratios = {\n    1: treatment_df[treatment_df[\"ratio\"] == 1][\"gave\"],\n    2: treatment_df[treatment_df[\"ratio\"] == 2][\"gave\"],\n    3: treatment_df[treatment_df[\"ratio\"] == 3][\"gave\"]\n}\n\n# t-test\ndef manual_ttest(group1, group2):\n    mean_diff = group1.mean() - group2.mean()\n    se = np.sqrt(group1.var(ddof=1)/len(group1) + group2.var(ddof=1)/len(group2))\n    t_stat = mean_diff / se\n    df_denom = (group1.var(ddof=1)/len(group1) + group2.var(ddof=1)/len(group2))**2 / (\n        (group1.var(ddof=1)**2 / ((len(group1)**2)*(len(group1)-1))) + \n        (group2.var(ddof=1)**2 / ((len(group2)**2)*(len(group2)-1)))\n    )\n    p_val = 2 * (1 - t_dist.cdf(abs(t_stat), df=df_denom))\n    return round(mean_diff, 4), round(t_stat, 3), round(p_val, 4)\n\nresults = []\nfor high, low in [(2, 1), (3, 1), (3, 2)]:\n    diff, t_stat, p_val = manual_ttest(ratios[high], ratios[low])\n    results.append({\n        \"Comparison\": f\"{high}:1 vs {low}:1\",\n        \"Mean Diff\": diff,\n        \"T-stat\": t_stat,\n        \"P-value\": p_val\n    })\n\npd.DataFrame(results)\n\n\n\n\n\n\n\n\n\nComparison\nMean Diff\nT-stat\nP-value\n\n\n\n\n0\n2:1 vs 1:1\n0.0019\n0.965\n0.3345\n\n\n1\n3:1 vs 1:1\n0.0020\n1.015\n0.3101\n\n\n2\n3:1 vs 2:1\n0.0001\n0.050\n0.9600\n\n\n\n\n\n\n\nTo determine whether higher match ratios influence the likelihood of donating, I regress the binary outcome gave on the categorical variable ratio, using only the treatment group. The 1:1 match ratio serves as the reference category, and the model estimates how donation rates differ under 2:1 and 3:1 matching offers.\n\n\nCode\nimport pandas as pd\ndf = pd.read_stata(\"../data/karlan_list_2007.dta\")\n\ntreat_df = df[df[\"ratio\"].isin([1, 2, 3])].copy()\n\ntreat_df[\"ratio\"] = treat_df[\"ratio\"].astype(\"category\")\n\nimport statsmodels.formula.api as smf\nmodel = smf.ols(\"gave ~ C(ratio, Treatment(reference=1))\", data=treat_df).fit()\n\nparams = model.params\npvals = model.pvalues\n\nprint(\"Linear regression comparing match ratios (baseline: 1:1):\")\nprint(f\"  • Intercept (1:1 match): {params['Intercept']:.4f}\")\nprint(f\"  • 2:1 vs 1:1 match: Coef = {params['C(ratio, Treatment(reference=1))[T.2]']:.4f}, p = {pvals['C(ratio, Treatment(reference=1))[T.2]']:.4f}\")\nprint(f\"  • 3:1 vs 1:1 match: Coef = {params['C(ratio, Treatment(reference=1))[T.3]']:.4f}, p = {pvals['C(ratio, Treatment(reference=1))[T.3]']:.4f}\")\n\n\nLinear regression comparing match ratios (baseline: 1:1):\n  • Intercept (1:1 match): 0.0207\n  • 2:1 vs 1:1 match: Coef = 0.0019, p = 0.3383\n  • 3:1 vs 1:1 match: Coef = 0.0020, p = 0.3133\n\n\nThe estimated coefficients for ratio2 and ratio3 are positive, suggesting slightly higher donation rates compared to the 1:1 baseline. However, both coefficients are not statistically significant (p-values &gt; 0.3), indicating that the observed differences could be due to random variation rather than a true treatment effect.\nTo assess whether increasing the match ratio leads to higher response, I compared donation rates directly from the data and from the regression coefficients.\n\n\nCode\nimport pandas as pd\ndf = pd.read_stata(\"../data/karlan_list_2007.dta\")\n\nrate_1 = df[(df[\"treatment\"] == 1) & (df[\"ratio\"] == 1)][\"gave\"].mean()\nrate_2 = df[(df[\"treatment\"] == 1) & (df[\"ratio\"] == 2)][\"gave\"].mean()\nrate_3 = df[(df[\"treatment\"] == 1) & (df[\"ratio\"] == 3)][\"gave\"].mean()\n\ndiff_2_vs_1_data = rate_2 - rate_1\ndiff_3_vs_2_data = rate_3 - rate_2\n\ncoef_2 = model.params['C(ratio, Treatment(reference=1))[T.2]']\ncoef_3 = model.params['C(ratio, Treatment(reference=1))[T.3]']\ndiff_3_vs_2_coef = coef_3 - coef_2\n\nprint(\"Donation rate differences (direct from data):\")\nprint(f\"  • 2:1 vs 1:1: {round(diff_2_vs_1_data, 4)}\")\nprint(f\"  • 3:1 vs 2:1: {round(diff_3_vs_2_data, 4)}\")\n\nprint(\"\\nEstimated difference from regression coefficients:\")\nprint(f\"  • 2:1 vs 1:1: {round(coef_2, 4)}\")\nprint(f\"  • 3:1 vs 2:1: {round(diff_3_vs_2_coef, 4)}\")\n\n\nDonation rate differences (direct from data):\n  • 2:1 vs 1:1: 0.0019\n  • 3:1 vs 2:1: 0.0001\n\nEstimated difference from regression coefficients:\n  • 2:1 vs 1:1: 0.0019\n  • 3:1 vs 2:1: 0.0001\n\n\nMoving from a 1:1 to a 2:1 match produced a small increase of 0.19%, while the difference between 3:1 and 2:1 was nearly zero (0.01%). These results were mirrored exactly in the regression estimates.\nIn practical terms, offering a match matters, but increasing the match size does not appear to further improve donation response. For fundraisers, this suggests that a basic 1:1 match may be just as effective as higher ratios, but at a lower cost.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\nI regress donation amount on the match ratio using the treatment group, with 1:1 as the reference group. The baseline average donation under 1:1 is $0.94. Compared to this:\n\nThe 2:1 group gives $0.09 more (p = 0.46)\nThe 3:1 group gives almost the same (p = 0.99)\n\nThis suggests that higher match ratios do not meaningfully affect how much people donate, even if they influence whether they donate at all.\n\n\nCode\nimport pandas as pd\ndf = pd.read_stata(\"../data/karlan_list_2007.dta\")\n\nimport statsmodels.formula.api as smf\n\namount_df = df[(df[\"treatment\"] == 1) & (df[\"ratio\"].isin([1, 2, 3]))].copy()\n\n\namount_df[\"ratio\"] = amount_df[\"ratio\"].astype(\"category\")\namount_df[\"ratio\"] = amount_df[\"ratio\"].cat.remove_unused_categories()\n\nmodel_amount = smf.ols(\"amount ~ C(ratio, Treatment(reference=1))\", data=amount_df).fit()\n\nmodel_amount.summary()\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\namount\nR-squared:\n0.000\n\n\nModel:\nOLS\nAdj. R-squared:\n-0.000\n\n\nMethod:\nLeast Squares\nF-statistic:\n0.3651\n\n\nDate:\nFri, 16 May 2025\nProb (F-statistic):\n0.694\n\n\nTime:\n16:51:44\nLog-Likelihood:\n-1.2063e+05\n\n\nNo. Observations:\n33396\nAIC:\n2.413e+05\n\n\nDf Residuals:\n33393\nBIC:\n2.413e+05\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n0.9367\n0.085\n11.026\n0.000\n0.770\n1.103\n\n\nC(ratio, Treatment(reference=1))[T.2]\n0.0895\n0.120\n0.745\n0.456\n-0.146\n0.325\n\n\nC(ratio, Treatment(reference=1))[T.3]\n0.0011\n0.120\n0.009\n0.993\n-0.234\n0.237\n\n\n\n\n\n\n\n\nOmnibus:\n64921.717\nDurbin-Watson:\n2.010\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n172634606.522\n\n\nSkew:\n15.430\nProb(JB):\n0.00\n\n\nKurtosis:\n353.872\nCond. No.\n3.73\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n I regress donation amount on the match ratio using only individuals who made a donation. The average donation in the 1:1 group is $45.14. Compared to this:\n\nThe 2:1 group gives $0.19 more (p = 0.96)\nThe 3:1 group gives $3.89 less (p = 0.31)\n\nThese differences are small and statistically insignificant. This suggests that once someone decides to donate, the size of the match does not influence how much they give.\n\n\nCode\nimport pandas as pd\ndf = pd.read_stata(\"../data/karlan_list_2007.dta\")\n\nimport statsmodels.formula.api as smf\n\namount_df = df[(df[\"treatment\"] == 1) & (df[\"amount\"] &gt; 0) & (df[\"ratio\"].isin([1, 2, 3]))].copy()\n\namount_df[\"ratio\"] = amount_df[\"ratio\"].astype(\"category\")\namount_df[\"ratio\"] = amount_df[\"ratio\"].cat.remove_unused_categories()\n\nmodel_amount = smf.ols(\"amount ~ C(ratio, Treatment(reference=1))\", data=amount_df).fit()\n\nmodel_amount.summary()\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\namount\nR-squared:\n0.002\n\n\nModel:\nOLS\nAdj. R-squared:\n-0.001\n\n\nMethod:\nLeast Squares\nF-statistic:\n0.7504\n\n\nDate:\nFri, 16 May 2025\nProb (F-statistic):\n0.473\n\n\nTime:\n16:51:44\nLog-Likelihood:\n-3794.3\n\n\nNo. Observations:\n736\nAIC:\n7595.\n\n\nDf Residuals:\n733\nBIC:\n7608.\n\n\nDf Model:\n2\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n45.1429\n2.765\n16.324\n0.000\n39.714\n50.572\n\n\nC(ratio, Treatment(reference=1))[T.2]\n0.1944\n3.829\n0.051\n0.960\n-7.322\n7.711\n\n\nC(ratio, Treatment(reference=1))[T.3]\n-3.8911\n3.825\n-1.017\n0.309\n-11.400\n3.618\n\n\n\n\n\n\n\n\nOmnibus:\n458.865\nDurbin-Watson:\n1.918\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n5192.758\n\n\nSkew:\n2.659\nProb(JB):\n0.00\n\n\nKurtosis:\n14.876\nCond. No.\n3.83\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n The histograms display the distribution of donation amounts among individuals who donated, separated by treatment and control groups. Both distributions are heavily right-skewed, with most donations concentrated under $100.\nThe treatment group’s average donation is $43.87, while the control group’s average is slightly higher at $45.54. Despite the visual and numerical difference, the gap is small and not statistically significant, reinforcing earlier regression results that match offers influence whether people give, but not how much they give once they’ve decided to donate.\n\n\nCode\nimport pandas as pd\ndf = pd.read_stata(\"../data/karlan_list_2007.dta\")\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndonors = df[df[\"amount\"] &gt; 0]\n\ndonors_treat = donors[donors[\"treatment\"] == 1]\ndonors_ctrl = donors[donors[\"treatment\"] == 0]\n\nmean_treat = donors_treat[\"amount\"].mean()\nmean_ctrl = donors_ctrl[\"amount\"].mean()\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\n\n# treatment group\nsns.histplot(donors_treat[\"amount\"], bins=30, ax=axes[0], color='skyblue')\naxes[0].axvline(mean_treat, color='red', linestyle='--')  # Add vertical line for mean\naxes[0].set_title(\"Treatment\")\naxes[0].set_xlabel(\"Donation Amount\")\naxes[0].set_ylabel(\"Number of Donors\")\naxes[0].text(mean_treat + 2, axes[0].get_ylim()[1]*0.9, f\"Mean = ${mean_treat:.2f}\", color='red')\n\n# control group\nsns.histplot(donors_ctrl[\"amount\"], bins=30, ax=axes[1], color='lightgreen')\naxes[1].axvline(mean_ctrl, color='red', linestyle='--')\naxes[1].set_title(\"Control\")\naxes[1].set_xlabel(\"Donation Amount\")\naxes[1].text(mean_ctrl + 2, axes[1].get_ylim()[1]*0.9, f\"Mean = ${mean_ctrl:.2f}\", color='red')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "homework/hw1_questions.html#simulation-experiment",
    "href": "homework/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nThe plot shows the cumulative average difference in donation amounts between the treatment and control groups, based on 10,000 simulated donor pairs. Initially, the cumulative average fluctuates substantially, reflecting noise in small samples. As the number of simulated pairs increases, the cumulative average steadily converges toward the true mean difference (indicated by the red dashed line).\nThis pattern illustrates the Law of Large Numbers: with enough data, the average of simulated differences approximates the population-level treatment effect. In this case, the cumulative average stabilizes around the true mean difference, providing visual confirmation that our estimate becomes more reliable as sample size increases.\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndf = pd.read_stata(\"../data/karlan_list_2007.dta\")\nnp.random.seed(123)\n\ncontrol_data = df[(df[\"treatment\"] == 0) & (df[\"amount\"] &gt; 0)][\"amount\"]\ntreat_data = df[(df[\"treatment\"] == 1) & (df[\"amount\"] &gt; 0)][\"amount\"]\n\n# simulation\ncontrol_draws = np.random.choice(control_data, size=100000, replace=True)\ntreat_draws = np.random.choice(treat_data, size=10000, replace=True)\n\ndiffs = treat_draws - control_draws[:10000]\ncum_avg_diff = np.cumsum(diffs) / np.arange(1, len(diffs) + 1)\ntrue_diff = treat_data.mean() - control_data.mean()\n\n# Plot\nplt.figure(figsize=(10, 5))\nplt.plot(cum_avg_diff, label=\"Cumulative Avg. Difference\", color='steelblue', linewidth=2)\nplt.axhline(y=true_diff, color='crimson', linestyle='--', linewidth=2, label=\"True Mean Difference\")\n\nplt.xlabel(\"Number of Simulated Pairs\", fontsize=12)\nplt.ylabel(\"Cumulative Average (Treatment - Control)\", fontsize=12)\nplt.title(\"Cumulative Average of Donation Differences\\nSimulated from Treatment and Control Groups\", fontsize=14, fontweight='bold')\nplt.legend()\nplt.grid(alpha=0.3)\nplt.xlim(0, 10000)\nplt.ylim(-20, 20)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCentral Limit Theorem\nThese histograms show the distribution of simulated mean donation differences (treatment minus control) across 1000 replications, at four different sample sizes.\nAs the sample size increases, the distributions become narrower and more centered. This reflects a reduction in uncertainty—with small samples, estimates of the mean difference vary widely due to random noise. As the number of observations grows, the law of large numbers ensures that sample estimates converge toward the true underlying difference, and the variability across simulations shrinks.\nAt all sample sizes, the distributions tend to center slightly below zero, which is consistent with the observed data where treatment donors gave slightly less than control donors, on average. While the difference is small and not statistically significant, the simulation reinforces that larger sample sizes produce more stable and precise estimates, even when the underlying effect is near zero.\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndf = pd.read_stata(\"../data/karlan_list_2007.dta\")\nnp.random.seed(123)\n\ncontrol_data = df[(df[\"treatment\"] == 0) & (df[\"amount\"] &gt; 0)][\"amount\"].values\ntreat_data = df[(df[\"treatment\"] == 1) & (df[\"amount\"] &gt; 0)][\"amount\"].values\n\nsample_sizes = [50, 200, 500, 1000]\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\naxes = axes.flatten()\n\nfor i, n in enumerate(sample_sizes):\n    diffs = []\n    for _ in range(1000):\n        treat_sample = np.random.choice(treat_data, size=n, replace=True)\n        ctrl_sample = np.random.choice(control_data, size=n, replace=True)\n        diffs.append(treat_sample.mean() - ctrl_sample.mean())\n    \n    ax = axes[i]\n    ax.hist(diffs, bins=30, color='skyblue', edgecolor='white')\n    ax.axvline(0, color='red', linestyle='--', linewidth=2)\n    ax.set_title(f\"Sample Size = {n}\")\n    ax.set_xlabel(\"Difference in Mean Donation (Treatment - Control)\")\n    ax.set_ylabel(\"Frequency\")\n\nplt.suptitle(\"Distribution of Simulated Mean Differences\", fontsize=16, fontweight='bold')\nplt.tight_layout(rect=[0, 0, 1, 0.96])\nplt.show()"
  }
]