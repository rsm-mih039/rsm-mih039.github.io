{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Multinomial Logit Model\"\n",
        "author: \"Miya Huang\"\n",
        "date: today\n",
        "format:\n",
        "  html: default\n",
        "  pdf: default\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "This assignment expores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm. \n",
        "\n",
        "\n",
        "## 1. Likelihood for the Multi-nomial Logit (MNL) Model\n",
        "\n",
        "Suppose we have $i=1,\\ldots,n$ consumers who each select exactly one product $j$ from a set of $J$ products. The outcome variable is the identity of the product chosen $y_i \\in \\{1, \\ldots, J\\}$ or equivalently a vector of $J-1$ zeros and $1$ one, where the $1$ indicates the selected product. For example, if the third product was chosen out of 3 products, then either $y=3$ or $y=(0,0,1)$ depending on how we want to represent it. Suppose also that we have a vector of data on each product $x_j$ (eg, brand, price, etc.). \n",
        "\n",
        "We model the consumer's decision as the selection of the product that provides the most utility, and we'll specify the utility function as a linear function of the product characteristics:\n",
        "\n",
        "$$ U_{ij} = x_j'\\beta + \\epsilon_{ij} $$\n",
        "\n",
        "where $\\epsilon_{ij}$ is an i.i.d. extreme value error term. \n",
        "\n",
        "The choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer $i$ chooses product $j$:\n",
        "\n",
        "$$ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} $$\n",
        "\n",
        "For example, if there are 3 products, the probability that consumer $i$ chooses product 3 is:\n",
        "\n",
        "$$ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} $$\n",
        "\n",
        "A clever way to write the individual likelihood function for consumer $i$ is the product of the $J$ probabilities, each raised to the power of an indicator variable ($\\delta_{ij}$) that indicates the chosen product:\n",
        "\n",
        "$$ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}$$\n",
        "\n",
        "Notice that if the consumer selected product $j=3$, then $\\delta_{i3}=1$ while $\\delta_{i1}=\\delta_{i2}=0$ and the likelihood is:\n",
        "\n",
        "$$ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} $$\n",
        "\n",
        "The joint likelihood (across all consumers) is the product of the $n$ individual likelihoods:\n",
        "\n",
        "$$ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} $$\n",
        "\n",
        "And the joint log-likelihood function is:\n",
        "\n",
        "$$ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) $$\n",
        "\n",
        "\n",
        "\n",
        "## 2. Simulate Conjoint Data\n",
        "\n",
        "We will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a \"no choice\" option; each simulated respondent must select one of the 3 alternatives. \n",
        "\n",
        "Each alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from \\$4 to \\$32 in increments of \\$4.\n",
        "\n",
        "The part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer $i$ for hypothethical streaming service $j$ is \n",
        "\n",
        "$$\n",
        "u_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n",
        "$$\n",
        "\n",
        "where the variables are binary indicators and $\\varepsilon$ is Type 1 Extreme Value (ie, Gumble) distributed.\n",
        "\n",
        "The following code provides the simulation of the conjoint data.\n"
      ],
      "id": "0120cc1f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Set random seed\n",
        "np.random.seed(123)\n",
        "\n",
        "# Define attribute levels\n",
        "brands = [\"N\", \"P\", \"H\"]  # Netflix, Prime, Hulu\n",
        "ads = [\"Yes\", \"No\"]\n",
        "prices = list(range(8, 33, 4))  # from 8 to 32 by 4\n",
        "\n",
        "# Generate full factorial design\n",
        "import itertools\n",
        "profiles = pd.DataFrame(\n",
        "    list(itertools.product(brands, ads, prices)), \n",
        "    columns=[\"brand\", \"ad\", \"price\"])\n",
        "\n",
        "# Utility functions (true parameters)\n",
        "brand_utils = {\"N\": 1.0, \"P\": 0.5, \"H\": 0.0}\n",
        "ad_utils = {\"Yes\": -0.8, \"No\": 0.0}\n",
        "price_util = lambda p: -0.1 * p\n",
        "\n",
        "# Settings\n",
        "n_peeps = 100\n",
        "n_tasks = 10\n",
        "n_alts = 3\n",
        "\n",
        "# Function to simulate one respondent\n",
        "def simulate_respondent(pid):\n",
        "    respondent_data = []\n",
        "    for task in range(1, n_tasks + 1):\n",
        "        alts = profiles.sample(n=n_alts).copy()\n",
        "        alts[\"resp\"] = pid\n",
        "        alts[\"task\"] = task\n",
        "        \n",
        "        # Calculate deterministic utility\n",
        "        alts[\"v\"] = (\n",
        "            alts[\"brand\"].map(brand_utils) +\n",
        "            alts[\"ad\"].map(ad_utils) +\n",
        "            alts[\"price\"].apply(price_util)\n",
        "        ).round(10)\n",
        "        \n",
        "        # Add Gumbel-distributed error (Type I Extreme Value)\n",
        "        gumbel_noise = -np.log(-np.log(np.random.rand(n_alts)))\n",
        "        alts[\"u\"] = alts[\"v\"] + gumbel_noise\n",
        "        \n",
        "        # Choose the alternative with max utility\n",
        "        alts[\"choice\"] = (alts[\"u\"] == alts[\"u\"].max()).astype(int)\n",
        "        \n",
        "        respondent_data.append(alts)\n",
        "    \n",
        "    return pd.concat(respondent_data, ignore_index=True)\n",
        "\n",
        "# Simulate for all respondents\n",
        "df_list = [simulate_respondent(i) for i in range(1, n_peeps + 1)]\n",
        "conjoint_data = pd.concat(df_list, ignore_index=True)\n",
        "\n",
        "# Keep only relevant columns (as if unobservable utility components are hidden)\n",
        "conjoint_data = conjoint_data[[\"resp\", \"task\", \"brand\", \"ad\", \"price\", \"choice\"]]"
      ],
      "id": "1306b789",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Preparing the Data for Estimation\n",
        "\n",
        "The \"hard part\" of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer $i$, covariate $k$, and product $j$) instead of the typical 2 dimensions for cross-sectional regression models (consumer $i$ and covariate $k$). The fact that each task for each respondent has the same number of alternatives (3) helps.  In addition, we need to convert the categorical variables for brand and ads into binary variables.\n",
        "\n",
        "To estimate the MNL likelihood, we must reshape the data to panel (long) format, and encode categorical variables (brand and ad exposure) into binary indicators. Each row represents one alternative within a choice task.\n"
      ],
      "id": "e17d40bd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "conjoint_data = pd.read_csv(\"../data/conjoint_data.csv\")\n",
        "\n",
        "# One-hot encode\n",
        "X = pd.get_dummies(conjoint_data, columns=[\"brand\", \"ad\"], drop_first=True)\n",
        "\n",
        "X.rename(columns={\n",
        "    \"brand_P\": \"Prime\",\n",
        "    \"brand_N\": \"Netflix\",\n",
        "    \"ad_Yes\": \"Ads\"\n",
        "}, inplace=True)\n",
        "\n",
        "for col in ['resp', 'task', 'choice']:\n",
        "    X[col] = X[col].astype('int')\n",
        "\n",
        "X.head()"
      ],
      "id": "fd366442",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we have the data in a suitable format for estimating the likelihood. We will treat \"Hulu\" and \"Ad-Free\" as the base categories.\n",
        "\n",
        "\n",
        "\n",
        "## 4. Estimation via Maximum Likelihood\n",
        "\n",
        "We estimate the multinomial logit model by maximizing the joint log-likelihood across all respondents and tasks. The 4 parameters are:  \n",
        "- $\\beta_\\text{netflix}$  \n",
        "- $\\beta_\\text{prime}$  \n",
        "- $\\beta_\\text{ads}$  \n",
        "- $\\beta_\\text{price}$  \n"
      ],
      "id": "020c4ce9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from scipy.optimize import minimize\n",
        "\n",
        "features = [\"Netflix\", \"Prime\", \"Ads\", \"price\"]\n",
        "X[features] = X[features].astype(float)\n",
        "\n",
        "groups = X.groupby([\"resp\", \"task\"])\n",
        "\n",
        "X_mat = X[features].to_numpy()\n",
        "y = X[\"choice\"].to_numpy()\n",
        "n_rows = X.shape[0]\n",
        "n_params = len(features)"
      ],
      "id": "206e75ad",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now proceed to implement the log-likelihood function for the MNL model introduced earlier. For clarity, we restate the key expressions used in estimation below.\n",
        "\n",
        "### Log-Likelihood Function for the Multinomial Logit Model\n",
        "\n",
        "To estimate the parameters of the multinomial logit (MNL) model, we maximize the log-likelihood function based on the observed choices. For each respondent $i$ and choice task $t$, let $\\mathcal{J}_{it}$ be the set of alternatives available (typically 3). Let $x_{ijt}$ be the feature vector of alternative $j$, and $\\beta$ the parameter vector. The utility of alternative $j$ is modeled as:\n",
        "\n",
        "$$\n",
        "U_{ijt} = x_{ijt}^\\top \\beta + \\varepsilon_{ijt}\n",
        "$$\n",
        "\n",
        "Assuming $\\varepsilon_{ijt}$ follows an i.i.d. Type I Extreme Value distribution, the probability that individual $i$ chooses option $j$ in task $t$ is:\n",
        "\n",
        "$$\n",
        "\\mathbb{P}_{ijt} = \\frac{\\exp(x_{ijt}^\\top \\beta)}{\\sum_{k \\in \\mathcal{J}_{it}} \\exp(x_{ikt}^\\top \\beta)}\n",
        "$$\n",
        "\n",
        "Let $\\delta_{ijt}$ be an indicator variable equal to 1 if alternative $j$ was chosen by respondent $i$ in task $t$, and 0 otherwise. The log-likelihood across all respondents and tasks is:\n",
        "\n",
        "$$\n",
        "\\ell(\\beta) = \\sum_{i} \\sum_{t} \\sum_{j \\in \\mathcal{J}_{it}} \\delta_{ijt} \\cdot \\log \\mathbb{P}_{ijt}\n",
        "$$\n",
        "\n",
        "This is the function we aim to maximize with respect to $\\beta$ using numerical optimization.\n"
      ],
      "id": "2dc77ea7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def neg_log_likelihood(beta, X=X_mat, y=y, groups=groups):\n",
        "    utilities = X @ beta\n",
        "    log_likelihood = 0\n",
        "    start = 0\n",
        "\n",
        "    for _, group in groups:\n",
        "        n = group.shape[0]\n",
        "        util_slice = utilities[start:start + n]\n",
        "        choice_slice = y[start:start + n]\n",
        "        denom = np.sum(np.exp(util_slice))\n",
        "        probs = np.exp(util_slice) / denom\n",
        "        log_likelihood += np.log(probs @ choice_slice)\n",
        "        start += n\n",
        "\n",
        "    return -log_likelihood"
      ],
      "id": "ad09a912",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The table below presents the MLEs for each parameter in the MNL model, along with standard errors and 95% confidence intervals. These values are estimated using `scipy.optimize.minimize()` with the BFGS method.\n"
      ],
      "id": "c5bfb0ab"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Initial guess\n",
        "init_beta = np.zeros(n_params)\n",
        "\n",
        "# Optimize\n",
        "result = minimize(neg_log_likelihood, init_beta, method='BFGS')\n",
        "\n",
        "# Estimated betas\n",
        "beta_hat = result.x\n",
        "hessian_inv = result.hess_inv\n",
        "\n",
        "# Standard errors\n",
        "se = np.sqrt(np.diag(hessian_inv))\n",
        "\n",
        "# 95% confidence intervals\n",
        "z = 1.96\n",
        "ci_lower = beta_hat - z * se\n",
        "ci_upper = beta_hat + z * se\n",
        "\n",
        "# Combine results\n",
        "results = pd.DataFrame({\n",
        "    \"parameter\": features,\n",
        "    \"estimate\": beta_hat,\n",
        "    \"std_error\": se,\n",
        "    \"ci_lower\": ci_lower,\n",
        "    \"ci_upper\": ci_upper\n",
        "})\n",
        "\n",
        "from IPython.display import display\n",
        "display(results.round(4))"
      ],
      "id": "c628b3fa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Estimation via Bayesian Methods\n",
        "\n",
        "We now estimate the MNL model using Bayesian inference, via the Metropolis-Hastings (MH) algorithm. We use the same log-likelihood function from the MLE section and combine it with log-priors to compute the unnormalized log-posterior.\n",
        "\n",
        "The priors are:\n",
        "- $\\beta_{\\text{Netflix}}, \\beta_{\\text{Prime}}, \\beta_{\\text{Ads}} \\sim \\mathcal{N}(0, 5^2)$  \n",
        "- $\\beta_{\\text{price}} \\sim \\mathcal{N}(0, 1^2)$  \n",
        "\n",
        "The proposal distribution is a multivariate normal with zero covariance, i.e., independent proposals:\n",
        "- First 3 dimensions: $\\mathcal{N}(0, 0.05)$  \n",
        "- Last dimension (price): $\\mathcal{N}(0, 0.005)$  \n"
      ],
      "id": "fa08d95d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Prior log densities\n",
        "def log_prior(beta):\n",
        "    lp = -0.5 * (beta[0:3]**2 / 25).sum()\n",
        "    lp += -0.5 * (beta[3]**2 / 1)\n",
        "    return lp\n",
        "\n",
        "# Log posterior = log likelihood + log prior\n",
        "def log_posterior(beta):\n",
        "    return -neg_log_likelihood(beta) + log_prior(beta)"
      ],
      "id": "5d034888",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# MH sampler\n",
        "def metropolis_sampler(log_post_fn, start, steps=11000):\n",
        "    draws = np.zeros((steps, len(start)))\n",
        "    draws[0] = start\n",
        "    current_lp = log_post_fn(start)\n",
        "\n",
        "    for t in range(1, steps):\n",
        "        # Propose: independent normal steps\n",
        "        proposal = draws[t-1] + np.array([\n",
        "            np.random.normal(0, np.sqrt(0.05)),\n",
        "            np.random.normal(0, np.sqrt(0.05)),\n",
        "            np.random.normal(0, np.sqrt(0.05)),\n",
        "            np.random.normal(0, np.sqrt(0.005))\n",
        "        ])\n",
        "\n",
        "        proposal_lp = log_post_fn(proposal)\n",
        "\n",
        "        # MH acceptance\n",
        "        log_accept_ratio = proposal_lp - current_lp\n",
        "        if np.log(np.random.rand()) < log_accept_ratio:\n",
        "            draws[t] = proposal\n",
        "            current_lp = proposal_lp\n",
        "        else:\n",
        "            draws[t] = draws[t-1]\n",
        "\n",
        "    return draws"
      ],
      "id": "04d01b49",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "start_beta = np.zeros(4)\n",
        "samples = metropolis_sampler(log_posterior, start=start_beta, steps=11000)\n",
        "\n",
        "posterior = samples[1000:]  # remove burn-in\n",
        "\n",
        "# Compute MCMC acceptance rate\n",
        "accepted = np.sum(np.any(samples[1:] != samples[:-1], axis=1))\n",
        "accept_rate = accepted / (samples.shape[0] - 1)\n",
        "print(f\"Acceptance rate: {accept_rate:.3f}\")"
      ],
      "id": "e43b5fc7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "param_names = [\"$\\\\beta_{netflix}$\", \"$\\\\beta_{prime}$\", \"$\\\\beta_{ads}$\", \"$\\\\beta_{price}$\"]\n",
        "\n",
        "fig, axes = plt.subplots(4, 2, figsize=(12, 10))\n",
        "\n",
        "for i in range(4):\n",
        "    # Trace plot\n",
        "    axes[i, 0].plot(posterior[:, i], linewidth=0.7)\n",
        "    axes[i, 0].set_title(f\"Trace plot: {param_names[i]}\")\n",
        "    axes[i, 0].set_ylabel(\"Value\")\n",
        "    axes[i, 0].grid(alpha=0.3)\n",
        "\n",
        "    # Histogram\n",
        "    axes[i, 1].hist(posterior[:, i], bins=30, density=True)\n",
        "    axes[i, 1].axvline(posterior[:, i].mean(), color=\"red\", linestyle=\"--\", label=\"Mean\")\n",
        "    axes[i, 1].set_title(f\"Posterior histogram: {param_names[i]}\")\n",
        "    axes[i, 1].set_xlabel(\"Parameter value\")\n",
        "    axes[i, 1].set_ylabel(\"Density\")\n",
        "    axes[i, 1].grid(alpha=0.3)\n",
        "    axes[i, 1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "af7664b8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Posterior Diagnostics\n",
        "\n",
        "The trace plots and posterior histograms for all four parameters are shown below. Visually, the trace plots indicate that the Markov chains mix reasonably well and remain stable over time, suggesting adequate convergence. The chains explore the parameter space with some local variability but no major trends or drifts, which supports the credibility of the resulting posterior estimates.\n",
        "\n",
        "The posterior histograms further confirm the patterns observed in the trace plots:\n",
        "\n",
        "- **$\\beta_{netflix}$** shows a slightly right-skewed distribution centered near 0.93, with reasonable spread and no multimodality.\n",
        "- **$\\beta_{prime}$** exhibits greater dispersion but maintains a unimodal shape around 0.49.\n",
        "- **$\\beta_{ads}$** is centered tightly around -0.73, with most posterior mass between -0.90 and -0.57.\n",
        "- **$\\beta_{price}$** is the most precise, with a narrow posterior centered around -0.099, confirming strong evidence for price sensitivity.\n",
        "\n",
        "These diagnostics suggest that the Metropolis-Hastings sampler successfully captured the posterior distributions for all parameters, with trace plots supporting convergence and histograms indicating stable inference. No signs of severe autocorrelation or non-convergence are evident.\n"
      ],
      "id": "1fa9e075"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "posterior_summary = pd.DataFrame({\n",
        "    \"parameter\": features,\n",
        "    \"mean\": posterior.mean(axis=0),\n",
        "    \"std_dev\": posterior.std(axis=0),\n",
        "    \"ci_lower\": np.percentile(posterior, 2.5, axis=0),\n",
        "    \"ci_upper\": np.percentile(posterior, 97.5, axis=0)\n",
        "}).round(4)\n",
        "\n",
        "display(posterior_summary)"
      ],
      "id": "a7b3815a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The table above summarizes the posterior means, standard deviations, and 95% credible intervals for each parameter, based on 10,000 retained samples from the Metropolis-Hastings sampler.\n",
        "\n",
        "These can be directly compared with the MLE results from Section 4 to assess similarity in point estimates and uncertainty.\n",
        "\n",
        "### Comparison of MLE and Bayesian Estimates\n",
        "\n",
        "The table below compares the parameter estimates obtained via Maximum Likelihood Estimation (MLE) and Bayesian inference (via Metropolis-Hastings MCMC). Overall, the point estimates from both methods are very similar, with only minor differences across parameters. This consistency suggests that the data are informative and the priors used in the Bayesian method are relatively non-influential.\n",
        "\n",
        "- For **$\\beta_{netflix}$**, the MLE estimate is 0.9412 (95% CI: [0.8756, 1.0068]), while the Bayesian posterior mean is 0.9325 (95% CI: [0.7136, 1.1181]). Both suggest a strong positive preference for Netflix, and their intervals largely overlap.\n",
        "- For **$\\beta_{prime}$**, both approaches yield moderate positive estimates (MLE: 0.5016; Bayes: 0.4934), with the Bayesian interval slightly narrower due to the influence of the prior.\n",
        "- For **$\\beta_{ads}$**, both estimates indicate a strong negative effect of ads on utility (MLE: -0.7320; Bayes: -0.7307), with virtually identical standard errors and credible/confidence intervals.\n",
        "- The coefficient on **price** is also consistent (MLE: -0.0995; Bayes: -0.0994), confirming that higher prices reduce the probability of choice.\n",
        "\n",
        "The Bayesian credible intervals are slightly wider for some parameters (e.g., Netflix), likely reflecting greater uncertainty due to the prior. Overall, both methods tell a coherent story: consumers prefer Netflix and Prime over Hulu, dislike ads, and are price-sensitive.\n",
        "\n",
        "\n",
        "## 6. Discussion\n",
        "\n",
        "### Interpretation of Parameter Estimates\n",
        "\n",
        "If we did not know the data were simulated, we would still conclude from the estimates that respondents generally:\n",
        "\n",
        "- Prefer **Netflix** over **Amazon Prime**, and both over **Hulu** (the omitted reference level). This is reflected in the fact that $\\beta_{\\text{Netflix}} > \\beta_{\\text{Prime}} > 0$.\n",
        "- Dislike **advertisements**, as indicated by the consistently negative $\\beta_{\\text{ads}}$.\n",
        "- Are **price-sensitive**, as $\\beta_{\\text{price}} < 0$ implies that the likelihood of choosing an alternative decreases as its price increases.\n",
        "\n",
        "Specifically, $\\beta_{\\text{Netflix}} > \\beta_{\\text{Prime}}$ means that, holding other attributes constant, the utility (and therefore the probability of being chosen) is higher for Netflix than Prime. This aligns with common consumer preferences in the streaming market. Likewise, a negative $\\beta_{\\text{price}}$ is consistent with economic theory: higher prices reduce demand.\n",
        "\n",
        "### Toward a Hierarchical (Multi-level) Model\n",
        "\n",
        "In our current model, all respondents share the same set of preference parameters $\\beta$ — that is, we assume homogeneous preferences across individuals. However, in real-world conjoint studies, different people often have different tastes.\n",
        "\n",
        "To model this heterogeneity, we can move to a **hierarchical (random-parameter)** model. In this framework, each respondent $i$ has their own parameter vector $\\beta_i$, which is drawn from a population-level distribution:\n",
        "\n",
        "$$\n",
        "\\beta_i \\sim \\mathcal{N}(\\mu, \\Sigma)\n",
        "$$\n",
        "\n",
        "Here, $\\mu$ represents the **average preference across the population**, and $\\Sigma$ captures the **variation between individuals**.\n",
        "\n",
        "To simulate such data, we would:\n",
        "1. Draw a unique $\\beta_i$ for each respondent from the population distribution.\n",
        "2. Use that $\\beta_i$ to simulate choices for each of their tasks.\n",
        "\n",
        "To estimate the parameters, we would need to use **Bayesian hierarchical methods** or **maximum simulated likelihood**, such as:\n",
        "- MCMC with Gibbs or Hamiltonian sampling\n",
        "- Hierarchical Bayes via Stan or PyMC\n",
        "- Mixed logit estimation (if using frequentist methods)\n",
        "\n",
        "These models are more complex, but they better capture real consumer behavior by accounting for **individual-level preference variation**.\n",
        "\n",
        "### Sampler Performance Note\n",
        "\n",
        "The Metropolis-Hastings algorithm yielded an acceptance rate of approximately 1.8%, which is substantially lower than the typical recommended range (20%–40%). This suggests that the proposal distribution may not have been well-tuned for the posterior geometry, potentially resulting in poor mixing and inefficient exploration of the parameter space. While the trace plots do show some movement across the support, future implementations could benefit from increasing the proposal variance or adopting adaptive MCMC methods to improve efficiency.\n"
      ],
      "id": "aa8ed1f8"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Library/Frameworks/Python.framework/Versions/3.11/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}