{
  "hash": "b360b936986b089c5e5bf2b3284e8e57",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Multinomial Logit Model\"\nauthor: \"Miya Huang\"\ndate: today\nformat:\n  html: default\n  pdf: default\njupyter: python3\n---\n\n\n\n\n\n\n\n\n\nThis assignment expores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm. \n\n\n## 1. Likelihood for the Multi-nomial Logit (MNL) Model\n\nSuppose we have $i=1,\\ldots,n$ consumers who each select exactly one product $j$ from a set of $J$ products. The outcome variable is the identity of the product chosen $y_i \\in \\{1, \\ldots, J\\}$ or equivalently a vector of $J-1$ zeros and $1$ one, where the $1$ indicates the selected product. For example, if the third product was chosen out of 3 products, then either $y=3$ or $y=(0,0,1)$ depending on how we want to represent it. Suppose also that we have a vector of data on each product $x_j$ (eg, brand, price, etc.). \n\nWe model the consumer's decision as the selection of the product that provides the most utility, and we'll specify the utility function as a linear function of the product characteristics:\n\n$$ U_{ij} = x_j'\\beta + \\epsilon_{ij} $$\n\nwhere $\\epsilon_{ij}$ is an i.i.d. extreme value error term. \n\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer $i$ chooses product $j$:\n\n$$ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} $$\n\nFor example, if there are 3 products, the probability that consumer $i$ chooses product 3 is:\n\n$$ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} $$\n\nA clever way to write the individual likelihood function for consumer $i$ is the product of the $J$ probabilities, each raised to the power of an indicator variable ($\\delta_{ij}$) that indicates the chosen product:\n\n$$ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}$$\n\nNotice that if the consumer selected product $j=3$, then $\\delta_{i3}=1$ while $\\delta_{i1}=\\delta_{i2}=0$ and the likelihood is:\n\n$$ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} $$\n\nThe joint likelihood (across all consumers) is the product of the $n$ individual likelihoods:\n\n$$ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} $$\n\nAnd the joint log-likelihood function is:\n\n$$ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) $$\n\n\n\n## 2. Simulate Conjoint Data\n\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a \"no choice\" option; each simulated respondent must select one of the 3 alternatives. \n\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from \\$4 to \\$32 in increments of \\$4.\n\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer $i$ for hypothethical streaming service $j$ is \n\n$$\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n$$\n\nwhere the variables are binary indicators and $\\varepsilon$ is Type 1 Extreme Value (ie, Gumble) distributed.\n\nThe following code provides the simulation of the conjoint data.\n\n::: {#99526a49 .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\n\n# Set random seed\nnp.random.seed(123)\n\n# Define attribute levels\nbrands = [\"N\", \"P\", \"H\"]  # Netflix, Prime, Hulu\nads = [\"Yes\", \"No\"]\nprices = list(range(8, 33, 4))  # from 8 to 32 by 4\n\n# Generate full factorial design\nimport itertools\nprofiles = pd.DataFrame(\n    list(itertools.product(brands, ads, prices)), \n    columns=[\"brand\", \"ad\", \"price\"])\n\n# Utility functions (true parameters)\nbrand_utils = {\"N\": 1.0, \"P\": 0.5, \"H\": 0.0}\nad_utils = {\"Yes\": -0.8, \"No\": 0.0}\nprice_util = lambda p: -0.1 * p\n\n# Settings\nn_peeps = 100\nn_tasks = 10\nn_alts = 3\n\n# Function to simulate one respondent\ndef simulate_respondent(pid):\n    respondent_data = []\n    for task in range(1, n_tasks + 1):\n        alts = profiles.sample(n=n_alts).copy()\n        alts[\"resp\"] = pid\n        alts[\"task\"] = task\n        \n        # Calculate deterministic utility\n        alts[\"v\"] = (\n            alts[\"brand\"].map(brand_utils) +\n            alts[\"ad\"].map(ad_utils) +\n            alts[\"price\"].apply(price_util)\n        ).round(10)\n        \n        # Add Gumbel-distributed error (Type I Extreme Value)\n        gumbel_noise = -np.log(-np.log(np.random.rand(n_alts)))\n        alts[\"u\"] = alts[\"v\"] + gumbel_noise\n        \n        # Choose the alternative with max utility\n        alts[\"choice\"] = (alts[\"u\"] == alts[\"u\"].max()).astype(int)\n        \n        respondent_data.append(alts)\n    \n    return pd.concat(respondent_data, ignore_index=True)\n\n# Simulate for all respondents\ndf_list = [simulate_respondent(i) for i in range(1, n_peeps + 1)]\nconjoint_data = pd.concat(df_list, ignore_index=True)\n\n# Keep only relevant columns (as if unobservable utility components are hidden)\nconjoint_data = conjoint_data[[\"resp\", \"task\", \"brand\", \"ad\", \"price\", \"choice\"]]\n```\n:::\n\n\n## 3. Preparing the Data for Estimation\n\nThe \"hard part\" of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer $i$, covariate $k$, and product $j$) instead of the typical 2 dimensions for cross-sectional regression models (consumer $i$ and covariate $k$). The fact that each task for each respondent has the same number of alternatives (3) helps.  In addition, we need to convert the categorical variables for brand and ads into binary variables.\n\nTo estimate the MNL likelihood, we must reshape the data to panel (long) format, and encode categorical variables (brand and ad exposure) into binary indicators. Each row represents one alternative within a choice task.\n\n::: {#3ac3c249 .cell execution_count=2}\n``` {.python .cell-code}\nimport pandas as pd\nimport numpy as np\n\nconjoint_data = pd.read_csv(\"../data/conjoint_data.csv\")\n\n# One-hot encode\nX = pd.get_dummies(conjoint_data, columns=[\"brand\", \"ad\"], drop_first=True)\n\nX.rename(columns={\n    \"brand_P\": \"Prime\",\n    \"brand_N\": \"Netflix\",\n    \"ad_Yes\": \"Ads\"\n}, inplace=True)\n\nfor col in ['resp', 'task', 'choice']:\n    X[col] = X[col].astype('int')\n\nX.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>resp</th>\n      <th>task</th>\n      <th>choice</th>\n      <th>price</th>\n      <th>Netflix</th>\n      <th>Prime</th>\n      <th>Ads</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>28</td>\n      <td>True</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>16</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>16</td>\n      <td>False</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>32</td>\n      <td>True</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>16</td>\n      <td>False</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nNow we have the data in a suitable format for estimating the likelihood. We will treat \"Hulu\" and \"Ad-Free\" as the base categories.\n\n\n\n## 4. Estimation via Maximum Likelihood\n\nWe estimate the multinomial logit model by maximizing the joint log-likelihood across all respondents and tasks. The 4 parameters are:  \n- $\\beta_\\text{netflix}$  \n- $\\beta_\\text{prime}$  \n- $\\beta_\\text{ads}$  \n- $\\beta_\\text{price}$  \n\n::: {#cc635b38 .cell execution_count=3}\n``` {.python .cell-code}\nfrom scipy.optimize import minimize\n\nfeatures = [\"Netflix\", \"Prime\", \"Ads\", \"price\"]\nX[features] = X[features].astype(float)\n\ngroups = X.groupby([\"resp\", \"task\"])\n\nX_mat = X[features].to_numpy()\ny = X[\"choice\"].to_numpy()\nn_rows = X.shape[0]\nn_params = len(features)\n```\n:::\n\n\nWe now proceed to implement the log-likelihood function for the MNL model introduced earlier. For clarity, we restate the key expressions used in estimation below.\n\n### Log-Likelihood Function for the Multinomial Logit Model\n\nTo estimate the parameters of the multinomial logit (MNL) model, we maximize the log-likelihood function based on the observed choices. For each respondent $i$ and choice task $t$, let $\\mathcal{J}_{it}$ be the set of alternatives available (typically 3). Let $x_{ijt}$ be the feature vector of alternative $j$, and $\\beta$ the parameter vector. The utility of alternative $j$ is modeled as:\n\n$$\nU_{ijt} = x_{ijt}^\\top \\beta + \\varepsilon_{ijt}\n$$\n\nAssuming $\\varepsilon_{ijt}$ follows an i.i.d. Type I Extreme Value distribution, the probability that individual $i$ chooses option $j$ in task $t$ is:\n\n$$\n\\mathbb{P}_{ijt} = \\frac{\\exp(x_{ijt}^\\top \\beta)}{\\sum_{k \\in \\mathcal{J}_{it}} \\exp(x_{ikt}^\\top \\beta)}\n$$\n\nLet $\\delta_{ijt}$ be an indicator variable equal to 1 if alternative $j$ was chosen by respondent $i$ in task $t$, and 0 otherwise. The log-likelihood across all respondents and tasks is:\n\n$$\n\\ell(\\beta) = \\sum_{i} \\sum_{t} \\sum_{j \\in \\mathcal{J}_{it}} \\delta_{ijt} \\cdot \\log \\mathbb{P}_{ijt}\n$$\n\nThis is the function we aim to maximize with respect to $\\beta$ using numerical optimization.\n\n::: {#805be78e .cell execution_count=4}\n``` {.python .cell-code}\ndef neg_log_likelihood(beta, X=X_mat, y=y, groups=groups):\n    utilities = X @ beta\n    log_likelihood = 0\n    start = 0\n\n    for _, group in groups:\n        n = group.shape[0]\n        util_slice = utilities[start:start + n]\n        choice_slice = y[start:start + n]\n        denom = np.sum(np.exp(util_slice))\n        probs = np.exp(util_slice) / denom\n        log_likelihood += np.log(probs @ choice_slice)\n        start += n\n\n    return -log_likelihood\n```\n:::\n\n\nThe table below presents the MLEs for each parameter in the MNL model, along with standard errors and 95% confidence intervals. These values are estimated using `scipy.optimize.minimize()` with the BFGS method.\n\n::: {#c6e885a4 .cell execution_count=5}\n``` {.python .cell-code}\n# Initial guess\ninit_beta = np.zeros(n_params)\n\n# Optimize\nresult = minimize(neg_log_likelihood, init_beta, method='BFGS')\n\n# Estimated betas\nbeta_hat = result.x\nhessian_inv = result.hess_inv\n\n# Standard errors\nse = np.sqrt(np.diag(hessian_inv))\n\n# 95% confidence intervals\nz = 1.96\nci_lower = beta_hat - z * se\nci_upper = beta_hat + z * se\n\n# Combine results\nresults = pd.DataFrame({\n    \"parameter\": features,\n    \"estimate\": beta_hat,\n    \"std_error\": se,\n    \"ci_lower\": ci_lower,\n    \"ci_upper\": ci_upper\n})\n\nfrom IPython.display import display\ndisplay(results.round(4))\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>parameter</th>\n      <th>estimate</th>\n      <th>std_error</th>\n      <th>ci_lower</th>\n      <th>ci_upper</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Netflix</td>\n      <td>0.9412</td>\n      <td>0.0335</td>\n      <td>0.8756</td>\n      <td>1.0068</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Prime</td>\n      <td>0.5016</td>\n      <td>0.1180</td>\n      <td>0.2704</td>\n      <td>0.7329</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Ads</td>\n      <td>-0.7320</td>\n      <td>0.0898</td>\n      <td>-0.9080</td>\n      <td>-0.5560</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>price</td>\n      <td>-0.0995</td>\n      <td>0.0064</td>\n      <td>-0.1121</td>\n      <td>-0.0868</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## 5. Estimation via Bayesian Methods\n\nWe now estimate the MNL model using Bayesian inference, via the Metropolis-Hastings (MH) algorithm. We use the same log-likelihood function from the MLE section and combine it with log-priors to compute the unnormalized log-posterior.\n\nThe priors are:\n- $\\beta_{\\text{Netflix}}, \\beta_{\\text{Prime}}, \\beta_{\\text{Ads}} \\sim \\mathcal{N}(0, 5^2)$  \n- $\\beta_{\\text{price}} \\sim \\mathcal{N}(0, 1^2)$  \n\nThe proposal distribution is a multivariate normal with zero covariance, i.e., independent proposals:\n- First 3 dimensions: $\\mathcal{N}(0, 0.05)$  \n- Last dimension (price): $\\mathcal{N}(0, 0.005)$  \n\n::: {#a0eec145 .cell execution_count=6}\n``` {.python .cell-code}\n# Prior log densities\ndef log_prior(beta):\n    lp = -0.5 * (beta[0:3]**2 / 25).sum()\n    lp += -0.5 * (beta[3]**2 / 1)\n    return lp\n\n# Log posterior = log likelihood + log prior\ndef log_posterior(beta):\n    return -neg_log_likelihood(beta) + log_prior(beta)\n```\n:::\n\n\n::: {#ca30d650 .cell execution_count=7}\n``` {.python .cell-code}\n# MH sampler\ndef metropolis_sampler(log_post_fn, start, steps=11000):\n    draws = np.zeros((steps, len(start)))\n    draws[0] = start\n    current_lp = log_post_fn(start)\n\n    for t in range(1, steps):\n        # Propose: independent normal steps\n        proposal = draws[t-1] + np.array([\n            np.random.normal(0, np.sqrt(0.05)),\n            np.random.normal(0, np.sqrt(0.05)),\n            np.random.normal(0, np.sqrt(0.05)),\n            np.random.normal(0, np.sqrt(0.005))\n        ])\n\n        proposal_lp = log_post_fn(proposal)\n\n        # MH acceptance\n        log_accept_ratio = proposal_lp - current_lp\n        if np.log(np.random.rand()) < log_accept_ratio:\n            draws[t] = proposal\n            current_lp = proposal_lp\n        else:\n            draws[t] = draws[t-1]\n\n    return draws\n```\n:::\n\n\n::: {#2c6b6e07 .cell execution_count=8}\n``` {.python .cell-code}\nnp.random.seed(42)\n\nstart_beta = np.zeros(4)\nsamples = metropolis_sampler(log_posterior, start=start_beta, steps=11000)\n\nposterior = samples[1000:]  # remove burn-in\n\n# Compute MCMC acceptance rate\naccepted = np.sum(np.any(samples[1:] != samples[:-1], axis=1))\naccept_rate = accepted / (samples.shape[0] - 1)\nprint(f\"Acceptance rate: {accept_rate:.3f}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAcceptance rate: 0.018\n```\n:::\n:::\n\n\n::: {#7cee9146 .cell execution_count=9}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n\nparam_names = [\"$\\\\beta_{netflix}$\", \"$\\\\beta_{prime}$\", \"$\\\\beta_{ads}$\", \"$\\\\beta_{price}$\"]\n\nfig, axes = plt.subplots(4, 2, figsize=(12, 10))\n\nfor i in range(4):\n    # Trace plot\n    axes[i, 0].plot(posterior[:, i], linewidth=0.7)\n    axes[i, 0].set_title(f\"Trace plot: {param_names[i]}\")\n    axes[i, 0].set_ylabel(\"Value\")\n    axes[i, 0].grid(alpha=0.3)\n\n    # Histogram\n    axes[i, 1].hist(posterior[:, i], bins=30, density=True)\n    axes[i, 1].axvline(posterior[:, i].mean(), color=\"red\", linestyle=\"--\", label=\"Mean\")\n    axes[i, 1].set_title(f\"Posterior histogram: {param_names[i]}\")\n    axes[i, 1].set_xlabel(\"Parameter value\")\n    axes[i, 1].set_ylabel(\"Density\")\n    axes[i, 1].grid(alpha=0.3)\n    axes[i, 1].legend()\n\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](hw3_questions_files/figure-html/cell-10-output-1.png){width=1142 height=949}\n:::\n:::\n\n\n### Posterior Diagnostics\n\nThe trace plots and posterior histograms for all four parameters are shown below. Visually, the trace plots indicate that the Markov chains mix reasonably well and remain stable over time, suggesting adequate convergence. The chains explore the parameter space with some local variability but no major trends or drifts, which supports the credibility of the resulting posterior estimates.\n\nThe posterior histograms further confirm the patterns observed in the trace plots:\n\n- **$\\beta_{netflix}$** shows a slightly right-skewed distribution centered near 0.93, with reasonable spread and no multimodality.\n- **$\\beta_{prime}$** exhibits greater dispersion but maintains a unimodal shape around 0.49.\n- **$\\beta_{ads}$** is centered tightly around -0.73, with most posterior mass between -0.90 and -0.57.\n- **$\\beta_{price}$** is the most precise, with a narrow posterior centered around -0.099, confirming strong evidence for price sensitivity.\n\nThese diagnostics suggest that the Metropolis-Hastings sampler successfully captured the posterior distributions for all parameters, with trace plots supporting convergence and histograms indicating stable inference. No signs of severe autocorrelation or non-convergence are evident.\n\n::: {#5dc6f8f4 .cell execution_count=10}\n``` {.python .cell-code}\nposterior_summary = pd.DataFrame({\n    \"parameter\": features,\n    \"mean\": posterior.mean(axis=0),\n    \"std_dev\": posterior.std(axis=0),\n    \"ci_lower\": np.percentile(posterior, 2.5, axis=0),\n    \"ci_upper\": np.percentile(posterior, 97.5, axis=0)\n}).round(4)\n\ndisplay(posterior_summary)\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>parameter</th>\n      <th>mean</th>\n      <th>std_dev</th>\n      <th>ci_lower</th>\n      <th>ci_upper</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Netflix</td>\n      <td>0.9325</td>\n      <td>0.1000</td>\n      <td>0.7136</td>\n      <td>1.1181</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Prime</td>\n      <td>0.4934</td>\n      <td>0.0917</td>\n      <td>0.3041</td>\n      <td>0.6559</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Ads</td>\n      <td>-0.7307</td>\n      <td>0.0882</td>\n      <td>-0.9066</td>\n      <td>-0.5691</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>price</td>\n      <td>-0.0994</td>\n      <td>0.0061</td>\n      <td>-0.1109</td>\n      <td>-0.0885</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nThe table above summarizes the posterior means, standard deviations, and 95% credible intervals for each parameter, based on 10,000 retained samples from the Metropolis-Hastings sampler.\n\nThese can be directly compared with the MLE results from Section 4 to assess similarity in point estimates and uncertainty.\n\n### Comparison of MLE and Bayesian Estimates\n\nThe table below compares the parameter estimates obtained via Maximum Likelihood Estimation (MLE) and Bayesian inference (via Metropolis-Hastings MCMC). Overall, the point estimates from both methods are very similar, with only minor differences across parameters. This consistency suggests that the data are informative and the priors used in the Bayesian method are relatively non-influential.\n\n- For **$\\beta_{netflix}$**, the MLE estimate is 0.9412 (95% CI: [0.8756, 1.0068]), while the Bayesian posterior mean is 0.9325 (95% CI: [0.7136, 1.1181]). Both suggest a strong positive preference for Netflix, and their intervals largely overlap.\n- For **$\\beta_{prime}$**, both approaches yield moderate positive estimates (MLE: 0.5016; Bayes: 0.4934), with the Bayesian interval slightly narrower due to the influence of the prior.\n- For **$\\beta_{ads}$**, both estimates indicate a strong negative effect of ads on utility (MLE: -0.7320; Bayes: -0.7307), with virtually identical standard errors and credible/confidence intervals.\n- The coefficient on **price** is also consistent (MLE: -0.0995; Bayes: -0.0994), confirming that higher prices reduce the probability of choice.\n\nThe Bayesian credible intervals are slightly wider for some parameters (e.g., Netflix), likely reflecting greater uncertainty due to the prior. Overall, both methods tell a coherent story: consumers prefer Netflix and Prime over Hulu, dislike ads, and are price-sensitive.\n\n\n## 6. Discussion\n\n### Interpretation of Parameter Estimates\n\nIf we did not know the data were simulated, we would still conclude from the estimates that respondents generally:\n\n- Prefer **Netflix** over **Amazon Prime**, and both over **Hulu** (the omitted reference level). This is reflected in the fact that $\\beta_{\\text{Netflix}} > \\beta_{\\text{Prime}} > 0$.\n- Dislike **advertisements**, as indicated by the consistently negative $\\beta_{\\text{ads}}$.\n- Are **price-sensitive**, as $\\beta_{\\text{price}} < 0$ implies that the likelihood of choosing an alternative decreases as its price increases.\n\nSpecifically, $\\beta_{\\text{Netflix}} > \\beta_{\\text{Prime}}$ means that, holding other attributes constant, the utility (and therefore the probability of being chosen) is higher for Netflix than Prime. This aligns with common consumer preferences in the streaming market. Likewise, a negative $\\beta_{\\text{price}}$ is consistent with economic theory: higher prices reduce demand.\n\n### Toward a Hierarchical (Multi-level) Model\n\nIn our current model, all respondents share the same set of preference parameters $\\beta$ — that is, we assume homogeneous preferences across individuals. However, in real-world conjoint studies, different people often have different tastes.\n\nTo model this heterogeneity, we can move to a **hierarchical (random-parameter)** model. In this framework, each respondent $i$ has their own parameter vector $\\beta_i$, which is drawn from a population-level distribution:\n\n$$\n\\beta_i \\sim \\mathcal{N}(\\mu, \\Sigma)\n$$\n\nHere, $\\mu$ represents the **average preference across the population**, and $\\Sigma$ captures the **variation between individuals**.\n\nTo simulate such data, we would:\n1. Draw a unique $\\beta_i$ for each respondent from the population distribution.\n2. Use that $\\beta_i$ to simulate choices for each of their tasks.\n\nTo estimate the parameters, we would need to use **Bayesian hierarchical methods** or **maximum simulated likelihood**, such as:\n- MCMC with Gibbs or Hamiltonian sampling\n- Hierarchical Bayes via Stan or PyMC\n- Mixed logit estimation (if using frequentist methods)\n\nThese models are more complex, but they better capture real consumer behavior by accounting for **individual-level preference variation**.\n\n### Sampler Performance Note\n\nThe Metropolis-Hastings algorithm yielded an acceptance rate of approximately 1.8%, which is substantially lower than the typical recommended range (20%–40%). This suggests that the proposal distribution may not have been well-tuned for the posterior geometry, potentially resulting in poor mixing and inefficient exploration of the parameter space. While the trace plots do show some movement across the support, future implementations could benefit from increasing the proposal variance or adopting adaptive MCMC methods to improve efficiency.\n\n",
    "supporting": [
      "hw3_questions_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}